{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain with AzureML\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Endpoint: https://aoai.openai.azure.com/\n",
      "MLFlow Tracking URI:  azureml://eastus2.api.azureml.ms/mlflow/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/ray/providers/Microsoft.MachineLearningServices/workspaces/ray\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai, os\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "print(\"OpenAI Endpoint:\", openai.api_base)\n",
    "print(\"MLFlow Tracking URI: \", os.getenv(\"MLFLOW_TRACKING_URI\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What did the large language model say when it was asked to tell a joke?\n",
      "A: I'm sorry, I don't know any jokes yet. I'm still learning!\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0.5)\n",
    "prompt=\"\"\"\n",
    "Tell me a joke about large language models.\n",
    "\"\"\"\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing well, thank you for asking. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    ")\n",
    "\n",
    "reply = chat([SystemMessage(content=\"You are a friendly chat agent.\"),\n",
    "              HumanMessage(content=\"Hello, how are you?\")])\n",
    "\n",
    "print(reply.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Types\n",
    "\n",
    "### Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "prompt=\"\"\"\n",
    "Classify the following news headline into 1 of the following categories: Business, Tech, Politics, Sport, Entertainment\n",
    "\n",
    "Headline: Major Retailer Announces Plans to Close Over 100 Stores\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "prompt=\"\"\"\n",
    "Classify the following news headline into 1 of the following categories: Business, Tech, Politics, Sport, Entertainment\n",
    "\n",
    "Headline 1: Donna Steffensen Is Cooking Up a New Kind of Perfection. The Internet's most beloved cooking guru has a buzzy new book and a fresh new perspective\n",
    "Category: Entertainment\n",
    "\n",
    "Headline 2: Major Retailer Announces Plans to Close Over 100 Stores\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The answer is x = 4.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text=\"\"\"\n",
    "what is the anwer to this equation: 3x - 7 = -2x + 5\n",
    "\"\"\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12/5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "x = symbols('x')\n",
    "solve(3*x - 7 + 2*x - 5, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Add 7 to both sides of the equation.\n",
      "3x - 7 + 7 = -2x + 5 + 7\n",
      "\n",
      "Step 2: Simplify the left side of the equation.\n",
      "3x = -2x + 12\n",
      "\n",
      "Step 3: Subtract -2x from both sides of the equation.\n",
      "3x - (-2x) = -2x + 12 - (-2x)\n",
      "\n",
      "Step 4: Simplify the left side of the equation.\n",
      "5x = 12\n",
      "\n",
      "Step 5: Divide both sides of the equation by 5.\n",
      "x = 12/5\n",
      "\n",
      "Step 6: Simplify the right side of the equation.\n",
      "x = 2.4\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text=\"\"\"\n",
    "solve the following algebraic equation step-by-step and explain you reasoning at each step: 3x - 7 = -2x + 5\n",
    "\"\"\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/chains/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nature's Nibbles.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"organic snacks\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Langchain\n",
    "\n",
    "The first thing I did when I started to work with Langchain was to patch it to get more logging information. Langchain does a great job at abstracting away what is really going on under the hood, in particular when working with more complex chains or agents. However, this abstraction comes at the cost of not being able to see what is really going on.\n",
    "\n",
    "Instead of patching the code, I could have also use a callback, but I found that monkey patching was easier to use since I didn't have to change the code I was running -- for instance, I could just run an off-the-shelf agent and get the logging information.\n",
    "\n",
    "What my patch does is to add wrap some key functions that langchain uses and log the parameters that go into them and the value that is returned by them to MLFlow as an artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.langchain.patch import patch_langchain\n",
    "import mlflow\n",
    "\n",
    "patch_langchain()\n",
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "See this run for the logs: [joyful_airport_dtlm7qm9](https://ml.azure.com/experiments/id/3b32df62-b496-4f39-8498-2f08f7258493/runs/954a26f7-4291-45f2-8038-f5dc6cdefd00?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/ray/workspaces/ray&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to know the interest rate of the bonds\n",
      "Action: Search\n",
      "Action Input: \"2 year government bond rate\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m2 Year Treasury Rate is at 4.08%, compared to 3.96% the previous market day and 2.47% last year. This is higher than the long term average of 3.16%. The 2 Year Treasury Rate is the yield received for investing in a US government issued treasury security that has a maturity of 2 years.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the amount of money I will have in 2 years\n",
      "Action: Calculator\n",
      "Action Input: 1000 * (1 + 0.0408)^2\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1083.2646399999999\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: In two years, I will have 1083.26 dollars if I invest 1000 dollars in 2 year government bonds.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI, AzureOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import openai, os\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "agent.run(\"I have 1000 dollars. given today's governement bond rate, how much money would I have in 2 years if I invested them in 2 year government bonds?\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Retrieval Augmented Generation\"-Style Chain\n",
    "\n",
    "The main task used here is a simple question answering task on AzureML. The user asks a question and the system answers it. In order to answer the question, the system needs to retrieve some context from a cognitive services search index which is then used to generate the answer. \n",
    "\n",
    "Langchain has the `RetrievaQA` chain to easily build a system that does this -- all I need is Retriever that implements the `get_relevant_documents` method that returns a list of documents relevant to the user's query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "class CognitiveSearchRetriever(BaseRetriever):\n",
    "    def __init__(self, endpoint: str, index_name: str, searchkey: str, top: int = 3):\n",
    "        self.endpoint = endpoint\n",
    "        self.index_name = index_name\n",
    "        self.searchkey = searchkey\n",
    "        self.top = top\n",
    "        self.client = SearchClient(endpoint=endpoint, index_name=index_name, credential=AzureKeyCredential(searchkey))\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        docs = []\n",
    "        for i in self.client.search(query, top=self.top):\n",
    "            docs.append(Document(page_content=i['content'], metadata={\"sourcefile\": i['sourcefile']}))\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use the above class to query the search index and generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-attach-compute-cluster-0.md'}\n",
      "\n",
      "# Create an Azure Machine Learning compute cluster\n",
      "\n",
      "[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n",
      "\n",
      "> [!div class=\"op_single_selector\" title1=\"Select the Azure Machine Learning CLI or SDK version you are using:\"]\n",
      "> * [v1](v1/how-to-create-attach-compute-cluster.md)\n",
      "> * [v2 (current \n",
      "...\n",
      "---------------------------------------\n",
      "{'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-quotas-66.md'}\n",
      "> To learn more about which VM family to request a quota increase for, check out [virtual machine sizes in Azure](../virtual-machines/sizes.md). For instance GPU VM families start with an \"N\" in their family name (eg. NCv3 series)\n",
      "\n",
      "The following table shows more limits in the platform. Reach out to \n",
      "...\n",
      "---------------------------------------\n",
      "{'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-configure-databricks-automl-environment-0.md'}\n",
      "\n",
      "# Set up a development environment with Azure Databricks and AutoML in Azure Machine Learning \n",
      "\n",
      "Learn how to configure a development environment in Azure Machine Learning that uses Azure Databricks and automated ML.\n",
      "\n",
      "Azure Databricks is ideal for running large-scale intensive machine learning workf\n",
      "...\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cog_search = CognitiveSearchRetriever(endpoint=os.getenv(\"COG_SEARCH_ENDPOINT\"), index_name=\"amldocs\", searchkey=os.environ[\"COG_SEARCH_KEY\"])\n",
    "docs = cog_search.get_relevant_documents(\"how to set up a compute cluster\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content[:300])\n",
    "    print(\"...\")\n",
    "    print('---------------------------------------')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it takes only a few lines to build a chain that uses that search index to answer the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                    chain_type=\"stuff\",\n",
    "                                    retriever=cog_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To set up a compute cluster in Azure Machine Learning workspace, you can follow the instructions given in the \"Create an Azure Machine Learning compute cluster\" article. This article provides step-by-step instructions on how to create and manage a compute cluster, and how to lower the compute cluster cost with low priority VMs. The article also explains how to set up a managed identity for the cluster. Here is the link to the article: \n",
      "\n",
      "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?view=azure-ml-py.\n"
     ]
    }
   ],
   "source": [
    "result = qa(\"how can I set up a compute cluster?\")\n",
    "mlflow.end_run()\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here for the full source code of the chain: [rag_with_cog_search.py](src/langchain/rag_with_cog_search.py)\n",
    "\n",
    "Here is a link to the job that shows the logs for the run: [tender_carnival_zjr2f2kw](https://ml.azure.com/experiments/id/3b32df62-b496-4f39-8498-2f08f7258493/runs/acebb051-af91-408e-8f7f-edfec7da6101?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/ray/workspaces/ray&tid=72f988bf-86f1-41af-91ab-2d7cd011db47#overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running offline, checking secrets\n",
      "Q: how can I set up a compute cluster?\n",
      "A: To set up a compute cluster, you can follow the instructions provided in the following Azure documentation:\n",
      "\n",
      "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster\n",
      "\n",
      "This documentation provides detailed guidance on how to create and manage a compute cluster in your Azure Machine Learning workspace. It describes how to use Compute Cluster to distribute a training or batch inference process across a cluster of CPU or GPU compute nodes in the cloud. It also outlines how to lower your compute cluster cost with low priority VMs and set up a managed identity for the cluster.\n"
     ]
    }
   ],
   "source": [
    "!python src/langchain/rag_with_cog_search.py --question \"how can I set up a compute cluster?\" --top 3 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundedness\n",
    "\n",
    "The model might sometimes fill in information that was not provided in the search context or that would be common knowledge. If that happens, we call the reply as not grounded. \n",
    "\n",
    "For instance the query below suggests a command `az ml workspace compute show-quota` which doesn't exist in the documentation or the product. Instead `az ml compute list-usage` would be the right command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running offline, checking secrets\n",
      "Q: how can I see the quota assigned to my workspace in the v2 cli?\n",
      "A: To see the quota assigned to your workspace in the V2 CLI, you can run the following command:\n",
      "\n",
      "```\n",
      "az ml workspace show-quota --workspace-name <workspace-name> --resource-group <resource-group-name>\n",
      "```\n",
      "\n",
      "Make sure to replace <workspace-name> and <resource-group-name> with the appropriate values for your workspace.\n"
     ]
    }
   ],
   "source": [
    "!python src/langchain/rag_with_cog_search.py --question \"how can I see the quota assigned to my workspace in the v2 cli?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](data/amldocs/groundedness/conversation_context.json) is the output of the above command -- it contains both the query, the reply and the context based on which the reply was given.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": \"how can I see the quota assigned to my workspace in the v2 cli?\",\n",
    "  \"result\": \"You can use the following Azure CLI command to view the quotas for your Machine Learning workspace:\\n```\\naz ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\\n```\\n\\nReplace `<name>` with the name of the Machine Learning workspace and `<resource-group>` with the resource group that contains the workspace. You can also add `--compute-target` and `--ids` options if you have specific requirements for viewing compute targets or identity information.\",\n",
    "  \"context\": [\n",
    "    {\n",
    "      \"page_content\": \"\\nAfter deployment, this role becomes available in the specified workspace. Now you can add and assign this role in the Azure portal.\\n\\nFor more information on custom roles, see [Azure custom roles](../role-based-access-control/custom-roles.md). \\n\\n### Azure Machine Learning operations\\n\\nFor more information on the operations (actions and not actions) usable with custom roles, see [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices). You can also use the following Azure CLI command to list operations:\\n\\n```azurecli-interactive\\naz provider operation show â€“n Microsoft.MachineLearningServices\\n```\\n\\n## List custom roles\\n\\nIn the Azure CLI, run the following command:\\n\\n```azurecli-interactive\\naz role definition list --subscription <sub-id> --custom-role-only true\\n```\\n\\nTo view the role definition for a specific custom role, use the following Azure CLI command. The `<role-name>` should be in the same format returned by the command above:\\n\\n```azurecli-interactive\\naz role definition list -n <role-name> --subscription <sub-id>\\n```\\n\\n## Update a custom role\\n\\nIn the Azure CLI, run the following command:\\n\\n```azurecli-interactive\\naz role definition update --role-definition update_def.json --subscription <sub-id>\\n```\\n\\nYou need to have permissions on the entire scope of your new role definition. For example if this new role has a scope across three subscriptions, you need to have permissions on all three subscriptions. \\n\\n> [!NOTE]\\n> Role updates can take 15 minutes to an hour to apply across all role assignments in that scope.\\n\\n## Use Azure Resource Manager templates for repeatability\\n\\nIf you anticipate that you'll need to recreate complex role assignments, an Azure Resource Manager template can be a significant help. The [machine-learning-dependencies-role-assignment template](https://github.com/Azure/azure-quickstart-templates/tree/master//quickstarts/microsoft.machinelearningservices/machine-learning-dependencies-role-assignment) shows how role assignments can be specified in source code for reuse. \\n\\n## Common scenarios\\n\\nThe following table is a summary of Azure Machine Learning activities and the permissions required to perform them at the least scope. For example, if an activity can be performed with a workspace scope (Column 4), then all higher scope with that permission will also work automatically. Note that for certain activities the permissions differ between V1 and V2 APIs.\\n\\n> [!IMPORTANT]\\n> All paths in this table that start with `/` are **relative paths** to `Microsoft.MachineLearningServices/` :\\n\\n| Activity | Subscription-level scope | Resource group-level scope | Workspace-level scope |\\n| ----- | ----- | ----- | ----- |\\n| Create new workspace <sub>1</sub> | Not required | Owner or contributor | N/A (becomes Owner or inherits higher scope role after creation) |\\n| Request subscription level Amlcompute quota or set workspace level quota | Owner, or contributor, or custom role </br>allowing `/locations/updateQuotas/action`</br> at subscription scope | Not Authorized | Not Authorized |\\n| Create new compute cluster | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\n| Create new compute instance | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\n| Submitting any type of run (V1) | Not required | Not required | Owner, contributor, or custom role allowing: `\\\"/workspaces/*/read\\\", \\\"/workspaces/environments/write\\\", \\\"/workspaces/experiments/runs/write\\\", \\\"/workspaces/metadata/artifacts/write\\\", \\\"/workspaces/metadata/snapshots/write\\\", \\\"/workspaces/environments/build/action\\\", \\\"/workspaces/experiments/runs/submit/action\\\", \\\"/workspaces/environments/readSecrets/action\\\"` |\\n| Submitting any type of run (V2) | Not required | Not required | Owner, contributor, or custom role allowing: `\\\"/workspaces/*/read\\\", \\\"/workspaces/environments/write\\\", \\\"/workspaces/jobs/*\\\", \\\"/workspaces/metadata/artifacts/write\\\", \\\"/workspaces/metadata/codes/*/write\\\", \\\"/workspaces/environments/build/action\\\", \\\"/workspaces/environments/readSecrets/action\\\"` |\\n\",\n",
    "      \"metadata\": {\n",
    "        \"sourcefile\": \"UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-108.md\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"page_content\": \"\\n## Troubleshooting\\n\\nHere are a few things to be aware of while you use Azure role-based access control (Azure RBAC):\\n\\n- When you create a resource in Azure, such as a workspace, you're not directly the owner of the resource. Your role is inherited from the highest scope role that you're authorized against in that subscription. As an example if you're a Network Administrator, and have the permissions to create a Machine Learning workspace, you would be assigned the Network Administrator role against that workspace, and not the Owner role.\\n\\n- To perform quota operations in a workspace, you need subscription level permissions. This means setting either subscription level quota or workspace level quota for your managed compute resources can only happen if you have write permissions at the subscription scope.\\n\\n- When there are two role assignments to the same Azure Active Directory user with conflicting sections of Actions/NotActions, your operations listed in NotActions from one role might not take effect if they are also listed as Actions in another role. To learn more about how Azure parses role assignments, read [How Azure RBAC determines if a user has access to a resource](../role-based-access-control/overview.md#how-azure-rbac-determines-if-a-user-has-access-to-a-resource)\\n\\n- To deploy your compute resources inside a VNet, you need to explicitly have permissions for the following actions:\\n    - `Microsoft.Network/virtualNetworks/*/read` on the VNet resources.\\n    - `Microsoft.Network/virtualNetworks/subnets/join/action` on the subnet resource.\\n    \\n    For more information on Azure RBAC with networking, see the [Networking built-in roles](../role-based-access-control/built-in-roles.md#networking).\\n\\n- It can sometimes take up to 1 hour for your new role assignments to take effect over cached permissions across the stack.\\n\\n## Next steps\\n\\n- [Enterprise security overview](concept-enterprise-security.md)\\n- [Virtual network isolation and privacy overview](how-to-network-security-overview.md)\\n- [Tutorial: Train and deploy a model](tutorial-train-deploy-notebook.md)\\n- [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices)\\n\",\n",
    "      \"metadata\": {\n",
    "        \"sourcefile\": \"UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-573.md\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"page_content\": \"\\n## Assign managed identity\\n\\nYou can assign a system- or user-assigned [managed identity](../active-directory/managed-identities-azure-resources/overview.md) to a compute instance, to authenticate against other Azure resources such as storage. Using managed identities for authentication helps improve workspace security and management. For example, you can allow users to access training data only when logged in to a compute instance. Or use a common user-assigned managed identity to permit access to a specific storage account. \\n\\nYou can create compute instance with managed identity from Azure ML Studio:\\n\\n1.\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create).\\n1.\\tSelect **Next: Advanced Settings**.\\n1.\\tEnable **Assign a managed identity**.\\n1.  Select **System-assigned** or **User-assigned** under **Identity type**.\\n1.  If you selected **User-assigned**, select subscription and name of the identity.\\n\\nYou can use V2 CLI to create compute instance with assign system-assigned managed identity:\\n\\n```azurecli\\naz ml compute create --name myinstance --identity-type SystemAssigned --type ComputeInstance --resource-group my-resource-group --workspace-name my-workspace\\n```\\n\\nYou can also use V2 CLI with yaml file, for example to create a compute instance with user-assigned managed identity:\\n\\n```azurecli\\nazure ml compute create --file compute.yaml --resource-group my-resource-group --workspace-name my-workspace\\n```\\n\\nThe identity definition is contained in compute.yaml file:\\n\\n```yaml\\nhttps://azuremlschemas.azureedge.net/latest/computeInstance.schema.json\\nname: myinstance\\ntype: computeinstance\\nidentity:\\n  type: user_assigned\\n  user_assigned_identities: \\n    - resource_id: identity_resource_id\\n```\\n\\nOnce the managed identity is created, grant the managed identity at least Storage Blob Data Reader role on the storage account of the datastore, see [Accessing storage services](how-to-identity-based-service-authentication.md?tabs=cli#accessing-storage-services). Then, when you work on the compute instance, the managed identity is used automatically to authenticate against datastores.\\n\\n> [!NOTE]\\n> The name of the created system managed identity will be in the format /workspace-name/computes/compute-instance-name in your Azure Active Directory. \\n\\nYou can also use the managed identity manually to authenticate against other Azure resources. The following example shows how to use it to get an Azure Resource Manager access token:\\n\\n```python\\nimport requests\\n\\ndef get_access_token_msi(resource):\\n    client_id = os.environ.get(\\\"DEFAULT_IDENTITY_CLIENT_ID\\\", None)\\n    resp = requests.get(f\\\"{os.environ['MSI_ENDPOINT']}?resource={resource}&clientid={client_id}&api-version=2017-09-01\\\", headers={'Secret': os.environ[\\\"MSI_SECRET\\\"]})\\n    resp.raise_for_status()\\n    return resp.json()[\\\"access_token\\\"]\\n\\narm_access_token = get_access_token_msi(\\\"https://management.azure.com\\\")\\n```\\n\\nTo use Azure CLI with the managed identity for authentication, specify the identity client ID as the username when logging in: \\n```azurecli\\naz login --identity --username $DEFAULT_IDENTITY_CLIENT_ID\\n```\\n\\n> [!NOTE]\\n> You cannot use ```azcopy``` when trying to use managed identity. ```azcopy login --identity``` will not work.\\n\\n## Add custom applications such as RStudio or Posit Workbench (preview)\\n\\n> [!IMPORTANT]\\n> Items marked (preview) below are currently in public preview.\\n> The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.\\n> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\\n\\nYou can set up other applications, such as RStudio, or Posit Workbench (formerly RStudio Workbench), when creating a compute instance. Follow these steps in studio to set up a custom application on your compute instance\\n\\n1.\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create)\\n\",\n",
    "      \"metadata\": {\n",
    "        \"sourcefile\": \"UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-534.md\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now set up a labeling project that would show the user the query, the context and the reply and ask them to label the reply as grounded or not grounded. We will need to do that from time to time, but it would be nice to have an automated way to do this -- if fact, GPT-4 does an OK job at this. It is not perfect, but it is good enough to be useful.\n",
    "\n",
    "See here for the code of a groundedness check: [groundedness.py](src/langchain/groundedness.py) -- as you can see the meta_prompt reads very much like a briefing you would give to a human labeler.\n",
    "\n",
    "Now, let's run the groundedness check on the above query and see what it says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running offline, checking secrets\n",
      "{\n",
      "    \"question\": \"how can I see the quota assigned to my workspace in the v2 cli?\",\n",
      "    \"reply\": \"You can use the following Azure CLI command to view the quotas for your Machine Learning workspace:\\n```\\naz ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\\n```\\n\\nReplace `<name>` with the name of the Machine Learning workspace and `<resource-group>` with the resource group that contains the workspace. You can also add `--compute-target` and `--ids` options if you have specific requirements for viewing compute targets or identity information.\",\n",
      "    \"ungrounded_facts\": [],\n",
      "    \"rating_out_of_10\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python src/langchain/groundedness.py --conversation_context data/amldocs/groundedness/conversation_context.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... it does indeed flag the wrong command as an **ungrounded fact** and gives a groundedness rating of 0/10. So, now we can actually measure the groundedness of our `RetrievalQA` chain -- let's run the groundedness check on all the queries in the test set and see how it does. To do this, we are running both [rag_with_cog_search.py](src/langchain/rag_with_cog_search.py) and [groundedness.py](src/langchain/groundedness.py) in batch and we connect the batches in a pipeline -- this is where AzureML comes in handy. \n",
    "\n",
    "Here is the run: [jovial_yogurt_56h2qwvj96](https://ml.azure.com/runs/jovial_yogurt_56h2qwvj96?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/ray/workspaces/ray)\n",
    "\n",
    "Once it has completed, each of the questions have been run through the rag to give us a reply and then the reply has been checked for groundedness. The output of the "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/short-pipeline.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each individual groundedness check is a child run of the groundedness check and we can see the average groundedness value given by GPT-4 as a metric of this run. The average groundedness value is 7.9875 -- it is nothing I would take to the bank, but it is a metric we can look at when we make changes to the rag in order to make it better. After all, that's what we want to do -- we want to systematically make the rag better and better until it is good enough to be useful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different Prompts\n",
    "\n",
    "So, let's try to make it better by changing the prompt used in the RAG. Instead of the default prompt that is being used, let's use one that is asking the model to give us a more concise answer. Instead of the default prompt:\n",
    "\n",
    "#### System_prompt:\n",
    "```text\n",
    "Use the following pieces of context to answer the users question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "```\n",
    "\n",
    "#### User_prompt:\n",
    "```text\n",
    "{question}\n",
    "```\n",
    "\n",
    "We will use the following prompt: [rag_brief.mime](data/amldocs/prompts/rag_brief.mime)\n",
    "\n",
    "#### System_prompt:\n",
    "```text\n",
    "Use the following pieces of context to answer the users question in a brief way. Don't use more than 100 words. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "```\n",
    "\n",
    "#### User_prompt:\n",
    "```text\n",
    "Please give a short answer to the following question. Remember to be brief and not use more than 100 words.\n",
    "And, do not make stuff up -- if you don't know the answer, just say so.\n",
    "{question}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run those two prompts in parallel and compare the results.\n",
    "\n",
    "Here is the run [rate_limit_rerun_loyal_brush_8w6sp6rf9q](https://ml.azure.com/experiments/id/191370e3-619a-48c6-b1ca-09f5498e7e11/runs/45d21b27-0f3d-46ec-86f6-194d8cd1b1a3?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/ray/workspaces/ray&tid=72f988bf-86f1-41af-91ab-2d7cd011db47#)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/long-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
