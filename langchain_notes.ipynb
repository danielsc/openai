{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain with AzureML\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Endpoint: https://aoai.openai.azure.com/\n",
      "MLFlow Tracking URI:  azureml://eastus2.api.azureml.ms/mlflow/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/ray/providers/Microsoft.MachineLearningServices/workspaces/ray\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai, os\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "print(\"OpenAI Endpoint:\", openai.api_base)\n",
    "print(\"MLFlow Tracking URI: \", os.getenv(\"MLFLOW_TRACKING_URI\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a virtual assistant and don't have emotions like humans but I am always ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    ")\n",
    "\n",
    "reply = chat([SystemMessage(content=\"You are a friendly chat agent.\"),\n",
    "              HumanMessage(content=\"Hello, how are you?\")])\n",
    "\n",
    "print(reply.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion Model\n",
    "\n",
    "### Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "text=\"\"\"\n",
    "Classify the following news headline into 1 of the following categories: Business, Tech, Politics, Sport, Entertainment\n",
    "\n",
    "Headline: Major Retailer Announces Plans to Close Over 100 Stores\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text=\"\"\"\n",
    "Classify the following news headline into 1 of the following categories: Business, Tech, Politics, Sport, Entertainment\n",
    "\n",
    "Headline 1: Donna Steffensen Is Cooking Up a New Kind of Perfection. The Internetâ€™s most beloved cooking guru has a buzzy new book and a fresh new perspective\n",
    "Category: Entertainment\n",
    "\n",
    "Headline 2: Major Retailer Announces Plans to Close Over 100 Stores\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The answer is x = 4.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text=\"\"\"\n",
    "what is the anwer to this equation: 3x - 7 = -2x + 5\n",
    "\"\"\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12/5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "x = symbols('x')\n",
    "solve(3*x - 7 + 2*x - 5, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Add 7 to both sides of the equation.\n",
      "3x - 7 + 7 = -2x + 5 + 7\n",
      "\n",
      "Step 2: Simplify the left side of the equation.\n",
      "3x = -2x + 12\n",
      "\n",
      "Step 3: Subtract -2x from both sides of the equation.\n",
      "3x - (-2x) = -2x + 12 - (-2x)\n",
      "\n",
      "Step 4: Simplify the left side of the equation.\n",
      "5x = 12\n",
      "\n",
      "Step 5: Divide both sides of the equation by 5.\n",
      "x = 12/5\n",
      "\n",
      "Step 6: Simplify the right side of the equation.\n",
      "x = 2.4\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text=\"\"\"\n",
    "solve the following algebraic equation step-by-step and explain you reasoning at each step: 3x - 7 = -2x + 5\n",
    "\"\"\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/chains/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_call: {'module': 'langchain.chat_models.openai', 'name': '_generate', 'args': (AzureChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='', request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='gpt-35-turbo', openai_api_type='azure', openai_api_base='', openai_api_version='2023-03-15-preview'), [HumanMessage(content='What is a good name for a company that makes organic snacks?', additional_kwargs={})]), 'kwargs': {'stop': None}}\n",
      "json_result: generations=[ChatGeneration(text='Here are 5 potential names: \\n\\n1. EarthBites \\n2. Purely Nosh \\n3. Wholesome Munchies \\n4. FreshFusion \\n5. NatureNibbles', generation_info=None, message=AIMessage(content='Here are 5 potential names: \\n\\n1. EarthBites \\n2. Purely Nosh \\n3. Wholesome Munchies \\n4. FreshFusion \\n5. NatureNibbles', additional_kwargs={}))] llm_output={'token_usage': <OpenAIObject at 0x7fa0008de6b0> JSON: {\n",
      "  \"completion_tokens\": 41,\n",
      "  \"prompt_tokens\": 21,\n",
      "  \"total_tokens\": 62\n",
      "}, 'model_name': 'gpt-3.5-turbo'}\n",
      "Here are 5 potential names: \n",
      "\n",
      "1. EarthBites \n",
      "2. Purely Nosh \n",
      "3. Wholesome Munchies \n",
      "4. FreshFusion \n",
      "5. NatureNibbles\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"organic snacks\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Langchain\n",
    "\n",
    "The first thing I did when I started to work with Langchain was to patch it to get more logging information. Langchain does a great job at abstracting away what is really going on under the hood, in particular when working with more complex chains or agents. However, this abstraction comes at the cost of not being able to see what is really going on.\n",
    "\n",
    "Instead of patching the code, I could have also use a callback, but I found that monkey patching was easier to use since I didn't have to change the code I was running -- for instance, I could just run an off-the-shelf agent and get the logging information.\n",
    "\n",
    "What my patch does is to add wrap some key functions that langchain uses and log the parameters that go into them and the value that is returned by them to MLFlow as an artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.langchain.patch import patch_langchain\n",
    "\n",
    "patch_langchain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "json_call: {'module': 'langchain.llms.openai', 'name': '_generate', 'args': (AzureOpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, deployment_name='text-davinci-003'), [\"Answer the following questions as best you can. You have access to the following tools:\\n\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: I have 1000 dollars. given today's governement bond rate, how much money would I have in 2 years if I invested them in 2 year government bonds?\\nThought:\"]), 'kwargs': {'stop': ['\\nObservation:', '\\n\\tObservation:']}}\n",
      "json_result: generations=[[Generation(text=' I need to know the interest rate of the bonds\\nAction: Search\\nAction Input: \"2 year government bond rate\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 208, 'total_tokens': 233, 'completion_tokens': 25}, 'model_name': 'text-davinci-003'}\n",
      "\u001b[32;1m\u001b[1;3m I need to know the interest rate of the bonds\n",
      "Action: Search\n",
      "Action Input: \"2 year government bond rate\"\u001b[0mjson_call: {'module': 'langchain.utilities.serpapi', 'name': 'run', 'args': (SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='22b4aa755585902707d009ef0baef623083911b8323fb6d9ae5f36b1482e3782', aiosession=None), '2 year government bond rate'), 'kwargs': {}}\n",
      "json_result: 2 Year Treasury Rate is at 4.08%, compared to 3.96% the previous market day and 2.47% last year. This is higher than the long term average of 3.16%. The 2 Year Treasury Rate is the yield received for investing in a US government issued treasury security that has a maturity of 2 years.\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m2 Year Treasury Rate is at 4.08%, compared to 3.96% the previous market day and 2.47% last year. This is higher than the long term average of 3.16%. The 2 Year Treasury Rate is the yield received for investing in a US government issued treasury security that has a maturity of 2 years.\u001b[0m\n",
      "Thought:json_call: {'module': 'langchain.llms.openai', 'name': '_generate', 'args': (AzureOpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, deployment_name='text-davinci-003'), ['Answer the following questions as best you can. You have access to the following tools:\\n\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: I have 1000 dollars. given today\\'s governement bond rate, how much money would I have in 2 years if I invested them in 2 year government bonds?\\nThought: I need to know the interest rate of the bonds\\nAction: Search\\nAction Input: \"2 year government bond rate\"\\nObservation: 2 Year Treasury Rate is at 4.08%, compared to 3.96% the previous market day and 2.47% last year. This is higher than the long term average of 3.16%. The 2 Year Treasury Rate is the yield received for investing in a US government issued treasury security that has a maturity of 2 years.\\nThought:']), 'kwargs': {'stop': ['\\nObservation:', '\\n\\tObservation:']}}\n",
      "json_result: generations=[[Generation(text=' I need to calculate the amount of money I will have in 2 years\\nAction: Calculator\\nAction Input: 1000 * (1 + 0.0408)^2', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 308, 'total_tokens': 342, 'completion_tokens': 34}, 'model_name': 'text-davinci-003'}\n",
      "\u001b[32;1m\u001b[1;3m I need to calculate the amount of money I will have in 2 years\n",
      "Action: Calculator\n",
      "Action Input: 1000 * (1 + 0.0408)^2\u001b[0mjson_call: {'module': 'langchain.chains.base', 'name': 'run', 'args': (LLMMathChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, verbose=False, llm=AzureOpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, deployment_name='text-davinci-003'), prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into Python code that can be executed in Python 3 REPL. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```python\\n${{Code that solves the problem and prints the solution}}\\n```\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n\\n```python\\nprint(37593 * 67)\\n```\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), input_key='question', output_key='answer'), '1000 * (1 + 0.0408)^2'), 'kwargs': {}}\n",
      "json_call: {'module': 'langchain.llms.openai', 'name': '_generate', 'args': (AzureOpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, deployment_name='text-davinci-003'), ['Translate a math problem into Python code that can be executed in Python 3 REPL. Use the output of running this code to answer the question.\\n\\nQuestion: ${Question with math problem.}\\n```python\\n${Code that solves the problem and prints the solution}\\n```\\n```output\\n${Output of running the code}\\n```\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n\\n```python\\nprint(37593 * 67)\\n```\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 1000 * (1 + 0.0408)^2\\n']), 'kwargs': {'stop': ['```output']}}\n",
      "json_result: generations=[[Generation(text='\\n```python\\nprint(1000 * (1 + 0.0408)**2)\\n```\\n', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 144, 'total_tokens': 168, 'completion_tokens': 24}, 'model_name': 'text-davinci-003'}\n",
      "json_call: {'module': 'langchain.python', 'name': 'run', 'args': (PythonREPL(globals={}, locals={}), '\\nprint(1000 * (1 + 0.0408)**2)'), 'kwargs': {}}\n",
      "json_result: 1083.2646399999999\n",
      "\n",
      "json_result: Answer: 1083.2646399999999\n",
      "\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1083.2646399999999\n",
      "\u001b[0m\n",
      "Thought:json_call: {'module': 'langchain.llms.openai', 'name': '_generate', 'args': (AzureOpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, deployment_name='text-davinci-003'), ['Answer the following questions as best you can. You have access to the following tools:\\n\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: I have 1000 dollars. given today\\'s governement bond rate, how much money would I have in 2 years if I invested them in 2 year government bonds?\\nThought: I need to know the interest rate of the bonds\\nAction: Search\\nAction Input: \"2 year government bond rate\"\\nObservation: 2 Year Treasury Rate is at 4.08%, compared to 3.96% the previous market day and 2.47% last year. This is higher than the long term average of 3.16%. The 2 Year Treasury Rate is the yield received for investing in a US government issued treasury security that has a maturity of 2 years.\\nThought: I need to calculate the amount of money I will have in 2 years\\nAction: Calculator\\nAction Input: 1000 * (1 + 0.0408)^2\\nObservation: Answer: 1083.2646399999999\\n\\nThought:']), 'kwargs': {'stop': ['\\nObservation:', '\\n\\tObservation:']}}\n",
      "json_result: generations=[[Generation(text=' I now know the final answer\\nFinal Answer: In two years, I will have 1083.26 dollars if I invest 1000 dollars in 2 year government bonds.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 360, 'total_tokens': 393, 'completion_tokens': 33}, 'model_name': 'text-davinci-003'}\n",
      "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: In two years, I will have 1083.26 dollars if I invest 1000 dollars in 2 year government bonds.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In two years, I will have 1083.26 dollars if I invest 1000 dollars in 2 year government bonds.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI, AzureOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import openai, os\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "agent.run(\"I have 1000 dollars. given today's governement bond rate, how much money would I have in 2 years if I invested them in 2 year government bonds?\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Retrieval Augmented Generation\"-Style Chain\n",
    "\n",
    "The main task used here is a simple question answering task on AzureML. The user asks a question and the system answers it. In order to answer the question, the system needs to retrieve some context from a cognitive services search index which is then used to generate the answer. \n",
    "\n",
    "Langchain has the `RetrievaQA` chain to easily build a system that does this -- all I need is Retriever that implements the `get_relevant_documents` method that returns a list of documents relevant to the user's query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "class CognitiveSearchRetriever(BaseRetriever):\n",
    "    def __init__(self, endpoint: str, index_name: str, searchkey: str, top: int = 3):\n",
    "        self.endpoint = endpoint\n",
    "        self.index_name = index_name\n",
    "        self.searchkey = searchkey\n",
    "        self.top = top\n",
    "        self.client = SearchClient(endpoint=endpoint, index_name=index_name, credential=AzureKeyCredential(searchkey))\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        docs = []\n",
    "        for i in self.client.search(query, top=self.top):\n",
    "            docs.append(Document(page_content=i['content'], metadata={\"sourcefile\": i['sourcefile']}))\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use the above class to query the search index and generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='   > You can publish multiple pipelines to a single endpoint. Each pipeline in a given endpoint is given a version number, which you can specify when you call the pipeline endpoint.\\n\\n1. Select **Publish**.\\n\\n## Retrain your model\\n\\nNow that you have a published training pipeline, you can use it to retrain your model on new data. You can submit jobs from a pipeline endpoint from the studio workspace or programmatically.\\n\\n### Submit jobs by using the studio portal\\n\\nUse the following steps to submit a parameterized pipeline endpoint job from the studio portal:\\n\\n1. Go to the **Endpoints** page in your studio workspace.\\n1. Select the **Pipeline endpoints** tab. Then, select your pipeline endpoint.\\n1. Select the **Published pipelines** tab. Then, select the pipeline version that you want to run.\\n1. Select **Submit**.\\n1. In the setup dialog box, you can specify the parameters values for the job. For this example, update the data path to train your model using a non-US dataset.\\n\\n![Screenshot that shows how to set up a parameterized pipeline job in the designer](./media/how-to-retrain-designer/published-pipeline-run.png)\\n\\n### Submit jobs by using code\\n\\nYou can find the REST endpoint of a published pipeline in the overview panel. By calling the endpoint, you can retrain the published pipeline.\\n\\nTo make a REST call, you need an OAuth 2.0 bearer-type authentication header. For information about setting up authentication to your workspace and making a parameterized REST call, see [Build an Azure Machine Learning pipeline for batch scoring](tutorial-pipeline-batch-scoring-classification.md#publish-and-run-from-a-rest-endpoint).\\n\\n## Next steps\\n\\nIn this article, you learned how to create a parameterized training pipeline endpoint using the designer.\\n\\nFor a complete walkthrough of how you can deploy a model to make predictions, see the [designer tutorial](tutorial-designer-automobile-price-train-score.md) to train and deploy a regression model.\\n\\nFor how to publish and submit a job to pipeline endpoint using the SDK v1, see [this article](v1/how-to-deploy-pipelines.md).\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-retrain-designer-74.md'}),\n",
       " Document(page_content='\\n# Create and run machine learning pipelines using components with the Azure Machine Learning studio\\n\\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\\n\\nIn this article, you\\'ll learn how to create and run [machine learning pipelines](concept-ml-pipelines.md) by using the Azure Machine Learning studio and [Components](concept-component.md). You can create pipelines without using components, but components offer better amount of flexibility and reuse. Azure ML Pipelines may be defined in YAML and [run from the CLI](how-to-create-component-pipelines-cli.md), [authored in Python](how-to-create-component-pipeline-python.md), or composed in Azure ML Studio Designer with a drag-and-drop UI. This document focuses on the AzureML studio designer UI.\\n\\n## Prerequisites\\n\\n* If you don\\'t have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\\n\\n* An Azure Machine Learning workspace[Create workspace resources](quickstart-create-resources.md).\\n\\n* [Install and set up the Azure CLI extension for Machine Learning](how-to-configure-cli.md).\\n\\n* Clone the examples repository:\\n\\n    ```azurecli-interactive\\n    git clone https://github.com/Azure/azureml-examples --depth 1\\n    cd azureml-examples/cli/jobs/pipelines-with-components/\\n    ```\\n\\n## Register component in your workspace\\n\\n>[!Note]\\n> Designer supports two type of components, classic prebuilt components and custom components. These two types of components are not compatible.  \\n>\\n>Classic prebuilt components provides prebuilt components majorly for data processing and traditional machine learning tasks like regression and classification. This type of component continues to be supported but will not have any new components added.\\n>\\n>\\n>Custom components allow you to provide your own code as a component. It supports sharing across workspaces and seamless authoring across Studio, CLI, and SDK interfaces.\\n>\\n>This article applies to custom components. \\n\\nTo build pipeline using components in UI, you need to register components to your workspace first. You can use CLI or SDK to register components to your workspace, so that you can share and reuse the component within the workspace. Registered components support automatic versioning so you can update the component but assure that pipelines that require an older version will continue to work.  \\n\\nIn the example below take using CLI for example. If you want to learn more about how to build a component, see [Create and run pipelines using components with  CLI](how-to-create-component-pipelines-cli.md).\\n\\n1. From the `cli/jobs/pipelines-with-components/basics` directory of the [`azureml-examples` repository](https://github.com/Azure/azureml-examples), navigate to the `1b_e2e_registered_components` subdirectory.\\n\\n1. Register the components to AzureML workspace using following commands. Learn more about [ML components](concept-component.md).\\n\\n    ```CLI\\n    az ml component create --file train.yml\\n    az ml component create --file score.yml\\n    az ml component create --file eval.yml\\n    ```\\n\\n1. After register component successfully, you can see your component in the studio UI.\\n\\n:::image type=\"content\" source=\"./media/how-to-create-component-pipelines-ui/component-page.png\" alt-text=\"Screenshot showing registered component in component page.\" lightbox =\"./media/how-to-create-component-pipelines-ui/component-page.png\":::\\n\\n## Create pipeline using registered component\\n\\n1. Create a new pipeline in the designer.\\n\\n    :::image type=\"content\" source=\"./media/how-to-create-component-pipelines-ui/new-pipeline.png\" alt-text=\"Screenshot showing creating new pipeline in designer homepage.\" lightbox =\"./media/how-to-create-component-pipelines-ui/new-pipeline.png\":::\\n\\n1. Set the default compute target of the pipeline. \\n\\n    Select the **Gear icon** ![Screenshot of the gear icon that is in the UI.](./media/tutorial-designer-automobile-price-train-score/gear-icon.png) at the top right of the canvas to open the **Settings** pane. Select the default compute target for your pipeline.\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-component-pipelines-ui-0.md'}),\n",
       " Document(page_content='\\n# Use GitHub Actions with Azure Machine Learning\\n[!INCLUDE [v2](../../includes/machine-learning-dev-v2.md)]\\nGet started with [GitHub Actions](https://docs.github.com/en/actions) to train a model on Azure Machine Learning. \\n\\nThis article will teach you how to create a GitHub Actions workflow that builds and deploys a machine learning model to [Azure Machine Learning](./overview-what-is-azure-machine-learning.md). You\\'ll train a [scikit-learn](https://scikit-learn.org/) linear regression model on the NYC Taxi dataset. \\n\\nGitHub Actions uses a workflow YAML (.yml) file in the `/.github/workflows/` path in your repository. This definition contains the various steps and parameters that make up the workflow.\\n\\n\\n## Prerequisites\\n\\n[!INCLUDE [sdk](../../includes/machine-learning-sdk-v2-prereqs.md)]\\n\\n* A GitHub account. If you don\\'t have one, sign up for [free](https://github.com/join).  \\n\\n## Step 1. Get the code\\n\\nFork the following repo at GitHub:\\n\\n```\\nhttps://github.com/azure/azureml-examples\\n```\\n\\n## Step 2. Authenticate with Azure\\n\\nYou\\'ll need to first define how to authenticate with Azure. You can use a [service principal](../active-directory/develop/app-objects-and-service-principals.md#service-principal-object) or [OpenID Connect](https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect). \\n\\n### Generate deployment credentials\\n\\n[!INCLUDE [include](~/articles/reusable-content/github-actions/generate-deployment-credentials.md)]\\n\\n### Create secrets\\n\\n[!INCLUDE [include](~/articles/reusable-content/github-actions/create-secrets-with-openid.md)]\\n\\n## Step 3. Update `setup.sh` to connect to your Azure Machine Learning workspace\\n\\nYou\\'ll need to update the CLI setup file variables to match your workspace. \\n\\n1. In your cloned repository, go to `azureml-examples/cli/`. \\n1. Edit `setup.sh` and update these variables in the file. \\n   \\n    |Variable  | Description  |\\n    |---------|---------|\\n    |GROUP     |      Name of resource group    |\\n    |LOCATION     |    Location of your workspace (example: `eastus2`)    |\\n    |WORKSPACE     |     Name of Azure ML workspace     | \\n\\n## Step 4. Update `pipeline.yml` with your compute cluster name\\n\\nYou\\'ll use a `pipeline.yml` file to deploy your Azure ML pipeline. This is a machine learning pipeline and not a DevOps pipeline. You only need to make this update if you\\'re using a name other than `cpu-cluster` for your computer cluster name. \\n\\n1. In your cloned repository, go to `azureml-examples/cli/jobs/pipelines/nyc-taxi/pipeline.yml`. \\n1. Each time you see `compute: azureml:cpu-cluster`, update the value of `cpu-cluster` with your compute cluster name. For example, if your cluster is named `my-cluster`, your new value would be `azureml:my-cluster`. There are five updates.\\n\\n## Step 5: Run your GitHub Actions workflow\\n\\nYour workflow authenticates with Azure, sets up the Azure Machine Learning CLI, and uses the CLI to train a model in Azure Machine Learning. \\n\\n# [Service principal](#tab/userlevel)\\n\\n\\nYour workflow file is made up of a trigger section and jobs:\\n- A trigger starts the workflow in the `on` section. The workflow runs by default on a cron schedule and when a pull request is made from matching branches and paths. Learn more about [events that trigger workflows](https://docs.github.com/actions/using-workflows/events-that-trigger-workflows). \\n- In the jobs section of the workflow, you checkout code and log into Azure with your service principal secret.\\n- The jobs section also includes a setup action that installs and sets up the [Machine Learning CLI (v2)](how-to-configure-cli.md). Once the CLI is installed, the run job action runs your Azure Machine Learning `pipeline.yml` file to train a model with NYC taxi data.\\n\\n\\n### Enable your workflow\\n\\n1. In your cloned repository, open `.github/workflows/cli-jobs-pipelines-nyc-taxi-pipeline.yml` and verify that your workflow looks like this. \\n\\n    ```yaml\\n    name: cli-jobs-pipelines-nyc-taxi-pipeline\\n    on:\\n      workflow_dispatch:\\n      schedule:\\n        - cron: \"0 0/4 * * *\"\\n      pull_request:\\n        branches:\\n          - main\\n          - sdk-preview\\n        paths:\\n          - cli/jobs/pipelines/nyc-taxi/**\\n          - .github/workflows/cli-jobs-pipelines-nyc-taxi-pipeline.yml\\n          - cli/run-pipeline-jobs.sh\\n          - cli/setup.sh\\n    jobs:\\n      build:\\n        runs-on: ubuntu-latest\\n        steps:\\n        - name: check out repo\\n          uses: actions/checkout@v2\\n        - name: azure login\\n          uses: azure/login@v1\\n          with:\\n            creds: ${{secrets.AZ_CREDS}}\\n        - name: setup\\n          run: bash setup.sh\\n          working-directory: cli\\n          continue-on-error: true\\n        - name: run job\\n          run: bash -x ../../../run-job.sh pipeline.yml\\n          working-directory: cli/jobs/pipelines/nyc-taxi\\n    ```\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-github-actions-machine-learning-0.md'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cog_search = CognitiveSearchRetriever(endpoint=os.getenv(\"COG_SEARCH_ENDPOINT\"), index_name=\"amldocs\", searchkey=os.environ[\"COG_SEARCH_KEY\"])\n",
    "cog_search.get_relevant_documents(\"pipelines\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it takes only a few lines to build a chain that uses that search index to answer the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                    chain_type=\"stuff\",\n",
    "                                    retriever=cog_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is a pipeline?',\n",
       " 'result': 'In the context of this article, a pipeline is a sequence of steps to complete a task in a systematic manner. In Azure Machine Learning context, a pipeline refers to machine learning pipelines which stitch together all the steps in your model training process, such as data preparation, feature extraction, hyperparameter tuning and model evaluation. Machine Learning pipelines can also encompass more than just the training of models.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"What is a pipeline?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here for the full source code of the chain: [rag_with_cog_search.py](src/langchain/rag_with_cog_search.py)\n",
    "\n",
    "Here is a link to a notebook that shows how to use the chain: [tender_carnival_zjr2f2kw](https://ml.azure.com/experiments/id/3b32df62-b496-4f39-8498-2f08f7258493/runs/acebb051-af91-408e-8f7f-edfec7da6101?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/ray/workspaces/ray&tid=72f988bf-86f1-41af-91ab-2d7cd011db47#overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running offline, checking secrets\n",
      "json_call: {'module': '__main__', 'name': 'get_relevant_documents', 'args': (<__main__.CognitiveSearchRetriever object at 0x7fe869ae4880>, 'how can I set up a compute cluster?'), 'kwargs': {}}\n",
      "cog_search_top: 3\n",
      "cog_search_query: how can I set up a compute cluster?\n",
      "cog_search_docs: [Document(page_content='\\n# Create an Azure Machine Learning compute cluster\\n\\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\\n\\n> [!div class=\"op_single_selector\" title1=\"Select the Azure Machine Learning CLI or SDK version you are using:\"]\\n> * [v1](v1/how-to-create-attach-compute-cluster.md)\\n> * [v2 (current version)](how-to-create-attach-compute-cluster.md)\\n\\nLearn how to create and manage a [compute cluster](concept-compute-target.md#azure-machine-learning-compute-managed) in your Azure Machine Learning workspace.\\n\\nYou can use Azure Machine Learning compute cluster to distribute a training or batch inference process across a cluster of CPU or GPU compute nodes in the cloud. For more information on the VM sizes that include GPUs, see [GPU-optimized virtual machine sizes](../virtual-machines/sizes-gpu.md). \\n\\nIn this article, learn how to:\\n\\n* Create a compute cluster\\n* Lower your compute cluster cost with low priority VMs\\n* Set up a [managed identity](../active-directory/managed-identities-azure-resources/overview.md) for the cluster\\n\\n## Prerequisites\\n\\n* An Azure Machine Learning workspace. For more information, see [Create an Azure Machine Learning workspace](how-to-manage-workspace.md).\\n\\n* The [Azure CLI extension for Machine Learning service (v2)](reference-azure-machine-learning-cli.md), [Azure Machine Learning Python SDK](/python/api/overview/azure/ai-ml-readme), or the [Azure Machine Learning Visual Studio Code extension](how-to-setup-vs-code.md).\\n\\n* If using the Python SDK, [set up your development environment with a workspace](how-to-configure-environment.md).  Once your environment is set up, attach to the workspace in your Python script:\\n\\n    [!INCLUDE [connect ws v2](../../includes/machine-learning-connect-ws-v2.md)]\\n\\n\\n## What is a compute cluster?\\n\\nAzure Machine Learning compute cluster is a managed-compute infrastructure that allows you to easily create a single or multi-node compute. The compute cluster is a resource that can be shared with other users in your workspace. The compute scales up automatically when a job is submitted, and can be put in an Azure Virtual Network. Compute cluster supports **no public IP** deployment as well in virtual network. The compute executes in a containerized environment and packages your model dependencies in a [Docker container](https://www.docker.com/why-docker).\\n\\nCompute clusters can run jobs securely in a [virtual network environment](how-to-secure-training-vnet.md), without requiring enterprises to open up SSH ports. The job executes in a containerized environment and packages your model dependencies in a Docker container. \\n\\n## Limitations\\n\\n* Some of the scenarios listed in this document are marked as __preview__. Preview functionality is provided without a service level agreement, and it\\'s not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\\n\\n* Compute clusters can be created in a different region than your workspace. This functionality is in __preview__, and is only available for __compute clusters__, not compute instances. This preview isn\\'t available if you\\'re using a private endpoint-enabled workspace. \\n\\n    > [!WARNING]\\n    > When using a compute cluster in a different region than your workspace or datastores, you may see increased network latency and data transfer costs. The latency and costs can occur when creating the cluster, and when running jobs on it.\\n\\n* We currently support only creation (and not updating) of clusters through [ARM templates](/azure/templates/microsoft.machinelearningservices/workspaces/computes). For updating compute, we recommend using the SDK, Azure CLI or UX for now.\\n\\n* Azure Machine Learning Compute has default limits, such as the number of cores that can be allocated. For more information, see [Manage and request quotas for Azure resources](how-to-manage-quotas.md).\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-attach-compute-cluster-0.md'}), Document(page_content='> To learn more about which VM family to request a quota increase for, check out [virtual machine sizes in Azure](../virtual-machines/sizes.md). For instance GPU VM families start with an \"N\" in their family name (eg. NCv3 series)\\n\\nThe following table shows more limits in the platform. Reach out to the AzureML product team through a **technical** support ticket to request an exception.\\n\\n| **Resource or Action** | **Maximum limit** |\\n| --- | --- |\\n| Workspaces per resource group | 800 |\\n| Nodes in a single Azure Machine Learning Compute (AmlCompute) **cluster** set up as a non communication-enabled pool (that is, can\\'t run MPI jobs) | 100 nodes but configurable up to 65,000 nodes |\\n| Nodes in a single Parallel Run Step **run** on an Azure Machine Learning Compute (AmlCompute) cluster | 100 nodes but configurable up to 65,000 nodes if your cluster is set up to scale per above |\\n| Nodes in a single Azure Machine Learning Compute (AmlCompute) **cluster** set up as a communication-enabled pool | 300 nodes but configurable up to 4000 nodes |\\n| Nodes in a single Azure Machine Learning Compute (AmlCompute) **cluster** set up as a communication-enabled pool on an RDMA enabled VM Family | 100 nodes |\\n| Nodes in a single MPI **run** on an Azure Machine Learning Compute (AmlCompute) cluster | 100 nodes but can be increased to 300 nodes |\\n| Job lifetime | 21 days<sup>1</sup> |\\n| Job lifetime on a low-priority node | 7 days<sup>2</sup> |\\n| Parameter servers per node | 1 |\\n\\n<sup>1</sup> Maximum lifetime is the duration between when a job starts and when it finishes. Completed jobs persist indefinitely. Data for jobs not completed within the maximum lifetime isn\\'t accessible.\\n\\n<sup>2</sup> Jobs on a low-priority node can be preempted whenever there\\'s a capacity constraint. We recommend that you implement checkpoints in your job.\\n\\n### Azure Machine Learning managed online endpoints\\n\\nAzure Machine Learning managed online endpoints have limits described in the following table. \\n\\n| **Resource** | **Limit** |\\n| --- | --- |\\n| Endpoint name| Endpoint names must <li> Begin with a letter <li> Be 3-32 characters in length  <li> Only consist of letters and numbers <sup>1</sup> |\\n| Deployment name| Deployment names must <li> Begin with a letter <li> Be 3-32 characters in length  <li>  Only consist of letters and numbers <sup>1</sup> |\\n| Number of endpoints per subscription | 50 |\\n| Number of deployments per subscription | 200 |\\n| Number of deployments per endpoint | 20 |\\n| Number of instances per deployment | 20 <sup>2</sup> |\\n| Max request time-out at endpoint level  | 90 seconds |\\n| Total requests per second at endpoint level for all deployments  | 500 <sup>3</sup> |\\n| Total connections per second at endpoint level for all deployments  | 500 <sup>3</sup> |\\n| Total connections active at endpoint level for all deployments  | 500 <sup>3</sup> |\\n| Total bandwidth at endpoint level for all deployments  | 5 MBPS <sup>3</sup> |\\n\\n<sup>1</sup> Single dashes like, `my-endpoint-name`, are accepted in endpoint and deployment names.\\n\\n<sup>2</sup> We reserve 20% extra compute resources for performing upgrades. For example, if you request 10 instances in a deployment, you must have a quota for 12. Otherwise, you\\'ll receive an error.\\n\\n<sup>3</sup> If you request a limit increase, be sure to calculate related limit increases you might need. For example, if you request a limit increase for requests per second, you might also want to compute the required connections and bandwidth limits and include these limit increases in the same request.\\n\\nTo determine the current usage for an endpoint, [view the metrics](how-to-monitor-online-endpoints.md#metrics). \\n\\nTo request an exception from the Azure Machine Learning product team, use the steps in the [Request quota increases](#request-quota-increases).\\n\\n\\n### Azure Machine Learning pipelines\\n[Azure Machine Learning pipelines](concept-ml-pipelines.md) have the following limits.\\n\\n| **Resource** | **Limit** |\\n| --- | --- |\\n| Steps in a pipeline | 30,000 |\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-quotas-66.md'}), Document(page_content=\"\\n# Set up a development environment with Azure Databricks and AutoML in Azure Machine Learning \\n\\nLearn how to configure a development environment in Azure Machine Learning that uses Azure Databricks and automated ML.\\n\\nAzure Databricks is ideal for running large-scale intensive machine learning workflows on the scalable Apache Spark platform in the Azure cloud. It provides a collaborative Notebook-based environment with a CPU or GPU-based compute cluster.\\n\\nFor information on other machine learning development environments, see [Set up Python development environment](how-to-configure-environment.md).\\n\\n\\n## Prerequisite\\n\\nAzure Machine Learning workspace. To create one, use the steps in the [Create workspace resources](quickstart-create-resources.md) article.\\n\\n\\n## Azure Databricks with Azure Machine Learning and AutoML\\n\\nAzure Databricks integrates with Azure Machine Learning and its AutoML capabilities. \\n\\nYou can use Azure Databricks:\\n\\n+ To train a model using Spark MLlib and deploy the model to ACI/AKS.\\n+ With [automated machine learning](concept-automated-ml.md) capabilities using an Azure ML SDK.\\n+ As a compute target from an [Azure Machine Learning pipeline](concept-ml-pipelines.md).\\n\\n## Set up a Databricks cluster\\n\\nCreate a [Databricks cluster](/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal). Some settings apply only if you install the SDK for automated machine learning on Databricks.\\n\\n**It takes few minutes to create the cluster.**\\n\\nUse these settings:\\n\\n| Setting |Applies to| Value |\\n|----|---|---|\\n| Cluster Name |always| yourclustername |\\n| Databricks Runtime Version |always| 9.1 LTS|\\n| Python version |always| 3 |\\n| Worker Type <br>(determines max # of concurrent iterations) |Automated ML<br>only| Memory optimized VM preferred |\\n| Workers |always| 2 or higher |\\n| Enable Autoscaling |Automated ML<br>only| Uncheck |\\n\\nWait until the cluster is running before proceeding further.\\n\\n## Add the Azure ML SDK to Databricks\\n\\nOnce the cluster is running, [create a library](https://docs.databricks.com/user-guide/libraries.html#create-a-library) to attach the appropriate Azure Machine Learning SDK package to your cluster. \\n\\nTo use automated ML, skip to [Add the Azure ML SDK with AutoML](#add-the-azure-ml-sdk-with-automl-to-databricks).\\n\\n\\n1. Right-click the current Workspace folder where you want to store the library. Select **Create** > **Library**.\\n    \\n    > [!TIP]\\n    > If you have an old SDK version, deselect it from cluster's installed libraries and move to trash. Install the new SDK version and restart the cluster. If there is an issue after the restart, detach and reattach your cluster.\\n\\n1. Choose the following option (no other SDK installations are supported)\\n\\n   |SDK&nbsp;package&nbsp;extras|Source|PyPi&nbsp;Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\\n   |----|---|---|\\n   |For Databricks| Upload Python Egg or PyPI | azureml-sdk[databricks]|\\n\\n   > [!WARNING]\\n   > No other SDK extras can be installed. Choose only the [`databricks`] option .\\n\\n   * Do not select **Attach automatically to all clusters**.\\n   * Select  **Attach** next to your cluster name.\\n\\n1. Monitor for errors until status changes to **Attached**, which may take several minutes.  If this step fails:\\n\\n   Try restarting your cluster by:\\n   1. In the left pane, select **Clusters**.\\n   1. In the table, select your cluster name.\\n   1. On the **Libraries** tab, select **Restart**.\\n\\n   A successful install looks like the following: \\n\\n  ![Azure Machine Learning SDK for Databricks](./media/how-to-configure-environment/amlsdk-withoutautoml.jpg) \\n\\n## Add the Azure ML SDK with AutoML to Databricks\\nIf the cluster was created with Databricks Runtime 7.3 LTS (*not* ML), run the following command in the first cell of your notebook to install the AzureML SDK.\\n\\n```\\n%pip install --upgrade --force-reinstall -r https://aka.ms/automl_linux_requirements.txt\\n```\\n\\n### AutoML config settings\\n\\nIn AutoML config, when using Azure Databricks add the following parameters:\\n\\n\", metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-configure-databricks-automl-environment-0.md'})]\n",
      "json_result: [Document(page_content='\\n# Create an Azure Machine Learning compute cluster\\n\\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\\n\\n> [!div class=\"op_single_selector\" title1=\"Select the Azure Machine Learning CLI or SDK version you are using:\"]\\n> * [v1](v1/how-to-create-attach-compute-cluster.md)\\n> * [v2 (current version)](how-to-create-attach-compute-cluster.md)\\n\\nLearn how to create and manage a [compute cluster](concept-compute-target.md#azure-machine-learning-compute-managed) in your Azure Machine Learning workspace.\\n\\nYou can use Azure Machine Learning compute cluster to distribute a training or batch inference process across a cluster of CPU or GPU compute nodes in the cloud. For more information on the VM sizes that include GPUs, see [GPU-optimized virtual machine sizes](../virtual-machines/sizes-gpu.md). \\n\\nIn this article, learn how to:\\n\\n* Create a compute cluster\\n* Lower your compute cluster cost with low priority VMs\\n* Set up a [managed identity](../active-directory/managed-identities-azure-resources/overview.md) for the cluster\\n\\n## Prerequisites\\n\\n* An Azure Machine Learning workspace. For more information, see [Create an Azure Machine Learning workspace](how-to-manage-workspace.md).\\n\\n* The [Azure CLI extension for Machine Learning service (v2)](reference-azure-machine-learning-cli.md), [Azure Machine Learning Python SDK](/python/api/overview/azure/ai-ml-readme), or the [Azure Machine Learning Visual Studio Code extension](how-to-setup-vs-code.md).\\n\\n* If using the Python SDK, [set up your development environment with a workspace](how-to-configure-environment.md).  Once your environment is set up, attach to the workspace in your Python script:\\n\\n    [!INCLUDE [connect ws v2](../../includes/machine-learning-connect-ws-v2.md)]\\n\\n\\n## What is a compute cluster?\\n\\nAzure Machine Learning compute cluster is a managed-compute infrastructure that allows you to easily create a single or multi-node compute. The compute cluster is a resource that can be shared with other users in your workspace. The compute scales up automatically when a job is submitted, and can be put in an Azure Virtual Network. Compute cluster supports **no public IP** deployment as well in virtual network. The compute executes in a containerized environment and packages your model dependencies in a [Docker container](https://www.docker.com/why-docker).\\n\\nCompute clusters can run jobs securely in a [virtual network environment](how-to-secure-training-vnet.md), without requiring enterprises to open up SSH ports. The job executes in a containerized environment and packages your model dependencies in a Docker container. \\n\\n## Limitations\\n\\n* Some of the scenarios listed in this document are marked as __preview__. Preview functionality is provided without a service level agreement, and it\\'s not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\\n\\n* Compute clusters can be created in a different region than your workspace. This functionality is in __preview__, and is only available for __compute clusters__, not compute instances. This preview isn\\'t available if you\\'re using a private endpoint-enabled workspace. \\n\\n    > [!WARNING]\\n    > When using a compute cluster in a different region than your workspace or datastores, you may see increased network latency and data transfer costs. The latency and costs can occur when creating the cluster, and when running jobs on it.\\n\\n* We currently support only creation (and not updating) of clusters through [ARM templates](/azure/templates/microsoft.machinelearningservices/workspaces/computes). For updating compute, we recommend using the SDK, Azure CLI or UX for now.\\n\\n* Azure Machine Learning Compute has default limits, such as the number of cores that can be allocated. For more information, see [Manage and request quotas for Azure resources](how-to-manage-quotas.md).\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-attach-compute-cluster-0.md'}), Document(page_content='> To learn more about which VM family to request a quota increase for, check out [virtual machine sizes in Azure](../virtual-machines/sizes.md). For instance GPU VM families start with an \"N\" in their family name (eg. NCv3 series)\\n\\nThe following table shows more limits in the platform. Reach out to the AzureML product team through a **technical** support ticket to request an exception.\\n\\n| **Resource or Action** | **Maximum limit** |\\n| --- | --- |\\n| Workspaces per resource group | 800 |\\n| Nodes in a single Azure Machine Learning Compute (AmlCompute) **cluster** set up as a non communication-enabled pool (that is, can\\'t run MPI jobs) | 100 nodes but configurable up to 65,000 nodes |\\n| Nodes in a single Parallel Run Step **run** on an Azure Machine Learning Compute (AmlCompute) cluster | 100 nodes but configurable up to 65,000 nodes if your cluster is set up to scale per above |\\n| Nodes in a single Azure Machine Learning Compute (AmlCompute) **cluster** set up as a communication-enabled pool | 300 nodes but configurable up to 4000 nodes |\\n| Nodes in a single Azure Machine Learning Compute (AmlCompute) **cluster** set up as a communication-enabled pool on an RDMA enabled VM Family | 100 nodes |\\n| Nodes in a single MPI **run** on an Azure Machine Learning Compute (AmlCompute) cluster | 100 nodes but can be increased to 300 nodes |\\n| Job lifetime | 21 days<sup>1</sup> |\\n| Job lifetime on a low-priority node | 7 days<sup>2</sup> |\\n| Parameter servers per node | 1 |\\n\\n<sup>1</sup> Maximum lifetime is the duration between when a job starts and when it finishes. Completed jobs persist indefinitely. Data for jobs not completed within the maximum lifetime isn\\'t accessible.\\n\\n<sup>2</sup> Jobs on a low-priority node can be preempted whenever there\\'s a capacity constraint. We recommend that you implement checkpoints in your job.\\n\\n### Azure Machine Learning managed online endpoints\\n\\nAzure Machine Learning managed online endpoints have limits described in the following table. \\n\\n| **Resource** | **Limit** |\\n| --- | --- |\\n| Endpoint name| Endpoint names must <li> Begin with a letter <li> Be 3-32 characters in length  <li> Only consist of letters and numbers <sup>1</sup> |\\n| Deployment name| Deployment names must <li> Begin with a letter <li> Be 3-32 characters in length  <li>  Only consist of letters and numbers <sup>1</sup> |\\n| Number of endpoints per subscription | 50 |\\n| Number of deployments per subscription | 200 |\\n| Number of deployments per endpoint | 20 |\\n| Number of instances per deployment | 20 <sup>2</sup> |\\n| Max request time-out at endpoint level  | 90 seconds |\\n| Total requests per second at endpoint level for all deployments  | 500 <sup>3</sup> |\\n| Total connections per second at endpoint level for all deployments  | 500 <sup>3</sup> |\\n| Total connections active at endpoint level for all deployments  | 500 <sup>3</sup> |\\n| Total bandwidth at endpoint level for all deployments  | 5 MBPS <sup>3</sup> |\\n\\n<sup>1</sup> Single dashes like, `my-endpoint-name`, are accepted in endpoint and deployment names.\\n\\n<sup>2</sup> We reserve 20% extra compute resources for performing upgrades. For example, if you request 10 instances in a deployment, you must have a quota for 12. Otherwise, you\\'ll receive an error.\\n\\n<sup>3</sup> If you request a limit increase, be sure to calculate related limit increases you might need. For example, if you request a limit increase for requests per second, you might also want to compute the required connections and bandwidth limits and include these limit increases in the same request.\\n\\nTo determine the current usage for an endpoint, [view the metrics](how-to-monitor-online-endpoints.md#metrics). \\n\\nTo request an exception from the Azure Machine Learning product team, use the steps in the [Request quota increases](#request-quota-increases).\\n\\n\\n### Azure Machine Learning pipelines\\n[Azure Machine Learning pipelines](concept-ml-pipelines.md) have the following limits.\\n\\n| **Resource** | **Limit** |\\n| --- | --- |\\n| Steps in a pipeline | 30,000 |\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-quotas-66.md'}), Document(page_content=\"\\n# Set up a development environment with Azure Databricks and AutoML in Azure Machine Learning \\n\\nLearn how to configure a development environment in Azure Machine Learning that uses Azure Databricks and automated ML.\\n\\nAzure Databricks is ideal for running large-scale intensive machine learning workflows on the scalable Apache Spark platform in the Azure cloud. It provides a collaborative Notebook-based environment with a CPU or GPU-based compute cluster.\\n\\nFor information on other machine learning development environments, see [Set up Python development environment](how-to-configure-environment.md).\\n\\n\\n## Prerequisite\\n\\nAzure Machine Learning workspace. To create one, use the steps in the [Create workspace resources](quickstart-create-resources.md) article.\\n\\n\\n## Azure Databricks with Azure Machine Learning and AutoML\\n\\nAzure Databricks integrates with Azure Machine Learning and its AutoML capabilities. \\n\\nYou can use Azure Databricks:\\n\\n+ To train a model using Spark MLlib and deploy the model to ACI/AKS.\\n+ With [automated machine learning](concept-automated-ml.md) capabilities using an Azure ML SDK.\\n+ As a compute target from an [Azure Machine Learning pipeline](concept-ml-pipelines.md).\\n\\n## Set up a Databricks cluster\\n\\nCreate a [Databricks cluster](/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal). Some settings apply only if you install the SDK for automated machine learning on Databricks.\\n\\n**It takes few minutes to create the cluster.**\\n\\nUse these settings:\\n\\n| Setting |Applies to| Value |\\n|----|---|---|\\n| Cluster Name |always| yourclustername |\\n| Databricks Runtime Version |always| 9.1 LTS|\\n| Python version |always| 3 |\\n| Worker Type <br>(determines max # of concurrent iterations) |Automated ML<br>only| Memory optimized VM preferred |\\n| Workers |always| 2 or higher |\\n| Enable Autoscaling |Automated ML<br>only| Uncheck |\\n\\nWait until the cluster is running before proceeding further.\\n\\n## Add the Azure ML SDK to Databricks\\n\\nOnce the cluster is running, [create a library](https://docs.databricks.com/user-guide/libraries.html#create-a-library) to attach the appropriate Azure Machine Learning SDK package to your cluster. \\n\\nTo use automated ML, skip to [Add the Azure ML SDK with AutoML](#add-the-azure-ml-sdk-with-automl-to-databricks).\\n\\n\\n1. Right-click the current Workspace folder where you want to store the library. Select **Create** > **Library**.\\n    \\n    > [!TIP]\\n    > If you have an old SDK version, deselect it from cluster's installed libraries and move to trash. Install the new SDK version and restart the cluster. If there is an issue after the restart, detach and reattach your cluster.\\n\\n1. Choose the following option (no other SDK installations are supported)\\n\\n   |SDK&nbsp;package&nbsp;extras|Source|PyPi&nbsp;Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\\n   |----|---|---|\\n   |For Databricks| Upload Python Egg or PyPI | azureml-sdk[databricks]|\\n\\n   > [!WARNING]\\n   > No other SDK extras can be installed. Choose only the [`databricks`] option .\\n\\n   * Do not select **Attach automatically to all clusters**.\\n   * Select  **Attach** next to your cluster name.\\n\\n1. Monitor for errors until status changes to **Attached**, which may take several minutes.  If this step fails:\\n\\n   Try restarting your cluster by:\\n   1. In the left pane, select **Clusters**.\\n   1. In the table, select your cluster name.\\n   1. On the **Libraries** tab, select **Restart**.\\n\\n   A successful install looks like the following: \\n\\n  ![Azure Machine Learning SDK for Databricks](./media/how-to-configure-environment/amlsdk-withoutautoml.jpg) \\n\\n## Add the Azure ML SDK with AutoML to Databricks\\nIf the cluster was created with Databricks Runtime 7.3 LTS (*not* ML), run the following command in the first cell of your notebook to install the AzureML SDK.\\n\\n```\\n%pip install --upgrade --force-reinstall -r https://aka.ms/automl_linux_requirements.txt\\n```\\n\\n### AutoML config settings\\n\\nIn AutoML config, when using Azure Databricks add the following parameters:\\n\\n\", metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-configure-databricks-automl-environment-0.md'})]\n",
      "json_call: {'module': 'langchain.chat_models.openai', 'name': '_generate', 'args': (AzureChatOpenAI(verbose=True, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fe8ba6edcf0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='', request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='gpt-35-turbo', openai_api_type='azure', openai_api_base='', openai_api_version='2023-03-15-preview'), [SystemMessage(content='Use the following pieces of context to answer the users question. \\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n----------------\\n\\n# Create an Azure Machine Learning compute cluster\\n\\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\\n\\n> [!div class=\"op_single_selector\" title1=\"Select the Azure Machine Learning CLI or SDK version you are using:\"]\\n> * [v1](v1/how-to-create-attach-compute-cluster.md)\\n> * [v2 (current version)](how-to-create-attach-compute-cluster.md)\\n\\nLearn how to create and manage a [compute cluster](concept-compute-target.md#azure-machine-learning-compute-managed) in your Azure Machine Learning workspace.\\n\\nYou can use Azure Machine Learning compute cluster to distribute a training or batch inference process across a cluster of CPU or GPU compute nodes in the cloud. For more information on the VM sizes that include GPUs, see [GPU-optimized virtual machine sizes](../virtual-machines/sizes-gpu.md). \\n\\nIn this article, learn how to:\\n\\n* Create a compute cluster\\n* Lower your compute cluster cost with low priority VMs\\n* Set up a [managed identity](../active-directory/managed-identities-azure-resources/overview.md) for the cluster\\n\\n## Prerequisites\\n\\n* An Azure Machine Learning workspace. For more information, see [Create an Azure Machine Learning workspace](how-to-manage-workspace.md).\\n\\n* The [Azure CLI extension for Machine Learning service (v2)](reference-azure-machine-learning-cli.md), [Azure Machine Learning Python SDK](/python/api/overview/azure/ai-ml-readme), or the [Azure Machine Learning Visual Studio Code extension](how-to-setup-vs-code.md).\\n\\n* If using the Python SDK, [set up your development environment with a workspace](how-to-configure-environment.md).  Once your environment is set up, attach to the workspace in your Python script:\\n\\n    [!INCLUDE [connect ws v2](../../includes/machine-learning-connect-ws-v2.md)]\\n\\n\\n## What is a compute cluster?\\n\\nAzure Machine Learning compute cluster is a managed-compute infrastructure that allows you to easily create a single or multi-node compute. The compute cluster is a resource that can be shared with other users in your workspace. The compute scales up automatically when a job is submitted, and can be put in an Azure Virtual Network. Compute cluster supports **no public IP** deployment as well in virtual network. The compute executes in a containerized environment and packages your model dependencies in a [Docker container](https://www.docker.com/why-docker).\\n\\nCompute clusters can run jobs securely in a [virtual network environment](how-to-secure-training-vnet.md), without requiring enterprises to open up SSH ports. The job executes in a containerized environment and packages your model dependencies in a Docker container. \\n\\n## Limitations\\n\\n* Some of the scenarios listed in this document are marked as __preview__. Preview functionality is provided without a service level agreement, and it\\'s not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\\n\\n* Compute clusters can be created in a different region than your workspace. This functionality is in __preview__, and is only available for __compute clusters__, not compute instances. This preview isn\\'t available if you\\'re using a private endpoint-enabled workspace. \\n\\n    > [!WARNING]\\n    > When using a compute cluster in a different region than your workspace or datastores, you may see increased network latency and data transfer costs. The latency and costs can occur when creating the cluster, and when running jobs on it.\\n\\n* We currently support only creation (and not updating) of clusters through [ARM templates](/azure/templates/microsoft.machinelearningservices/workspaces/computes). For updating compute, we recommend using the SDK, Azure CLI or UX for now.\\n\\n* Azure Machine Learning Compute has default limits, such as the number of cores that can be allocated. For more information, see [Manage and request quotas for Azure resources](how-to-manage-quotas.md).\\n\\n\\n> To learn more about which VM family to request a quota increase for, check out [virtual machine sizes in Azure](../virtual-machines/sizes.md). For instance GPU VM families start with an \"N\" in their family name (eg. NCv3 series)\\n\\nThe following table shows more limits in the platform. Reach out to the AzureML product team through a **technical** support ticket to request an exception.\\n\\n| **Resource or Action** | **Maximum limit** |\\n| --- | --- |\\n| Workspaces per resource group | 800 |\\n| Nodes in a single Azure Machine Learning Compute (AmlCompute) **cluster** set up as a non communication-enabled pool (that is, can\\'t run MPI jobs) | 100 nodes but configurable up to 65,000 nodes |\\n| Nodes in a single Parallel Run Step **run** on an Azure Machine Learning Compute (AmlCompute) cluster | 100 nodes but configurable up to 65,000 nodes if your cluster is set up to scale per above |\\n| Nodes in a single Azure Machine Learning Compute (AmlCompute) **cluster** set up as a communication-enabled pool | 300 nodes but configurable up to 4000 nodes |\\n| Nodes in a single Azure Machine Learning Compute (AmlCompute) **cluster** set up as a communication-enabled pool on an RDMA enabled VM Family | 100 nodes |\\n| Nodes in a single MPI **run** on an Azure Machine Learning Compute (AmlCompute) cluster | 100 nodes but can be increased to 300 nodes |\\n| Job lifetime | 21 days<sup>1</sup> |\\n| Job lifetime on a low-priority node | 7 days<sup>2</sup> |\\n| Parameter servers per node | 1 |\\n\\n<sup>1</sup> Maximum lifetime is the duration between when a job starts and when it finishes. Completed jobs persist indefinitely. Data for jobs not completed within the maximum lifetime isn\\'t accessible.\\n\\n<sup>2</sup> Jobs on a low-priority node can be preempted whenever there\\'s a capacity constraint. We recommend that you implement checkpoints in your job.\\n\\n### Azure Machine Learning managed online endpoints\\n\\nAzure Machine Learning managed online endpoints have limits described in the following table. \\n\\n| **Resource** | **Limit** |\\n| --- | --- |\\n| Endpoint name| Endpoint names must <li> Begin with a letter <li> Be 3-32 characters in length  <li> Only consist of letters and numbers <sup>1</sup> |\\n| Deployment name| Deployment names must <li> Begin with a letter <li> Be 3-32 characters in length  <li>  Only consist of letters and numbers <sup>1</sup> |\\n| Number of endpoints per subscription | 50 |\\n| Number of deployments per subscription | 200 |\\n| Number of deployments per endpoint | 20 |\\n| Number of instances per deployment | 20 <sup>2</sup> |\\n| Max request time-out at endpoint level  | 90 seconds |\\n| Total requests per second at endpoint level for all deployments  | 500 <sup>3</sup> |\\n| Total connections per second at endpoint level for all deployments  | 500 <sup>3</sup> |\\n| Total connections active at endpoint level for all deployments  | 500 <sup>3</sup> |\\n| Total bandwidth at endpoint level for all deployments  | 5 MBPS <sup>3</sup> |\\n\\n<sup>1</sup> Single dashes like, `my-endpoint-name`, are accepted in endpoint and deployment names.\\n\\n<sup>2</sup> We reserve 20% extra compute resources for performing upgrades. For example, if you request 10 instances in a deployment, you must have a quota for 12. Otherwise, you\\'ll receive an error.\\n\\n<sup>3</sup> If you request a limit increase, be sure to calculate related limit increases you might need. For example, if you request a limit increase for requests per second, you might also want to compute the required connections and bandwidth limits and include these limit increases in the same request.\\n\\nTo determine the current usage for an endpoint, [view the metrics](how-to-monitor-online-endpoints.md#metrics). \\n\\nTo request an exception from the Azure Machine Learning product team, use the steps in the [Request quota increases](#request-quota-increases).\\n\\n\\n### Azure Machine Learning pipelines\\n[Azure Machine Learning pipelines](concept-ml-pipelines.md) have the following limits.\\n\\n| **Resource** | **Limit** |\\n| --- | --- |\\n| Steps in a pipeline | 30,000 |\\n\\n\\n\\n# Set up a development environment with Azure Databricks and AutoML in Azure Machine Learning \\n\\nLearn how to configure a development environment in Azure Machine Learning that uses Azure Databricks and automated ML.\\n\\nAzure Databricks is ideal for running large-scale intensive machine learning workflows on the scalable Apache Spark platform in the Azure cloud. It provides a collaborative Notebook-based environment with a CPU or GPU-based compute cluster.\\n\\nFor information on other machine learning development environments, see [Set up Python development environment](how-to-configure-environment.md).\\n\\n\\n## Prerequisite\\n\\nAzure Machine Learning workspace. To create one, use the steps in the [Create workspace resources](quickstart-create-resources.md) article.\\n\\n\\n## Azure Databricks with Azure Machine Learning and AutoML\\n\\nAzure Databricks integrates with Azure Machine Learning and its AutoML capabilities. \\n\\nYou can use Azure Databricks:\\n\\n+ To train a model using Spark MLlib and deploy the model to ACI/AKS.\\n+ With [automated machine learning](concept-automated-ml.md) capabilities using an Azure ML SDK.\\n+ As a compute target from an [Azure Machine Learning pipeline](concept-ml-pipelines.md).\\n\\n## Set up a Databricks cluster\\n\\nCreate a [Databricks cluster](/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal). Some settings apply only if you install the SDK for automated machine learning on Databricks.\\n\\n**It takes few minutes to create the cluster.**\\n\\nUse these settings:\\n\\n| Setting |Applies to| Value |\\n|----|---|---|\\n| Cluster Name |always| yourclustername |\\n| Databricks Runtime Version |always| 9.1 LTS|\\n| Python version |always| 3 |\\n| Worker Type <br>(determines max # of concurrent iterations) |Automated ML<br>only| Memory optimized VM preferred |\\n| Workers |always| 2 or higher |\\n| Enable Autoscaling |Automated ML<br>only| Uncheck |\\n\\nWait until the cluster is running before proceeding further.\\n\\n## Add the Azure ML SDK to Databricks\\n\\nOnce the cluster is running, [create a library](https://docs.databricks.com/user-guide/libraries.html#create-a-library) to attach the appropriate Azure Machine Learning SDK package to your cluster. \\n\\nTo use automated ML, skip to [Add the Azure ML SDK with AutoML](#add-the-azure-ml-sdk-with-automl-to-databricks).\\n\\n\\n1. Right-click the current Workspace folder where you want to store the library. Select **Create** > **Library**.\\n    \\n    > [!TIP]\\n    > If you have an old SDK version, deselect it from cluster\\'s installed libraries and move to trash. Install the new SDK version and restart the cluster. If there is an issue after the restart, detach and reattach your cluster.\\n\\n1. Choose the following option (no other SDK installations are supported)\\n\\n   |SDK&nbsp;package&nbsp;extras|Source|PyPi&nbsp;Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\\n   |----|---|---|\\n   |For Databricks| Upload Python Egg or PyPI | azureml-sdk[databricks]|\\n\\n   > [!WARNING]\\n   > No other SDK extras can be installed. Choose only the [`databricks`] option .\\n\\n   * Do not select **Attach automatically to all clusters**.\\n   * Select  **Attach** next to your cluster name.\\n\\n1. Monitor for errors until status changes to **Attached**, which may take several minutes.  If this step fails:\\n\\n   Try restarting your cluster by:\\n   1. In the left pane, select **Clusters**.\\n   1. In the table, select your cluster name.\\n   1. On the **Libraries** tab, select **Restart**.\\n\\n   A successful install looks like the following: \\n\\n  ![Azure Machine Learning SDK for Databricks](./media/how-to-configure-environment/amlsdk-withoutautoml.jpg) \\n\\n## Add the Azure ML SDK with AutoML to Databricks\\nIf the cluster was created with Databricks Runtime 7.3 LTS (*not* ML), run the following command in the first cell of your notebook to install the AzureML SDK.\\n\\n```\\n%pip install --upgrade --force-reinstall -r https://aka.ms/automl_linux_requirements.txt\\n```\\n\\n### AutoML config settings\\n\\nIn AutoML config, when using Azure Databricks add the following parameters:\\n\\n\\n', additional_kwargs={}), HumanMessage(content='how can I set up a compute cluster?', additional_kwargs={})]), 'kwargs': {'stop': None}}\n",
      "json_result: generations=[ChatGeneration(text=\"To create and manage a compute cluster, you should follow these steps:\\n\\n1. Ensure that you have an Azure Machine Learning workspace. If you don't have one, you can create a new one by following the steps in [Create an Azure Machine Learning workspace](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python).\\n2. Depending on the tool you use, set up your environment:\\n  * For the Azure CLI, install the Azure Machine Learning CLI extension using the instructions specified in [Install the Azure Machine Learning CLI extension](https://docs.microsoft.com/en-us/azure/machine-learning/reference-azure-machine-learning-cli).\\n  * For the Python SDK, install the Azure Machine Learning Python SDK and configure your development environment by following the instructions in [Configure your development environment for Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment).\\n  * For Visual Studio Code, follow the instructions in [Set up your development environment in Visual Studio Code](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-vs-code) to install the Azure Machine Learning Visual Studio Code extension.\\n3. Create a compute cluster using the corresponding tool:\\n  * For Azure CLI, follow the instructions in [How to create and attach an Azure Machine Learning compute cluster using the Azure CLI](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=cli).\\n  * For Python SDK, refer to [Create Azure Machine Learning Compute cluster](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computeinstance.computeinstance?view=azure-ml-py#create-azure-machine-learning-compute-cluster-name--vm-size--min-nodes-0--max-nodes-none--idle-seconds-before-scaledown-none--admin-username-none--admin-user-ssh-key-none--vnet-resourcegroup-none--vnet-name-none--subnet-name-none--tags-none--description-none--remote-login-port-public-ssh--existing-vnet-name-none--priority-mode-default--scopes-none---priority-none--tags-none--validate-none--user-managed-identity-none--identity-type-system--data-disks-per-node-none--description-none-)\\n  * For Visual Studio Code, follow the instructions in [Create and manage compute targets](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-vs-code#CREATE) to set up a compute cluster.\\n  \\nAdditionally, you can learn more about the different types of compute targets that can be used with Azure Machine Learning, such as Azure Machine Learning Compute, HDInsight, and Azure Databricks, in the [Compute targets for training and deployment](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target) article.\", generation_info=None, message=AIMessage(content=\"To create and manage a compute cluster, you should follow these steps:\\n\\n1. Ensure that you have an Azure Machine Learning workspace. If you don't have one, you can create a new one by following the steps in [Create an Azure Machine Learning workspace](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python).\\n2. Depending on the tool you use, set up your environment:\\n  * For the Azure CLI, install the Azure Machine Learning CLI extension using the instructions specified in [Install the Azure Machine Learning CLI extension](https://docs.microsoft.com/en-us/azure/machine-learning/reference-azure-machine-learning-cli).\\n  * For the Python SDK, install the Azure Machine Learning Python SDK and configure your development environment by following the instructions in [Configure your development environment for Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment).\\n  * For Visual Studio Code, follow the instructions in [Set up your development environment in Visual Studio Code](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-vs-code) to install the Azure Machine Learning Visual Studio Code extension.\\n3. Create a compute cluster using the corresponding tool:\\n  * For Azure CLI, follow the instructions in [How to create and attach an Azure Machine Learning compute cluster using the Azure CLI](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=cli).\\n  * For Python SDK, refer to [Create Azure Machine Learning Compute cluster](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computeinstance.computeinstance?view=azure-ml-py#create-azure-machine-learning-compute-cluster-name--vm-size--min-nodes-0--max-nodes-none--idle-seconds-before-scaledown-none--admin-username-none--admin-user-ssh-key-none--vnet-resourcegroup-none--vnet-name-none--subnet-name-none--tags-none--description-none--remote-login-port-public-ssh--existing-vnet-name-none--priority-mode-default--scopes-none---priority-none--tags-none--validate-none--user-managed-identity-none--identity-type-system--data-disks-per-node-none--description-none-)\\n  * For Visual Studio Code, follow the instructions in [Create and manage compute targets](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-vs-code#CREATE) to set up a compute cluster.\\n  \\nAdditionally, you can learn more about the different types of compute targets that can be used with Azure Machine Learning, such as Azure Machine Learning Compute, HDInsight, and Azure Databricks, in the [Compute targets for training and deployment](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target) article.\", additional_kwargs={}))] llm_output={'token_usage': <OpenAIObject at 0x7fe869aa9b70> JSON: {\n",
      "  \"completion_tokens\": 582,\n",
      "  \"prompt_tokens\": 2778,\n",
      "  \"total_tokens\": 3360\n",
      "}, 'model_name': 'gpt-3.5-turbo'}\n",
      "Q: how can I set up a compute cluster?\n",
      "A: To create and manage a compute cluster, you should follow these steps:\n",
      "\n",
      "1. Ensure that you have an Azure Machine Learning workspace. If you don't have one, you can create a new one by following the steps in [Create an Azure Machine Learning workspace](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python).\n",
      "2. Depending on the tool you use, set up your environment:\n",
      "  * For the Azure CLI, install the Azure Machine Learning CLI extension using the instructions specified in [Install the Azure Machine Learning CLI extension](https://docs.microsoft.com/en-us/azure/machine-learning/reference-azure-machine-learning-cli).\n",
      "  * For the Python SDK, install the Azure Machine Learning Python SDK and configure your development environment by following the instructions in [Configure your development environment for Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment).\n",
      "  * For Visual Studio Code, follow the instructions in [Set up your development environment in Visual Studio Code](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-vs-code) to install the Azure Machine Learning Visual Studio Code extension.\n",
      "3. Create a compute cluster using the corresponding tool:\n",
      "  * For Azure CLI, follow the instructions in [How to create and attach an Azure Machine Learning compute cluster using the Azure CLI](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=cli).\n",
      "  * For Python SDK, refer to [Create Azure Machine Learning Compute cluster](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computeinstance.computeinstance?view=azure-ml-py#create-azure-machine-learning-compute-cluster-name--vm-size--min-nodes-0--max-nodes-none--idle-seconds-before-scaledown-none--admin-username-none--admin-user-ssh-key-none--vnet-resourcegroup-none--vnet-name-none--subnet-name-none--tags-none--description-none--remote-login-port-public-ssh--existing-vnet-name-none--priority-mode-default--scopes-none---priority-none--tags-none--validate-none--user-managed-identity-none--identity-type-system--data-disks-per-node-none--description-none-)\n",
      "  * For Visual Studio Code, follow the instructions in [Create and manage compute targets](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-vs-code#CREATE) to set up a compute cluster.\n",
      "  \n",
      "Additionally, you can learn more about the different types of compute targets that can be used with Azure Machine Learning, such as Azure Machine Learning Compute, HDInsight, and Azure Databricks, in the [Compute targets for training and deployment](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target) article.\n"
     ]
    }
   ],
   "source": [
    "!python src/langchain/rag_with_cog_search.py --question \"how can I set up a compute cluster?\" --top 3 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundedness\n",
    "\n",
    "The model might sometimes fill in information that was not provided in the search context or that would be common knowledge. If that happens, we call the reply as not grounded. \n",
    "\n",
    "For instance the query below suggests a command `az ml workspace compute show-quota` which doesn't exist in the documentation or the product. Instead `az ml compute list-usage` would be the right command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running offline, checking secrets\n",
      "json_call: {'module': '__main__', 'name': 'get_relevant_documents', 'args': (<__main__.CognitiveSearchRetriever object at 0x7fb722bb0940>, 'how can I see the quota assigned to my workspace in the v2 cli?'), 'kwargs': {}}\n",
      "cog_search_top: 3\n",
      "cog_search_query: how can I see the quota assigned to my workspace in the v2 cli?\n",
      "cog_search_docs: [Document(page_content='\\nAfter deployment, this role becomes available in the specified workspace. Now you can add and assign this role in the Azure portal.\\n\\nFor more information on custom roles, see [Azure custom roles](../role-based-access-control/custom-roles.md). \\n\\n### Azure Machine Learning operations\\n\\nFor more information on the operations (actions and not actions) usable with custom roles, see [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices). You can also use the following Azure CLI command to list operations:\\n\\n```azurecli-interactive\\naz provider operation show â€“n Microsoft.MachineLearningServices\\n```\\n\\n## List custom roles\\n\\nIn the Azure CLI, run the following command:\\n\\n```azurecli-interactive\\naz role definition list --subscription <sub-id> --custom-role-only true\\n```\\n\\nTo view the role definition for a specific custom role, use the following Azure CLI command. The `<role-name>` should be in the same format returned by the command above:\\n\\n```azurecli-interactive\\naz role definition list -n <role-name> --subscription <sub-id>\\n```\\n\\n## Update a custom role\\n\\nIn the Azure CLI, run the following command:\\n\\n```azurecli-interactive\\naz role definition update --role-definition update_def.json --subscription <sub-id>\\n```\\n\\nYou need to have permissions on the entire scope of your new role definition. For example if this new role has a scope across three subscriptions, you need to have permissions on all three subscriptions. \\n\\n> [!NOTE]\\n> Role updates can take 15 minutes to an hour to apply across all role assignments in that scope.\\n\\n## Use Azure Resource Manager templates for repeatability\\n\\nIf you anticipate that you\\'ll need to recreate complex role assignments, an Azure Resource Manager template can be a significant help. The [machine-learning-dependencies-role-assignment template](https://github.com/Azure/azure-quickstart-templates/tree/master//quickstarts/microsoft.machinelearningservices/machine-learning-dependencies-role-assignment) shows how role assignments can be specified in source code for reuse. \\n\\n## Common scenarios\\n\\nThe following table is a summary of Azure Machine Learning activities and the permissions required to perform them at the least scope. For example, if an activity can be performed with a workspace scope (Column 4), then all higher scope with that permission will also work automatically. Note that for certain activities the permissions differ between V1 and V2 APIs.\\n\\n> [!IMPORTANT]\\n> All paths in this table that start with `/` are **relative paths** to `Microsoft.MachineLearningServices/` :\\n\\n| Activity | Subscription-level scope | Resource group-level scope | Workspace-level scope |\\n| ----- | ----- | ----- | ----- |\\n| Create new workspace <sub>1</sub> | Not required | Owner or contributor | N/A (becomes Owner or inherits higher scope role after creation) |\\n| Request subscription level Amlcompute quota or set workspace level quota | Owner, or contributor, or custom role </br>allowing `/locations/updateQuotas/action`</br> at subscription scope | Not Authorized | Not Authorized |\\n| Create new compute cluster | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\n| Create new compute instance | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\n| Submitting any type of run (V1) | Not required | Not required | Owner, contributor, or custom role allowing: `\"/workspaces/*/read\", \"/workspaces/environments/write\", \"/workspaces/experiments/runs/write\", \"/workspaces/metadata/artifacts/write\", \"/workspaces/metadata/snapshots/write\", \"/workspaces/environments/build/action\", \"/workspaces/experiments/runs/submit/action\", \"/workspaces/environments/readSecrets/action\"` |\\n| Submitting any type of run (V2) | Not required | Not required | Owner, contributor, or custom role allowing: `\"/workspaces/*/read\", \"/workspaces/environments/write\", \"/workspaces/jobs/*\", \"/workspaces/metadata/artifacts/write\", \"/workspaces/metadata/codes/*/write\", \"/workspaces/environments/build/action\", \"/workspaces/environments/readSecrets/action\"` |\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-108.md'}), Document(page_content=\"\\n## Troubleshooting\\n\\nHere are a few things to be aware of while you use Azure role-based access control (Azure RBAC):\\n\\n- When you create a resource in Azure, such as a workspace, you're not directly the owner of the resource. Your role is inherited from the highest scope role that you're authorized against in that subscription. As an example if you're a Network Administrator, and have the permissions to create a Machine Learning workspace, you would be assigned the Network Administrator role against that workspace, and not the Owner role.\\n\\n- To perform quota operations in a workspace, you need subscription level permissions. This means setting either subscription level quota or workspace level quota for your managed compute resources can only happen if you have write permissions at the subscription scope.\\n\\n- When there are two role assignments to the same Azure Active Directory user with conflicting sections of Actions/NotActions, your operations listed in NotActions from one role might not take effect if they are also listed as Actions in another role. To learn more about how Azure parses role assignments, read [How Azure RBAC determines if a user has access to a resource](../role-based-access-control/overview.md#how-azure-rbac-determines-if-a-user-has-access-to-a-resource)\\n\\n- To deploy your compute resources inside a VNet, you need to explicitly have permissions for the following actions:\\n    - `Microsoft.Network/virtualNetworks/*/read` on the VNet resources.\\n    - `Microsoft.Network/virtualNetworks/subnets/join/action` on the subnet resource.\\n    \\n    For more information on Azure RBAC with networking, see the [Networking built-in roles](../role-based-access-control/built-in-roles.md#networking).\\n\\n- It can sometimes take up to 1 hour for your new role assignments to take effect over cached permissions across the stack.\\n\\n## Next steps\\n\\n- [Enterprise security overview](concept-enterprise-security.md)\\n- [Virtual network isolation and privacy overview](how-to-network-security-overview.md)\\n- [Tutorial: Train and deploy a model](tutorial-train-deploy-notebook.md)\\n- [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices)\\n\", metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-573.md'}), Document(page_content='\\n## Assign managed identity\\n\\nYou can assign a system- or user-assigned [managed identity](../active-directory/managed-identities-azure-resources/overview.md) to a compute instance, to authenticate against other Azure resources such as storage. Using managed identities for authentication helps improve workspace security and management. For example, you can allow users to access training data only when logged in to a compute instance. Or use a common user-assigned managed identity to permit access to a specific storage account. \\n\\nYou can create compute instance with managed identity from Azure ML Studio:\\n\\n1.\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create).\\n1.\\tSelect **Next: Advanced Settings**.\\n1.\\tEnable **Assign a managed identity**.\\n1.  Select **System-assigned** or **User-assigned** under **Identity type**.\\n1.  If you selected **User-assigned**, select subscription and name of the identity.\\n\\nYou can use V2 CLI to create compute instance with assign system-assigned managed identity:\\n\\n```azurecli\\naz ml compute create --name myinstance --identity-type SystemAssigned --type ComputeInstance --resource-group my-resource-group --workspace-name my-workspace\\n```\\n\\nYou can also use V2 CLI with yaml file, for example to create a compute instance with user-assigned managed identity:\\n\\n```azurecli\\nazure ml compute create --file compute.yaml --resource-group my-resource-group --workspace-name my-workspace\\n```\\n\\nThe identity definition is contained in compute.yaml file:\\n\\n```yaml\\nhttps://azuremlschemas.azureedge.net/latest/computeInstance.schema.json\\nname: myinstance\\ntype: computeinstance\\nidentity:\\n  type: user_assigned\\n  user_assigned_identities: \\n    - resource_id: identity_resource_id\\n```\\n\\nOnce the managed identity is created, grant the managed identity at least Storage Blob Data Reader role on the storage account of the datastore, see [Accessing storage services](how-to-identity-based-service-authentication.md?tabs=cli#accessing-storage-services). Then, when you work on the compute instance, the managed identity is used automatically to authenticate against datastores.\\n\\n> [!NOTE]\\n> The name of the created system managed identity will be in the format /workspace-name/computes/compute-instance-name in your Azure Active Directory. \\n\\nYou can also use the managed identity manually to authenticate against other Azure resources. The following example shows how to use it to get an Azure Resource Manager access token:\\n\\n```python\\nimport requests\\n\\ndef get_access_token_msi(resource):\\n    client_id = os.environ.get(\"DEFAULT_IDENTITY_CLIENT_ID\", None)\\n    resp = requests.get(f\"{os.environ[\\'MSI_ENDPOINT\\']}?resource={resource}&clientid={client_id}&api-version=2017-09-01\", headers={\\'Secret\\': os.environ[\"MSI_SECRET\"]})\\n    resp.raise_for_status()\\n    return resp.json()[\"access_token\"]\\n\\narm_access_token = get_access_token_msi(\"https://management.azure.com\")\\n```\\n\\nTo use Azure CLI with the managed identity for authentication, specify the identity client ID as the username when logging in: \\n```azurecli\\naz login --identity --username $DEFAULT_IDENTITY_CLIENT_ID\\n```\\n\\n> [!NOTE]\\n> You cannot use ```azcopy``` when trying to use managed identity. ```azcopy login --identity``` will not work.\\n\\n## Add custom applications such as RStudio or Posit Workbench (preview)\\n\\n> [!IMPORTANT]\\n> Items marked (preview) below are currently in public preview.\\n> The preview version is provided without a service level agreement, and it\\'s not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.\\n> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\\n\\nYou can set up other applications, such as RStudio, or Posit Workbench (formerly RStudio Workbench), when creating a compute instance. Follow these steps in studio to set up a custom application on your compute instance\\n\\n1.\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create)\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-534.md'})]\n",
      "json_result: [Document(page_content='\\nAfter deployment, this role becomes available in the specified workspace. Now you can add and assign this role in the Azure portal.\\n\\nFor more information on custom roles, see [Azure custom roles](../role-based-access-control/custom-roles.md). \\n\\n### Azure Machine Learning operations\\n\\nFor more information on the operations (actions and not actions) usable with custom roles, see [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices). You can also use the following Azure CLI command to list operations:\\n\\n```azurecli-interactive\\naz provider operation show â€“n Microsoft.MachineLearningServices\\n```\\n\\n## List custom roles\\n\\nIn the Azure CLI, run the following command:\\n\\n```azurecli-interactive\\naz role definition list --subscription <sub-id> --custom-role-only true\\n```\\n\\nTo view the role definition for a specific custom role, use the following Azure CLI command. The `<role-name>` should be in the same format returned by the command above:\\n\\n```azurecli-interactive\\naz role definition list -n <role-name> --subscription <sub-id>\\n```\\n\\n## Update a custom role\\n\\nIn the Azure CLI, run the following command:\\n\\n```azurecli-interactive\\naz role definition update --role-definition update_def.json --subscription <sub-id>\\n```\\n\\nYou need to have permissions on the entire scope of your new role definition. For example if this new role has a scope across three subscriptions, you need to have permissions on all three subscriptions. \\n\\n> [!NOTE]\\n> Role updates can take 15 minutes to an hour to apply across all role assignments in that scope.\\n\\n## Use Azure Resource Manager templates for repeatability\\n\\nIf you anticipate that you\\'ll need to recreate complex role assignments, an Azure Resource Manager template can be a significant help. The [machine-learning-dependencies-role-assignment template](https://github.com/Azure/azure-quickstart-templates/tree/master//quickstarts/microsoft.machinelearningservices/machine-learning-dependencies-role-assignment) shows how role assignments can be specified in source code for reuse. \\n\\n## Common scenarios\\n\\nThe following table is a summary of Azure Machine Learning activities and the permissions required to perform them at the least scope. For example, if an activity can be performed with a workspace scope (Column 4), then all higher scope with that permission will also work automatically. Note that for certain activities the permissions differ between V1 and V2 APIs.\\n\\n> [!IMPORTANT]\\n> All paths in this table that start with `/` are **relative paths** to `Microsoft.MachineLearningServices/` :\\n\\n| Activity | Subscription-level scope | Resource group-level scope | Workspace-level scope |\\n| ----- | ----- | ----- | ----- |\\n| Create new workspace <sub>1</sub> | Not required | Owner or contributor | N/A (becomes Owner or inherits higher scope role after creation) |\\n| Request subscription level Amlcompute quota or set workspace level quota | Owner, or contributor, or custom role </br>allowing `/locations/updateQuotas/action`</br> at subscription scope | Not Authorized | Not Authorized |\\n| Create new compute cluster | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\n| Create new compute instance | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\n| Submitting any type of run (V1) | Not required | Not required | Owner, contributor, or custom role allowing: `\"/workspaces/*/read\", \"/workspaces/environments/write\", \"/workspaces/experiments/runs/write\", \"/workspaces/metadata/artifacts/write\", \"/workspaces/metadata/snapshots/write\", \"/workspaces/environments/build/action\", \"/workspaces/experiments/runs/submit/action\", \"/workspaces/environments/readSecrets/action\"` |\\n| Submitting any type of run (V2) | Not required | Not required | Owner, contributor, or custom role allowing: `\"/workspaces/*/read\", \"/workspaces/environments/write\", \"/workspaces/jobs/*\", \"/workspaces/metadata/artifacts/write\", \"/workspaces/metadata/codes/*/write\", \"/workspaces/environments/build/action\", \"/workspaces/environments/readSecrets/action\"` |\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-108.md'}), Document(page_content=\"\\n## Troubleshooting\\n\\nHere are a few things to be aware of while you use Azure role-based access control (Azure RBAC):\\n\\n- When you create a resource in Azure, such as a workspace, you're not directly the owner of the resource. Your role is inherited from the highest scope role that you're authorized against in that subscription. As an example if you're a Network Administrator, and have the permissions to create a Machine Learning workspace, you would be assigned the Network Administrator role against that workspace, and not the Owner role.\\n\\n- To perform quota operations in a workspace, you need subscription level permissions. This means setting either subscription level quota or workspace level quota for your managed compute resources can only happen if you have write permissions at the subscription scope.\\n\\n- When there are two role assignments to the same Azure Active Directory user with conflicting sections of Actions/NotActions, your operations listed in NotActions from one role might not take effect if they are also listed as Actions in another role. To learn more about how Azure parses role assignments, read [How Azure RBAC determines if a user has access to a resource](../role-based-access-control/overview.md#how-azure-rbac-determines-if-a-user-has-access-to-a-resource)\\n\\n- To deploy your compute resources inside a VNet, you need to explicitly have permissions for the following actions:\\n    - `Microsoft.Network/virtualNetworks/*/read` on the VNet resources.\\n    - `Microsoft.Network/virtualNetworks/subnets/join/action` on the subnet resource.\\n    \\n    For more information on Azure RBAC with networking, see the [Networking built-in roles](../role-based-access-control/built-in-roles.md#networking).\\n\\n- It can sometimes take up to 1 hour for your new role assignments to take effect over cached permissions across the stack.\\n\\n## Next steps\\n\\n- [Enterprise security overview](concept-enterprise-security.md)\\n- [Virtual network isolation and privacy overview](how-to-network-security-overview.md)\\n- [Tutorial: Train and deploy a model](tutorial-train-deploy-notebook.md)\\n- [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices)\\n\", metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-573.md'}), Document(page_content='\\n## Assign managed identity\\n\\nYou can assign a system- or user-assigned [managed identity](../active-directory/managed-identities-azure-resources/overview.md) to a compute instance, to authenticate against other Azure resources such as storage. Using managed identities for authentication helps improve workspace security and management. For example, you can allow users to access training data only when logged in to a compute instance. Or use a common user-assigned managed identity to permit access to a specific storage account. \\n\\nYou can create compute instance with managed identity from Azure ML Studio:\\n\\n1.\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create).\\n1.\\tSelect **Next: Advanced Settings**.\\n1.\\tEnable **Assign a managed identity**.\\n1.  Select **System-assigned** or **User-assigned** under **Identity type**.\\n1.  If you selected **User-assigned**, select subscription and name of the identity.\\n\\nYou can use V2 CLI to create compute instance with assign system-assigned managed identity:\\n\\n```azurecli\\naz ml compute create --name myinstance --identity-type SystemAssigned --type ComputeInstance --resource-group my-resource-group --workspace-name my-workspace\\n```\\n\\nYou can also use V2 CLI with yaml file, for example to create a compute instance with user-assigned managed identity:\\n\\n```azurecli\\nazure ml compute create --file compute.yaml --resource-group my-resource-group --workspace-name my-workspace\\n```\\n\\nThe identity definition is contained in compute.yaml file:\\n\\n```yaml\\nhttps://azuremlschemas.azureedge.net/latest/computeInstance.schema.json\\nname: myinstance\\ntype: computeinstance\\nidentity:\\n  type: user_assigned\\n  user_assigned_identities: \\n    - resource_id: identity_resource_id\\n```\\n\\nOnce the managed identity is created, grant the managed identity at least Storage Blob Data Reader role on the storage account of the datastore, see [Accessing storage services](how-to-identity-based-service-authentication.md?tabs=cli#accessing-storage-services). Then, when you work on the compute instance, the managed identity is used automatically to authenticate against datastores.\\n\\n> [!NOTE]\\n> The name of the created system managed identity will be in the format /workspace-name/computes/compute-instance-name in your Azure Active Directory. \\n\\nYou can also use the managed identity manually to authenticate against other Azure resources. The following example shows how to use it to get an Azure Resource Manager access token:\\n\\n```python\\nimport requests\\n\\ndef get_access_token_msi(resource):\\n    client_id = os.environ.get(\"DEFAULT_IDENTITY_CLIENT_ID\", None)\\n    resp = requests.get(f\"{os.environ[\\'MSI_ENDPOINT\\']}?resource={resource}&clientid={client_id}&api-version=2017-09-01\", headers={\\'Secret\\': os.environ[\"MSI_SECRET\"]})\\n    resp.raise_for_status()\\n    return resp.json()[\"access_token\"]\\n\\narm_access_token = get_access_token_msi(\"https://management.azure.com\")\\n```\\n\\nTo use Azure CLI with the managed identity for authentication, specify the identity client ID as the username when logging in: \\n```azurecli\\naz login --identity --username $DEFAULT_IDENTITY_CLIENT_ID\\n```\\n\\n> [!NOTE]\\n> You cannot use ```azcopy``` when trying to use managed identity. ```azcopy login --identity``` will not work.\\n\\n## Add custom applications such as RStudio or Posit Workbench (preview)\\n\\n> [!IMPORTANT]\\n> Items marked (preview) below are currently in public preview.\\n> The preview version is provided without a service level agreement, and it\\'s not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.\\n> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\\n\\nYou can set up other applications, such as RStudio, or Posit Workbench (formerly RStudio Workbench), when creating a compute instance. Follow these steps in studio to set up a custom application on your compute instance\\n\\n1.\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create)\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-534.md'})]\n",
      "json_call: {'module': 'langchain.chat_models.openai', 'name': '_generate', 'args': (AzureChatOpenAI(verbose=True, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fb7011a5db0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='', request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='gpt-35-turbo', openai_api_type='azure', openai_api_base='', openai_api_version='2023-03-15-preview'), [SystemMessage(content='Use the following pieces of context to answer the users question. \\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n----------------\\n\\nAfter deployment, this role becomes available in the specified workspace. Now you can add and assign this role in the Azure portal.\\n\\nFor more information on custom roles, see [Azure custom roles](../role-based-access-control/custom-roles.md). \\n\\n### Azure Machine Learning operations\\n\\nFor more information on the operations (actions and not actions) usable with custom roles, see [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices). You can also use the following Azure CLI command to list operations:\\n\\n```azurecli-interactive\\naz provider operation show â€“n Microsoft.MachineLearningServices\\n```\\n\\n## List custom roles\\n\\nIn the Azure CLI, run the following command:\\n\\n```azurecli-interactive\\naz role definition list --subscription <sub-id> --custom-role-only true\\n```\\n\\nTo view the role definition for a specific custom role, use the following Azure CLI command. The `<role-name>` should be in the same format returned by the command above:\\n\\n```azurecli-interactive\\naz role definition list -n <role-name> --subscription <sub-id>\\n```\\n\\n## Update a custom role\\n\\nIn the Azure CLI, run the following command:\\n\\n```azurecli-interactive\\naz role definition update --role-definition update_def.json --subscription <sub-id>\\n```\\n\\nYou need to have permissions on the entire scope of your new role definition. For example if this new role has a scope across three subscriptions, you need to have permissions on all three subscriptions. \\n\\n> [!NOTE]\\n> Role updates can take 15 minutes to an hour to apply across all role assignments in that scope.\\n\\n## Use Azure Resource Manager templates for repeatability\\n\\nIf you anticipate that you\\'ll need to recreate complex role assignments, an Azure Resource Manager template can be a significant help. The [machine-learning-dependencies-role-assignment template](https://github.com/Azure/azure-quickstart-templates/tree/master//quickstarts/microsoft.machinelearningservices/machine-learning-dependencies-role-assignment) shows how role assignments can be specified in source code for reuse. \\n\\n## Common scenarios\\n\\nThe following table is a summary of Azure Machine Learning activities and the permissions required to perform them at the least scope. For example, if an activity can be performed with a workspace scope (Column 4), then all higher scope with that permission will also work automatically. Note that for certain activities the permissions differ between V1 and V2 APIs.\\n\\n> [!IMPORTANT]\\n> All paths in this table that start with `/` are **relative paths** to `Microsoft.MachineLearningServices/` :\\n\\n| Activity | Subscription-level scope | Resource group-level scope | Workspace-level scope |\\n| ----- | ----- | ----- | ----- |\\n| Create new workspace <sub>1</sub> | Not required | Owner or contributor | N/A (becomes Owner or inherits higher scope role after creation) |\\n| Request subscription level Amlcompute quota or set workspace level quota | Owner, or contributor, or custom role </br>allowing `/locations/updateQuotas/action`</br> at subscription scope | Not Authorized | Not Authorized |\\n| Create new compute cluster | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\n| Create new compute instance | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\n| Submitting any type of run (V1) | Not required | Not required | Owner, contributor, or custom role allowing: `\"/workspaces/*/read\", \"/workspaces/environments/write\", \"/workspaces/experiments/runs/write\", \"/workspaces/metadata/artifacts/write\", \"/workspaces/metadata/snapshots/write\", \"/workspaces/environments/build/action\", \"/workspaces/experiments/runs/submit/action\", \"/workspaces/environments/readSecrets/action\"` |\\n| Submitting any type of run (V2) | Not required | Not required | Owner, contributor, or custom role allowing: `\"/workspaces/*/read\", \"/workspaces/environments/write\", \"/workspaces/jobs/*\", \"/workspaces/metadata/artifacts/write\", \"/workspaces/metadata/codes/*/write\", \"/workspaces/environments/build/action\", \"/workspaces/environments/readSecrets/action\"` |\\n\\n\\n\\n## Troubleshooting\\n\\nHere are a few things to be aware of while you use Azure role-based access control (Azure RBAC):\\n\\n- When you create a resource in Azure, such as a workspace, you\\'re not directly the owner of the resource. Your role is inherited from the highest scope role that you\\'re authorized against in that subscription. As an example if you\\'re a Network Administrator, and have the permissions to create a Machine Learning workspace, you would be assigned the Network Administrator role against that workspace, and not the Owner role.\\n\\n- To perform quota operations in a workspace, you need subscription level permissions. This means setting either subscription level quota or workspace level quota for your managed compute resources can only happen if you have write permissions at the subscription scope.\\n\\n- When there are two role assignments to the same Azure Active Directory user with conflicting sections of Actions/NotActions, your operations listed in NotActions from one role might not take effect if they are also listed as Actions in another role. To learn more about how Azure parses role assignments, read [How Azure RBAC determines if a user has access to a resource](../role-based-access-control/overview.md#how-azure-rbac-determines-if-a-user-has-access-to-a-resource)\\n\\n- To deploy your compute resources inside a VNet, you need to explicitly have permissions for the following actions:\\n    - `Microsoft.Network/virtualNetworks/*/read` on the VNet resources.\\n    - `Microsoft.Network/virtualNetworks/subnets/join/action` on the subnet resource.\\n    \\n    For more information on Azure RBAC with networking, see the [Networking built-in roles](../role-based-access-control/built-in-roles.md#networking).\\n\\n- It can sometimes take up to 1 hour for your new role assignments to take effect over cached permissions across the stack.\\n\\n## Next steps\\n\\n- [Enterprise security overview](concept-enterprise-security.md)\\n- [Virtual network isolation and privacy overview](how-to-network-security-overview.md)\\n- [Tutorial: Train and deploy a model](tutorial-train-deploy-notebook.md)\\n- [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices)\\n\\n\\n\\n## Assign managed identity\\n\\nYou can assign a system- or user-assigned [managed identity](../active-directory/managed-identities-azure-resources/overview.md) to a compute instance, to authenticate against other Azure resources such as storage. Using managed identities for authentication helps improve workspace security and management. For example, you can allow users to access training data only when logged in to a compute instance. Or use a common user-assigned managed identity to permit access to a specific storage account. \\n\\nYou can create compute instance with managed identity from Azure ML Studio:\\n\\n1.\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create).\\n1.\\tSelect **Next: Advanced Settings**.\\n1.\\tEnable **Assign a managed identity**.\\n1.  Select **System-assigned** or **User-assigned** under **Identity type**.\\n1.  If you selected **User-assigned**, select subscription and name of the identity.\\n\\nYou can use V2 CLI to create compute instance with assign system-assigned managed identity:\\n\\n```azurecli\\naz ml compute create --name myinstance --identity-type SystemAssigned --type ComputeInstance --resource-group my-resource-group --workspace-name my-workspace\\n```\\n\\nYou can also use V2 CLI with yaml file, for example to create a compute instance with user-assigned managed identity:\\n\\n```azurecli\\nazure ml compute create --file compute.yaml --resource-group my-resource-group --workspace-name my-workspace\\n```\\n\\nThe identity definition is contained in compute.yaml file:\\n\\n```yaml\\nhttps://azuremlschemas.azureedge.net/latest/computeInstance.schema.json\\nname: myinstance\\ntype: computeinstance\\nidentity:\\n  type: user_assigned\\n  user_assigned_identities: \\n    - resource_id: identity_resource_id\\n```\\n\\nOnce the managed identity is created, grant the managed identity at least Storage Blob Data Reader role on the storage account of the datastore, see [Accessing storage services](how-to-identity-based-service-authentication.md?tabs=cli#accessing-storage-services). Then, when you work on the compute instance, the managed identity is used automatically to authenticate against datastores.\\n\\n> [!NOTE]\\n> The name of the created system managed identity will be in the format /workspace-name/computes/compute-instance-name in your Azure Active Directory. \\n\\nYou can also use the managed identity manually to authenticate against other Azure resources. The following example shows how to use it to get an Azure Resource Manager access token:\\n\\n```python\\nimport requests\\n\\ndef get_access_token_msi(resource):\\n    client_id = os.environ.get(\"DEFAULT_IDENTITY_CLIENT_ID\", None)\\n    resp = requests.get(f\"{os.environ[\\'MSI_ENDPOINT\\']}?resource={resource}&clientid={client_id}&api-version=2017-09-01\", headers={\\'Secret\\': os.environ[\"MSI_SECRET\"]})\\n    resp.raise_for_status()\\n    return resp.json()[\"access_token\"]\\n\\narm_access_token = get_access_token_msi(\"https://management.azure.com\")\\n```\\n\\nTo use Azure CLI with the managed identity for authentication, specify the identity client ID as the username when logging in: \\n```azurecli\\naz login --identity --username $DEFAULT_IDENTITY_CLIENT_ID\\n```\\n\\n> [!NOTE]\\n> You cannot use ```azcopy``` when trying to use managed identity. ```azcopy login --identity``` will not work.\\n\\n## Add custom applications such as RStudio or Posit Workbench (preview)\\n\\n> [!IMPORTANT]\\n> Items marked (preview) below are currently in public preview.\\n> The preview version is provided without a service level agreement, and it\\'s not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.\\n> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\\n\\nYou can set up other applications, such as RStudio, or Posit Workbench (formerly RStudio Workbench), when creating a compute instance. Follow these steps in studio to set up a custom application on your compute instance\\n\\n1.\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create)\\n\\n', additional_kwargs={}), HumanMessage(content='how can I see the quota assigned to my workspace in the v2 cli?', additional_kwargs={})]), 'kwargs': {'stop': None}}\n",
      "json_result: generations=[ChatGeneration(text='You can use the following Azure CLI command to view the quotas for your Machine Learning workspace:\\n```\\naz ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\\n```\\n\\nReplace `<name>` with the name of the Machine Learning workspace and `<resource-group>` with the resource group that contains the workspace. You can also add `--compute-target` and `--ids` options if you have specific requirements for viewing compute targets or identity information.', generation_info=None, message=AIMessage(content='You can use the following Azure CLI command to view the quotas for your Machine Learning workspace:\\n```\\naz ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\\n```\\n\\nReplace `<name>` with the name of the Machine Learning workspace and `<resource-group>` with the resource group that contains the workspace. You can also add `--compute-target` and `--ids` options if you have specific requirements for viewing compute targets or identity information.', additional_kwargs={}))] llm_output={'token_usage': <OpenAIObject at 0x7fb6d055ffb0> JSON: {\n",
      "  \"completion_tokens\": 105,\n",
      "  \"prompt_tokens\": 2262,\n",
      "  \"total_tokens\": 2367\n",
      "}, 'model_name': 'gpt-3.5-turbo'}\n",
      "Q: how can I see the quota assigned to my workspace in the v2 cli?\n",
      "A: You can use the following Azure CLI command to view the quotas for your Machine Learning workspace:\n",
      "```\n",
      "az ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\n",
      "```\n",
      "\n",
      "Replace `<name>` with the name of the Machine Learning workspace and `<resource-group>` with the resource group that contains the workspace. You can also add `--compute-target` and `--ids` options if you have specific requirements for viewing compute targets or identity information.\n"
     ]
    }
   ],
   "source": [
    "!python src/langchain/rag_with_cog_search.py --question \"how can I see the quota assigned to my workspace in the v2 cli?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](data/amldocs/groundedness/conversation_context.json) is the output of the above command -- it contains both the query, the reply and the context based on which the reply was given.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": \"how can I see the quota assigned to my workspace in the v2 cli?\",\n",
    "  \"result\": \"You can use the following Azure CLI command to view the quotas for your Machine Learning workspace:\\n```\\naz ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\\n```\\n\\nReplace `<name>` with the name of the Machine Learning workspace and `<resource-group>` with the resource group that contains the workspace. You can also add `--compute-target` and `--ids` options if you have specific requirements for viewing compute targets or identity information.\",\n",
    "  \"context\": [\n",
    "    {\n",
    "      \"page_content\": \"\\nAfter deployment, this role becomes available in the specified workspace. Now you can add and assign this role in the Azure portal.\\n\\nFor more information on custom roles, see [Azure custom roles](../role-based-access-control/custom-roles.md). \\n\\n### Azure Machine Learning operations\\n\\nFor more information on the operations (actions and not actions) usable with custom roles, see [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices). You can also use the following Azure CLI command to list operations:\\n\\n```azurecli-interactive\\naz provider operation show â€“n Microsoft.MachineLearningServices\\n```\\n\\n## List custom roles\\n\\nIn the Azure CLI, run the following command:\\n\\n```azurecli-interactive\\naz role definition list --subscription <sub-id> --custom-role-only true\\n```\\n\\nTo view the role definition for a specific custom role, use the following Azure CLI command. The `<role-name>` should be in the same format returned by the command above:\\n\\n```azurecli-interactive\\naz role definition list -n <role-name> --subscription <sub-id>\\n```\\n\\n## Update a custom role\\n\\nIn the Azure CLI, run the following command:\\n\\n```azurecli-interactive\\naz role definition update --role-definition update_def.json --subscription <sub-id>\\n```\\n\\nYou need to have permissions on the entire scope of your new role definition. For example if this new role has a scope across three subscriptions, you need to have permissions on all three subscriptions. \\n\\n> [!NOTE]\\n> Role updates can take 15 minutes to an hour to apply across all role assignments in that scope.\\n\\n## Use Azure Resource Manager templates for repeatability\\n\\nIf you anticipate that you'll need to recreate complex role assignments, an Azure Resource Manager template can be a significant help. The [machine-learning-dependencies-role-assignment template](https://github.com/Azure/azure-quickstart-templates/tree/master//quickstarts/microsoft.machinelearningservices/machine-learning-dependencies-role-assignment) shows how role assignments can be specified in source code for reuse. \\n\\n## Common scenarios\\n\\nThe following table is a summary of Azure Machine Learning activities and the permissions required to perform them at the least scope. For example, if an activity can be performed with a workspace scope (Column 4), then all higher scope with that permission will also work automatically. Note that for certain activities the permissions differ between V1 and V2 APIs.\\n\\n> [!IMPORTANT]\\n> All paths in this table that start with `/` are **relative paths** to `Microsoft.MachineLearningServices/` :\\n\\n| Activity | Subscription-level scope | Resource group-level scope | Workspace-level scope |\\n| ----- | ----- | ----- | ----- |\\n| Create new workspace <sub>1</sub> | Not required | Owner or contributor | N/A (becomes Owner or inherits higher scope role after creation) |\\n| Request subscription level Amlcompute quota or set workspace level quota | Owner, or contributor, or custom role </br>allowing `/locations/updateQuotas/action`</br> at subscription scope | Not Authorized | Not Authorized |\\n| Create new compute cluster | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\n| Create new compute instance | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\n| Submitting any type of run (V1) | Not required | Not required | Owner, contributor, or custom role allowing: `\\\"/workspaces/*/read\\\", \\\"/workspaces/environments/write\\\", \\\"/workspaces/experiments/runs/write\\\", \\\"/workspaces/metadata/artifacts/write\\\", \\\"/workspaces/metadata/snapshots/write\\\", \\\"/workspaces/environments/build/action\\\", \\\"/workspaces/experiments/runs/submit/action\\\", \\\"/workspaces/environments/readSecrets/action\\\"` |\\n| Submitting any type of run (V2) | Not required | Not required | Owner, contributor, or custom role allowing: `\\\"/workspaces/*/read\\\", \\\"/workspaces/environments/write\\\", \\\"/workspaces/jobs/*\\\", \\\"/workspaces/metadata/artifacts/write\\\", \\\"/workspaces/metadata/codes/*/write\\\", \\\"/workspaces/environments/build/action\\\", \\\"/workspaces/environments/readSecrets/action\\\"` |\\n\",\n",
    "      \"metadata\": {\n",
    "        \"sourcefile\": \"UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-108.md\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"page_content\": \"\\n## Troubleshooting\\n\\nHere are a few things to be aware of while you use Azure role-based access control (Azure RBAC):\\n\\n- When you create a resource in Azure, such as a workspace, you're not directly the owner of the resource. Your role is inherited from the highest scope role that you're authorized against in that subscription. As an example if you're a Network Administrator, and have the permissions to create a Machine Learning workspace, you would be assigned the Network Administrator role against that workspace, and not the Owner role.\\n\\n- To perform quota operations in a workspace, you need subscription level permissions. This means setting either subscription level quota or workspace level quota for your managed compute resources can only happen if you have write permissions at the subscription scope.\\n\\n- When there are two role assignments to the same Azure Active Directory user with conflicting sections of Actions/NotActions, your operations listed in NotActions from one role might not take effect if they are also listed as Actions in another role. To learn more about how Azure parses role assignments, read [How Azure RBAC determines if a user has access to a resource](../role-based-access-control/overview.md#how-azure-rbac-determines-if-a-user-has-access-to-a-resource)\\n\\n- To deploy your compute resources inside a VNet, you need to explicitly have permissions for the following actions:\\n    - `Microsoft.Network/virtualNetworks/*/read` on the VNet resources.\\n    - `Microsoft.Network/virtualNetworks/subnets/join/action` on the subnet resource.\\n    \\n    For more information on Azure RBAC with networking, see the [Networking built-in roles](../role-based-access-control/built-in-roles.md#networking).\\n\\n- It can sometimes take up to 1 hour for your new role assignments to take effect over cached permissions across the stack.\\n\\n## Next steps\\n\\n- [Enterprise security overview](concept-enterprise-security.md)\\n- [Virtual network isolation and privacy overview](how-to-network-security-overview.md)\\n- [Tutorial: Train and deploy a model](tutorial-train-deploy-notebook.md)\\n- [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices)\\n\",\n",
    "      \"metadata\": {\n",
    "        \"sourcefile\": \"UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-573.md\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"page_content\": \"\\n## Assign managed identity\\n\\nYou can assign a system- or user-assigned [managed identity](../active-directory/managed-identities-azure-resources/overview.md) to a compute instance, to authenticate against other Azure resources such as storage. Using managed identities for authentication helps improve workspace security and management. For example, you can allow users to access training data only when logged in to a compute instance. Or use a common user-assigned managed identity to permit access to a specific storage account. \\n\\nYou can create compute instance with managed identity from Azure ML Studio:\\n\\n1.\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create).\\n1.\\tSelect **Next: Advanced Settings**.\\n1.\\tEnable **Assign a managed identity**.\\n1.  Select **System-assigned** or **User-assigned** under **Identity type**.\\n1.  If you selected **User-assigned**, select subscription and name of the identity.\\n\\nYou can use V2 CLI to create compute instance with assign system-assigned managed identity:\\n\\n```azurecli\\naz ml compute create --name myinstance --identity-type SystemAssigned --type ComputeInstance --resource-group my-resource-group --workspace-name my-workspace\\n```\\n\\nYou can also use V2 CLI with yaml file, for example to create a compute instance with user-assigned managed identity:\\n\\n```azurecli\\nazure ml compute create --file compute.yaml --resource-group my-resource-group --workspace-name my-workspace\\n```\\n\\nThe identity definition is contained in compute.yaml file:\\n\\n```yaml\\nhttps://azuremlschemas.azureedge.net/latest/computeInstance.schema.json\\nname: myinstance\\ntype: computeinstance\\nidentity:\\n  type: user_assigned\\n  user_assigned_identities: \\n    - resource_id: identity_resource_id\\n```\\n\\nOnce the managed identity is created, grant the managed identity at least Storage Blob Data Reader role on the storage account of the datastore, see [Accessing storage services](how-to-identity-based-service-authentication.md?tabs=cli#accessing-storage-services). Then, when you work on the compute instance, the managed identity is used automatically to authenticate against datastores.\\n\\n> [!NOTE]\\n> The name of the created system managed identity will be in the format /workspace-name/computes/compute-instance-name in your Azure Active Directory. \\n\\nYou can also use the managed identity manually to authenticate against other Azure resources. The following example shows how to use it to get an Azure Resource Manager access token:\\n\\n```python\\nimport requests\\n\\ndef get_access_token_msi(resource):\\n    client_id = os.environ.get(\\\"DEFAULT_IDENTITY_CLIENT_ID\\\", None)\\n    resp = requests.get(f\\\"{os.environ['MSI_ENDPOINT']}?resource={resource}&clientid={client_id}&api-version=2017-09-01\\\", headers={'Secret': os.environ[\\\"MSI_SECRET\\\"]})\\n    resp.raise_for_status()\\n    return resp.json()[\\\"access_token\\\"]\\n\\narm_access_token = get_access_token_msi(\\\"https://management.azure.com\\\")\\n```\\n\\nTo use Azure CLI with the managed identity for authentication, specify the identity client ID as the username when logging in: \\n```azurecli\\naz login --identity --username $DEFAULT_IDENTITY_CLIENT_ID\\n```\\n\\n> [!NOTE]\\n> You cannot use ```azcopy``` when trying to use managed identity. ```azcopy login --identity``` will not work.\\n\\n## Add custom applications such as RStudio or Posit Workbench (preview)\\n\\n> [!IMPORTANT]\\n> Items marked (preview) below are currently in public preview.\\n> The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.\\n> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\\n\\nYou can set up other applications, such as RStudio, or Posit Workbench (formerly RStudio Workbench), when creating a compute instance. Follow these steps in studio to set up a custom application on your compute instance\\n\\n1.\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create)\\n\",\n",
    "      \"metadata\": {\n",
    "        \"sourcefile\": \"UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-534.md\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now set up a labeling project that would show the user the query, the context and the reply and ask them to label the reply as grounded or not grounded. We will need to do that from time to time, but it would be nice to have an automated way to do this -- if fact, GPT-4 does an OK job at this. It is not perfect, but it is good enough to be useful.\n",
    "\n",
    "See here for the code of a groundedness check: [groundedness.py](src/langchain/groundedness.py) -- as you can see the meta_prompt reads very much like a briefing you would give to a human labeler.\n",
    "\n",
    "Now, let's run the groundedness check on the above query and see what it says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running offline, checking secrets\n",
      "json_call: {'module': 'langchain.chat_models.openai', 'name': '_generate', 'args': (AzureChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7f8141daafb0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4-32k', temperature=0.0, model_kwargs={}, openai_api_key='', request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='gpt-4-32k', openai_api_type='azure', openai_api_base='', openai_api_version='2023-03-15-preview'), [HumanMessage(content='\\n# You will be given a question posed by a user (Human) and the response that was given by a generative AI bot (System). \\nThis task is to check if factual information in the System\\'s reply is grounded.\\n\\nThe information is presented to you in a JSON format below with the following fields:\\nquery: The question posed by the Human.\\nresult: The System\\'s reply.\\ncontext: The context of the System\\'s reply based on which the System should generate the reply.\\n\\n- Background: The System does a database searche before responding to the Human. \\nThe search results are shown in the context section of the JSON below containing the text that was retrieved.\\nThe System\\'s reply should be directly taken from the page_content of the SearchResults, paraphrased from them, or \\ninferred from them using common knowledge. When the System responds to information that is not mentioned or could not be \\ninferred from the SearchResults, we call it a groundedness issue.\\n\\n- Please review the conversation below between the Human and the System, and identify and list any information in the System\\'s\\nreply that is not grounded. If the information is directly taken from the text of the SearchResults, paraphrased\\nfrom them, or inferred from them using common knowledge, we do not need to list it. If the information is from outside \\nsources, we need to list it. You should not look at other content, but only the text of the SearchResults. \\nYou can follow the below steps:\\n\\n    1. Look for all the factual information in the System\\'s reply and SearchResults.\\n\\n    2. Compare the facts in the reply with the facts in the SearchResults. Check if there are any facts that are not in the \\n    SearchResults at all, or that contradict or distort the facts in the SearchResults. If there are, write them down. If \\n    there are none, leave it blank. Note that some facts may be implied or suggested by the SearchResults, but not explicitly\\n    stated. In that case, use your best judgment to decide if the fact is grounded or not. For example, if the SearchResults \\n    mention that a film was nominated for 12 Oscars, and the System states the same, you can consider that fact as grounded, \\n    as it is directly taken from the SearchResults. However, if the SearchResults do not mention the film won any awards at\\n    all, and  the System states that the film won some awards, you should consider that fact as not grounded regardless of \\n    whether it wins or not. If the SearchResults mention the film won several awards, but not the exact number, and the System \\n    states the exact number, even if the exact number is correct, you should still consider that fact as ungrounded. \\n    Subjective claims are considered as grounded if the information is based on the SearchResults, we do not need any evidence.\\n\\n    3. Carefully check each piece of information in the reply, especially on the **number, date, person name, phone number, \\n    etc.** to **make sure to list all the ungrounded facts before writing output**.\\n\\n    4. Finally, Rate how well the System responds only using information from the SearchResults and common knowledge on a scale \\n    of 0-10 in \"rating_out_of_10: X\" format. If  the System used information from outside sources, or if  the System made claims that are not \\n    backed up by the SearchResults, give it a low score. If  the System **only** used information from the SearchResults and general \\n    information, give it a 10.\\n\\n```json\\n{\\'query\\': \\'how can I see the quota assigned to my workspace in the v2 cli?\\', \\'result\\': \\'You can use the following Azure CLI command to view the quotas for your Machine Learning workspace:\\\\n```\\\\naz ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\\\\n```\\\\n\\\\nReplace `<name>` with the name of the Machine Learning workspace and `<resource-group>` with the resource group that contains the workspace. You can also add `--compute-target` and `--ids` options if you have specific requirements for viewing compute targets or identity information.\\', \\'context\\': [{\\'page_content\\': \\'\\\\nAfter deployment, this role becomes available in the specified workspace. Now you can add and assign this role in the Azure portal.\\\\n\\\\nFor more information on custom roles, see [Azure custom roles](../role-based-access-control/custom-roles.md). \\\\n\\\\n### Azure Machine Learning operations\\\\n\\\\nFor more information on the operations (actions and not actions) usable with custom roles, see [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices). You can also use the following Azure CLI command to list operations:\\\\n\\\\n```azurecli-interactive\\\\naz provider operation show â€“n Microsoft.MachineLearningServices\\\\n```\\\\n\\\\n## List custom roles\\\\n\\\\nIn the Azure CLI, run the following command:\\\\n\\\\n```azurecli-interactive\\\\naz role definition list --subscription <sub-id> --custom-role-only true\\\\n```\\\\n\\\\nTo view the role definition for a specific custom role, use the following Azure CLI command. The `<role-name>` should be in the same format returned by the command above:\\\\n\\\\n```azurecli-interactive\\\\naz role definition list -n <role-name> --subscription <sub-id>\\\\n```\\\\n\\\\n## Update a custom role\\\\n\\\\nIn the Azure CLI, run the following command:\\\\n\\\\n```azurecli-interactive\\\\naz role definition update --role-definition update_def.json --subscription <sub-id>\\\\n```\\\\n\\\\nYou need to have permissions on the entire scope of your new role definition. For example if this new role has a scope across three subscriptions, you need to have permissions on all three subscriptions. \\\\n\\\\n> [!NOTE]\\\\n> Role updates can take 15 minutes to an hour to apply across all role assignments in that scope.\\\\n\\\\n## Use Azure Resource Manager templates for repeatability\\\\n\\\\nIf you anticipate that you\\\\\\'ll need to recreate complex role assignments, an Azure Resource Manager template can be a significant help. The [machine-learning-dependencies-role-assignment template](https://github.com/Azure/azure-quickstart-templates/tree/master//quickstarts/microsoft.machinelearningservices/machine-learning-dependencies-role-assignment) shows how role assignments can be specified in source code for reuse. \\\\n\\\\n## Common scenarios\\\\n\\\\nThe following table is a summary of Azure Machine Learning activities and the permissions required to perform them at the least scope. For example, if an activity can be performed with a workspace scope (Column 4), then all higher scope with that permission will also work automatically. Note that for certain activities the permissions differ between V1 and V2 APIs.\\\\n\\\\n> [!IMPORTANT]\\\\n> All paths in this table that start with `/` are **relative paths** to `Microsoft.MachineLearningServices/` :\\\\n\\\\n| Activity | Subscription-level scope | Resource group-level scope | Workspace-level scope |\\\\n| ----- | ----- | ----- | ----- |\\\\n| Create new workspace <sub>1</sub> | Not required | Owner or contributor | N/A (becomes Owner or inherits higher scope role after creation) |\\\\n| Request subscription level Amlcompute quota or set workspace level quota | Owner, or contributor, or custom role </br>allowing `/locations/updateQuotas/action`</br> at subscription scope | Not Authorized | Not Authorized |\\\\n| Create new compute cluster | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\\\n| Create new compute instance | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\\\\n| Submitting any type of run (V1) | Not required | Not required | Owner, contributor, or custom role allowing: `\"/workspaces/*/read\", \"/workspaces/environments/write\", \"/workspaces/experiments/runs/write\", \"/workspaces/metadata/artifacts/write\", \"/workspaces/metadata/snapshots/write\", \"/workspaces/environments/build/action\", \"/workspaces/experiments/runs/submit/action\", \"/workspaces/environments/readSecrets/action\"` |\\\\n| Submitting any type of run (V2) | Not required | Not required | Owner, contributor, or custom role allowing: `\"/workspaces/*/read\", \"/workspaces/environments/write\", \"/workspaces/jobs/*\", \"/workspaces/metadata/artifacts/write\", \"/workspaces/metadata/codes/*/write\", \"/workspaces/environments/build/action\", \"/workspaces/environments/readSecrets/action\"` |\\\\n\\', \\'metadata\\': {\\'sourcefile\\': \\'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-108.md\\'}}, {\\'page_content\\': \"\\\\n## Troubleshooting\\\\n\\\\nHere are a few things to be aware of while you use Azure role-based access control (Azure RBAC):\\\\n\\\\n- When you create a resource in Azure, such as a workspace, you\\'re not directly the owner of the resource. Your role is inherited from the highest scope role that you\\'re authorized against in that subscription. As an example if you\\'re a Network Administrator, and have the permissions to create a Machine Learning workspace, you would be assigned the Network Administrator role against that workspace, and not the Owner role.\\\\n\\\\n- To perform quota operations in a workspace, you need subscription level permissions. This means setting either subscription level quota or workspace level quota for your managed compute resources can only happen if you have write permissions at the subscription scope.\\\\n\\\\n- When there are two role assignments to the same Azure Active Directory user with conflicting sections of Actions/NotActions, your operations listed in NotActions from one role might not take effect if they are also listed as Actions in another role. To learn more about how Azure parses role assignments, read [How Azure RBAC determines if a user has access to a resource](../role-based-access-control/overview.md#how-azure-rbac-determines-if-a-user-has-access-to-a-resource)\\\\n\\\\n- To deploy your compute resources inside a VNet, you need to explicitly have permissions for the following actions:\\\\n    - `Microsoft.Network/virtualNetworks/*/read` on the VNet resources.\\\\n    - `Microsoft.Network/virtualNetworks/subnets/join/action` on the subnet resource.\\\\n    \\\\n    For more information on Azure RBAC with networking, see the [Networking built-in roles](../role-based-access-control/built-in-roles.md#networking).\\\\n\\\\n- It can sometimes take up to 1 hour for your new role assignments to take effect over cached permissions across the stack.\\\\n\\\\n## Next steps\\\\n\\\\n- [Enterprise security overview](concept-enterprise-security.md)\\\\n- [Virtual network isolation and privacy overview](how-to-network-security-overview.md)\\\\n- [Tutorial: Train and deploy a model](tutorial-train-deploy-notebook.md)\\\\n- [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices)\\\\n\", \\'metadata\\': {\\'sourcefile\\': \\'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-573.md\\'}}, {\\'page_content\\': \\'\\\\n## Assign managed identity\\\\n\\\\nYou can assign a system- or user-assigned [managed identity](../active-directory/managed-identities-azure-resources/overview.md) to a compute instance, to authenticate against other Azure resources such as storage. Using managed identities for authentication helps improve workspace security and management. For example, you can allow users to access training data only when logged in to a compute instance. Or use a common user-assigned managed identity to permit access to a specific storage account. \\\\n\\\\nYou can create compute instance with managed identity from Azure ML Studio:\\\\n\\\\n1.\\\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create).\\\\n1.\\\\tSelect **Next: Advanced Settings**.\\\\n1.\\\\tEnable **Assign a managed identity**.\\\\n1.  Select **System-assigned** or **User-assigned** under **Identity type**.\\\\n1.  If you selected **User-assigned**, select subscription and name of the identity.\\\\n\\\\nYou can use V2 CLI to create compute instance with assign system-assigned managed identity:\\\\n\\\\n```azurecli\\\\naz ml compute create --name myinstance --identity-type SystemAssigned --type ComputeInstance --resource-group my-resource-group --workspace-name my-workspace\\\\n```\\\\n\\\\nYou can also use V2 CLI with yaml file, for example to create a compute instance with user-assigned managed identity:\\\\n\\\\n```azurecli\\\\nazure ml compute create --file compute.yaml --resource-group my-resource-group --workspace-name my-workspace\\\\n```\\\\n\\\\nThe identity definition is contained in compute.yaml file:\\\\n\\\\n```yaml\\\\nhttps://azuremlschemas.azureedge.net/latest/computeInstance.schema.json\\\\nname: myinstance\\\\ntype: computeinstance\\\\nidentity:\\\\n  type: user_assigned\\\\n  user_assigned_identities: \\\\n    - resource_id: identity_resource_id\\\\n```\\\\n\\\\nOnce the managed identity is created, grant the managed identity at least Storage Blob Data Reader role on the storage account of the datastore, see [Accessing storage services](how-to-identity-based-service-authentication.md?tabs=cli#accessing-storage-services). Then, when you work on the compute instance, the managed identity is used automatically to authenticate against datastores.\\\\n\\\\n> [!NOTE]\\\\n> The name of the created system managed identity will be in the format /workspace-name/computes/compute-instance-name in your Azure Active Directory. \\\\n\\\\nYou can also use the managed identity manually to authenticate against other Azure resources. The following example shows how to use it to get an Azure Resource Manager access token:\\\\n\\\\n```python\\\\nimport requests\\\\n\\\\ndef get_access_token_msi(resource):\\\\n    client_id = os.environ.get(\"DEFAULT_IDENTITY_CLIENT_ID\", None)\\\\n    resp = requests.get(f\"{os.environ[\\\\\\'MSI_ENDPOINT\\\\\\']}?resource={resource}&clientid={client_id}&api-version=2017-09-01\", headers={\\\\\\'Secret\\\\\\': os.environ[\"MSI_SECRET\"]})\\\\n    resp.raise_for_status()\\\\n    return resp.json()[\"access_token\"]\\\\n\\\\narm_access_token = get_access_token_msi(\"https://management.azure.com\")\\\\n```\\\\n\\\\nTo use Azure CLI with the managed identity for authentication, specify the identity client ID as the username when logging in: \\\\n```azurecli\\\\naz login --identity --username $DEFAULT_IDENTITY_CLIENT_ID\\\\n```\\\\n\\\\n> [!NOTE]\\\\n> You cannot use ```azcopy``` when trying to use managed identity. ```azcopy login --identity``` will not work.\\\\n\\\\n## Add custom applications such as RStudio or Posit Workbench (preview)\\\\n\\\\n> [!IMPORTANT]\\\\n> Items marked (preview) below are currently in public preview.\\\\n> The preview version is provided without a service level agreement, and it\\\\\\'s not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.\\\\n> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\\\\n\\\\nYou can set up other applications, such as RStudio, or Posit Workbench (formerly RStudio Workbench), when creating a compute instance. Follow these steps in studio to set up a custom application on your compute instance\\\\n\\\\n1.\\\\tFill out the form to [create a new compute instance](?tabs=azure-studio#create)\\\\n\\', \\'metadata\\': {\\'sourcefile\\': \\'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-534.md\\'}}]}\\n```\\n# Output \\n\\nReminder: please check each piece of information in the reply carefully, especially on the **number, date, person name, \\nphone number, etc.** to **make sure to list all the ungrounded facts before writing output**.\\n\\nIn JSON format repeat the User\\'s question and the System\\'s reply, then complete the ungrounded facts task described above. \\nIf you give a rating of less than 10, you always have to provide ungrounded facts. \\nThe resulting format should look like this:\\n\\n```json\\n{\\n    \"question\": \"<question>\",\\n    \"reply\": \"<reply>\",\\n    \"ungrounded_facts\": [\\n        \"<fact 1>\",\\n        \"<fact 2>\",\\n        \"...\"\\n    ],\\n    \"rating_out_of_10\": X\\n}\\n```\\nMake sure to properly escape the question, reply and ungrounded_facts fields -- the resulting JSON should be valid.\\n', additional_kwargs={})]), 'kwargs': {'stop': None}}\n",
      "json_result: generations=[ChatGeneration(text='{\\n    \"question\": \"how can I see the quota assigned to my workspace in the v2 cli?\",\\n    \"reply\": \"You can use the following Azure CLI command to view the quotas for your Machine Learning workspace:\\\\n```\\\\naz ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\\\\n```\\\\n\\\\nReplace `<name>` with the name of the Machine Learning workspace and `<resource-group>` with the resource group that contains the workspace. You can also add `--compute-target` and `--ids` options if you have specific requirements for viewing compute targets or identity information.\",\\n    \"ungrounded_facts\": [\\n        \"az ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\"\\n    ],\\n    \"rating_out_of_10\": 0\\n}', generation_info=None, message=AIMessage(content='{\\n    \"question\": \"how can I see the quota assigned to my workspace in the v2 cli?\",\\n    \"reply\": \"You can use the following Azure CLI command to view the quotas for your Machine Learning workspace:\\\\n```\\\\naz ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\\\\n```\\\\n\\\\nReplace `<name>` with the name of the Machine Learning workspace and `<resource-group>` with the resource group that contains the workspace. You can also add `--compute-target` and `--ids` options if you have specific requirements for viewing compute targets or identity information.\",\\n    \"ungrounded_facts\": [\\n        \"az ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\"\\n    ],\\n    \"rating_out_of_10\": 0\\n}', additional_kwargs={}))] llm_output={'token_usage': <OpenAIObject at 0x7f8141f01f30> JSON: {\n",
      "  \"completion_tokens\": 191,\n",
      "  \"prompt_tokens\": 3554,\n",
      "  \"total_tokens\": 3745\n",
      "}, 'model_name': 'gpt-4-32k'}\n",
      "{\n",
      "    \"question\": \"how can I see the quota assigned to my workspace in the v2 cli?\",\n",
      "    \"reply\": \"You can use the following Azure CLI command to view the quotas for your Machine Learning workspace:\\n```\\naz ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\\n```\\n\\nReplace `<name>` with the name of the Machine Learning workspace and `<resource-group>` with the resource group that contains the workspace. You can also add `--compute-target` and `--ids` options if you have specific requirements for viewing compute targets or identity information.\",\n",
      "    \"ungrounded_facts\": [\n",
      "        \"az ml workspace compute show-quota --name <name> --resource-group <resource-group> [--compute-target <compute-target>] [--ids]\"\n",
      "    ],\n",
      "    \"rating_out_of_10\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python src/langchain/groundedness.py --conversation_context data/amldocs/groundedness/conversation_context.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... it does indeed flag the wrong command as an **ungrounded fact** and gives a groundedness rating of 0/10. So, now we can actually measure the groundedness of our `RetrievalQA` chain -- let's run the groundedness check on all the queries in the test set and see how it does. To do this, we are running both [rag_with_cog_search.py](src/langchain/rag_with_cog_search.py) and [groundedness.py](src/langchain/groundedness.py) in batch and we connect the batches in a pipeline -- this is where AzureML comes in handy. \n",
    "\n",
    "Here is the run: [jovial_yogurt_56h2qwvj96](https://ml.azure.com/runs/jovial_yogurt_56h2qwvj96?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/ray/workspaces/ray)\n",
    "\n",
    "Once it has completed, each of the questions have been run through the rag to give us a reply and then the reply has been checked for groundedness. The output of the "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/short-pipeline.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each individual groundedness check is a child run of the groundedness check and we can see the average groundedness value given by GPT-4 as a metric of this run. The average groundedness value is 7.9875 -- it is nothing I would take to the bank, but it is a metric we can look at when we make changes to the rag in order to make it better. After all, that's what we want to do -- we want to systematically make the rag better and better until it is good enough to be useful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different Prompts\n",
    "\n",
    "So, let's try to make it better by changing the prompt used in the RAG. Instead of the default prompt that is being used, let's use one that is asking the model to give us a more concise answer. Instead of the default prompt:\n",
    "\n",
    "#### System_prompt:\n",
    "```text\n",
    "Use the following pieces of context to answer the users question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "```\n",
    "\n",
    "#### User_prompt:\n",
    "```text\n",
    "{question}\n",
    "```\n",
    "\n",
    "We will use the following prompt: [rag_brief.mime](data/amldocs/prompts/rag_brief.mime)\n",
    "\n",
    "#### System_prompt:\n",
    "```text\n",
    "Use the following pieces of context to answer the users question in a brief way. Don't use more than 100 words. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "```\n",
    "\n",
    "#### User_prompt:\n",
    "```text\n",
    "Please give a short answer to the following question. Remember to be brief and not use more than 100 words.\n",
    "And, do not make stuff up -- if you don't know the answer, just say so.\n",
    "{question}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run those two prompts in parallel and compare the results.\n",
    "\n",
    "Here is the run [rate_limit_rerun_loyal_brush_8w6sp6rf9q](https://ml.azure.com/experiments/id/191370e3-619a-48c6-b1ca-09f5498e7e11/runs/45d21b27-0f3d-46ec-86f6-194d8cd1b1a3?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/ray/workspaces/ray&tid=72f988bf-86f1-41af-91ab-2d7cd011db47#)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/long-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
