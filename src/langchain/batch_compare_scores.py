from aml_keyvault import load_secrets
load_secrets(["OPENAI_API_KEY", "OPENAI_API_BASE"])

# set up openai api
import openai, os
openai.api_type = "azure"
openai.api_version = "2022-12-01"
openai.api_base = os.environ["OPENAI_API_BASE"]
openai.api_key = os.environ["OPENAI_API_KEY"]

import mlflow
import json, tempfile
import os, re
from patch import log_json_artifact
from langchain import PromptTemplate
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from rag_with_cog_search import load_prompt_templates

default_system_template = """
# Comparative labeling task for chatbot messages

Description: You will be reviewing and comparing two different messages generated by a chatbot in response to the same question, and selecting the better message based on provided criteria. This is an offline task, meaning you will not be responding in real-time to users.

Tasks:

1. Review the two chat messages (response A and response B) generated by the chatbot in response to the same question.
2. Evaluate each message based on the provided criteria:
    - Is the message relevant to the user's question?
    - Is the message helpful or informative?
    - Does the message address the user's question or concern?
    - Is the message grammatically correct and free of errors?
    - Is the message concise and easy to understand?
3. Select label A if you think A is the better reply, select label B if you think B is the better reply. 
Choose "No preference" if you think both messages are equally good or equally bad.
"""

default_user_template = """
# Question

{question}

# Response A

{response_a}

# Response B

{response_b}

# Label

Preferred response: 
"""

from langchain.chat_models import AzureChatOpenAI
from patch import patch_langchain, log_function_call

def robust_result_parse(result, conversation_context):
    try: 
        result_dict = json.loads(result.content)
        rating = result_dict["rating_out_of_10"]
        assert isinstance(rating, int)
    except Exception as e:
        print("Error parsing result json", e)
        print(result.content)
        pattern = r"\"rating_out_of_10\":\s*(.*?)\n"
        result_dict = conversation_context
        rating = re.search(pattern, result.content)
        if rating:
            try:
                rating = int(rating.group(1))
            except Exception:
                rating = 0
        else:   
            rating = 0

        result_dict["rating_out_of_10"] = int(rating)
        result_dict["error"] = f"Error parsing result json: {e}"

    return result_dict
  

def compare_scores(option_a_file, option_b_file, scores_file, meta_prompt): 
        
    system_template, user_template = load_prompt_templates(meta_prompt)
    system_template = system_template or default_system_template
    user_template = user_template or default_user_template

    user_prompt_template = PromptTemplate(
        input_variables=["question", "response_a", "response_b"],
        template=user_template,
    )

    # create the llm
    llm = AzureChatOpenAI(
        deployment_name="gpt-4-32k",
        temperature=0,
        openai_api_version="2023-03-15-preview",
        model_name="gpt-4-32k"
    )

    user_prompt = PromptTemplate(template=user_template, input_variables=["question", "response_a", "response_b"])
    system_prompt = system_template # no parameters yet

    paired_list = build_paired_list(option_a_file, option_b_file)
    scores = []
    for i, (left, right) in enumerate(paired_list):
        assert left["query"] == right["query"]
        user_message = user_prompt.format(question=left["query"] , response_a=left["result"], response_b=right["result"])
        result = llm([SystemMessage(content=system_prompt),HumanMessage(content=user_message)])
        result_dict = dict(question=left["query"], response_a=left["result"], response_b=right["result"], result=str(result.content))
        scores.append(result_dict)
        print("Scored", i, "of", len(paired_list), "prompts")
        mlflow.log_metric("scored", i)
        mlflow.log_metric("total", len(paired_list))
        # write to file (use os.path.join to join paths)
        with open(scores_file, "w") as f:
            json.dump(scores, f, indent=4)

    return scores

def load_json(filename):
    with open(filename, 'r') as f:
        return json.load(f)
    
def load_textfile(filename):
    with open(filename, 'r') as f:
        return f.read()

def build_paired_list(option_a_file, option_b_file): 
    import json
    with open(option_a_file) as f:
        option_a = json.load(f)

    with open(option_b_file) as f:
        option_b = json.load(f)

    paired_list = []
    for i, left in enumerate(option_a):
        # find the right
        right = [x for x in option_b if x["query"] == left["query"]]
        if len(right) == 0:
            print("No match found -- skipping")
            continue
        right = right[0]

        paired_list.append((left, right))

    return paired_list

def create_labeling_files(option_a_file, option_b_file, output_folder, meta_prompt):
    import json, os
    
    system_template, user_template = load_prompt_templates(meta_prompt)
    system_template = system_template or default_system_template
    user_template = user_template or default_user_template

    labeling_prompt_template = PromptTemplate(template=user_template, input_variables=["question", "response_a", "response_b"])
    
    paired_list = build_paired_list(option_a_file, option_b_file)
    for i, (left, right) in enumerate(paired_list):
        assert left["query"] == right["query"]

        prompt = labeling_prompt_template.format(question=left["query"] , response_a=left["result"], response_b=right["result"])
        # write to file (use os.path.join to join paths)
        filename = os.path.join(output_folder, f"{i}.md")
        with open(filename, "w") as f:
            f.write(prompt)
        print("Wrote to file", filename)

    print("Wrote", len(paired_list), "prompts to", output_folder)


if __name__ == "__main__":
    patch_langchain()

    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--option_a_scores", type=str, default="data/amldocs/scores/rag-scores-brief-prompt-3.json")
    parser.add_argument("--option_b_scores", type=str, default="data/amldocs/scores/rag-scores-default-prompt-3.json")
    parser.add_argument("--labelling_project_folder", type=str, default="data/amldocs/compare_labeling/project")
    parser.add_argument("--scores", type=str, default="data/amldocs/compare_labeling/scores.json")
    parser.add_argument("--meta_prompt", type=str, default=None)
    args = parser.parse_args()
    mlflow.log_param("option_a_scores", args.option_a_scores)
    mlflow.log_param("option_b_scores", args.option_b_scores)
    mlflow.log_param("labelling_project_folder", args.labelling_project_folder)
    mlflow.log_param("scores", args.scores)
    mlflow.log_param("meta_prompt", args.meta_prompt)

    # write the labeling project first
    create_labeling_files(args.option_a_scores, 
                            args.option_b_scores, 
                            args.labelling_project_folder, 
                            meta_prompt=args.meta_prompt)


    # then run the scoring
    scores = compare_scores(args.option_a_scores, args.option_b_scores, args.scores, meta_prompt=args.meta_prompt)
    print("Scores:", scores)
    mlflow.log_metric("scored", len(scores))
    mlflow.log_metric("total", len(scores))

    # count results
    from collections import Counter
    counts = Counter([x["result"] for x in scores])
    print("Counts:", counts)

    # log counts
    for k, v in counts.items():
        mlflow.log_metric(k, v)

    # create a bar chart of the counts
    import matplotlib.pyplot as plt
    plt.bar(counts.keys(), counts.values())
    # save the plot to a mlflow
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_file = os.path.join(temp_dir, "count.png")
        plt.savefig(temp_file, bbox_inches='tight')
        # Log the file as an artifact
        mlflow.log_artifact(temp_file)


