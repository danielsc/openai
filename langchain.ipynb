{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Endpoint: https://aoai.openai.azure.com/\n",
      "MLFlow Tracking URI:  azureml://eastus2.api.azureml.ms/mlflow/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/ray/providers/Microsoft.MachineLearningServices/workspaces/ray\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai, os\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "print(\"OpenAI Endpoint:\", openai.api_base)\n",
    "print(\"MLFlow Tracking URI: \", os.getenv(\"MLFLOW_TRACKING_URI\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_call: {'module': 'langchain.chat_models.openai', 'name': '_generate', 'args': (AzureChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='', request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='gpt-35-turbo', openai_api_type='azure', openai_api_base='', openai_api_version='2023-03-15-preview'), [SystemMessage(content='You are a friendly chat agent.', additional_kwargs={}), HumanMessage(content='Hello, how are you?', additional_kwargs={})]), 'kwargs': {'stop': None}}\n",
      "json_result: generations=[ChatGeneration(text=\"Hello! I'm just an AI language model, so I don't have feelings, but I am programmed to assist and have a conversation with you. How can I help you today?\", generation_info=None, message=AIMessage(content=\"Hello! I'm just an AI language model, so I don't have feelings, but I am programmed to assist and have a conversation with you. How can I help you today?\", additional_kwargs={}))] llm_output={'token_usage': <OpenAIObject at 0x7fa000b8e340> JSON: {\n",
      "  \"completion_tokens\": 37,\n",
      "  \"prompt_tokens\": 26,\n",
      "  \"total_tokens\": 63\n",
      "}, 'model_name': 'gpt-3.5-turbo'}\n",
      "Hello! I'm just an AI language model, so I don't have feelings, but I am programmed to assist and have a conversation with you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    ")\n",
    "\n",
    "reply = chat([SystemMessage(content=\"You are a friendly chat agent.\"),\n",
    "              HumanMessage(content=\"Hello, how are you?\")])\n",
    "\n",
    "print(reply.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion Model\n",
    "\n",
    "### Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "text=\"\"\"\n",
    "Classify the following news headline into 1 of the following categories: Business, Tech, Politics, Sport, Entertainment\n",
    "\n",
    "Headline: Major Retailer Announces Plans to Close Over 100 Stores\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text=\"\"\"\n",
    "Classify the following news headline into 1 of the following categories: Business, Tech, Politics, Sport, Entertainment\n",
    "\n",
    "Headline 1: Donna Steffensen Is Cooking Up a New Kind of Perfection. The Internetâ€™s most beloved cooking guru has a buzzy new book and a fresh new perspective\n",
    "Category: Entertainment\n",
    "\n",
    "Headline 2: Major Retailer Announces Plans to Close Over 100 Stores\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The answer is x = 4.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text=\"\"\"\n",
    "what is the anwer to this equation: 3x - 7 = -2x + 5\n",
    "\"\"\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12/5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "x = symbols('x')\n",
    "solve(3*x - 7 + 2*x - 5, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Add 7 to both sides of the equation.\n",
      "3x - 7 + 7 = -2x + 5 + 7\n",
      "\n",
      "Step 2: Simplify the left side of the equation.\n",
      "3x = -2x + 12\n",
      "\n",
      "Step 3: Subtract -2x from both sides of the equation.\n",
      "3x - (-2x) = -2x + 12 - (-2x)\n",
      "\n",
      "Step 4: Simplify the left side of the equation.\n",
      "5x = 12\n",
      "\n",
      "Step 5: Divide both sides of the equation by 5.\n",
      "x = 12/5\n",
      "\n",
      "Step 6: Simplify the right side of the equation.\n",
      "x = 2.4\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text=\"\"\"\n",
    "solve the following algebraic equation step-by-step and explain you reasoning at each step: 3x - 7 = -2x + 5\n",
    "\"\"\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/chains/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_call: {'module': 'langchain.chat_models.openai', 'name': '_generate', 'args': (AzureChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='', request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='gpt-35-turbo', openai_api_type='azure', openai_api_base='', openai_api_version='2023-03-15-preview'), [HumanMessage(content='What is a good name for a company that makes organic snacks?', additional_kwargs={})]), 'kwargs': {'stop': None}}\n",
      "json_result: generations=[ChatGeneration(text='Here are 5 potential names: \\n\\n1. EarthBites \\n2. Purely Nosh \\n3. Wholesome Munchies \\n4. FreshFusion \\n5. NatureNibbles', generation_info=None, message=AIMessage(content='Here are 5 potential names: \\n\\n1. EarthBites \\n2. Purely Nosh \\n3. Wholesome Munchies \\n4. FreshFusion \\n5. NatureNibbles', additional_kwargs={}))] llm_output={'token_usage': <OpenAIObject at 0x7fa0008de6b0> JSON: {\n",
      "  \"completion_tokens\": 41,\n",
      "  \"prompt_tokens\": 21,\n",
      "  \"total_tokens\": 62\n",
      "}, 'model_name': 'gpt-3.5-turbo'}\n",
      "Here are 5 potential names: \n",
      "\n",
      "1. EarthBites \n",
      "2. Purely Nosh \n",
      "3. Wholesome Munchies \n",
      "4. FreshFusion \n",
      "5. NatureNibbles\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"organic snacks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.langchain.patch import patch_langchain\n",
    "\n",
    "patch_langchain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "json_call: {'module': 'langchain.llms.openai', 'name': '_generate', 'args': (AzureOpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, deployment_name='text-davinci-003'), [\"Answer the following questions as best you can. You have access to the following tools:\\n\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: I have 1000 dollars. given today's governement bond rate, how much money would I have in 2 years if I invested them in 2 year government bonds?\\nThought:\"]), 'kwargs': {'stop': ['\\nObservation:', '\\n\\tObservation:']}}\n",
      "json_result: generations=[[Generation(text=' I need to know the interest rate of the bonds\\nAction: Search\\nAction Input: \"2 year government bond rate\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 208, 'total_tokens': 233, 'completion_tokens': 25}, 'model_name': 'text-davinci-003'}\n",
      "\u001b[32;1m\u001b[1;3m I need to know the interest rate of the bonds\n",
      "Action: Search\n",
      "Action Input: \"2 year government bond rate\"\u001b[0mjson_call: {'module': 'langchain.utilities.serpapi', 'name': 'run', 'args': (SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='22b4aa755585902707d009ef0baef623083911b8323fb6d9ae5f36b1482e3782', aiosession=None), '2 year government bond rate'), 'kwargs': {}}\n",
      "json_result: 2 Year Treasury Rate is at 4.08%, compared to 3.96% the previous market day and 2.47% last year. This is higher than the long term average of 3.16%. The 2 Year Treasury Rate is the yield received for investing in a US government issued treasury security that has a maturity of 2 years.\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m2 Year Treasury Rate is at 4.08%, compared to 3.96% the previous market day and 2.47% last year. This is higher than the long term average of 3.16%. The 2 Year Treasury Rate is the yield received for investing in a US government issued treasury security that has a maturity of 2 years.\u001b[0m\n",
      "Thought:json_call: {'module': 'langchain.llms.openai', 'name': '_generate', 'args': (AzureOpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, deployment_name='text-davinci-003'), ['Answer the following questions as best you can. You have access to the following tools:\\n\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: I have 1000 dollars. given today\\'s governement bond rate, how much money would I have in 2 years if I invested them in 2 year government bonds?\\nThought: I need to know the interest rate of the bonds\\nAction: Search\\nAction Input: \"2 year government bond rate\"\\nObservation: 2 Year Treasury Rate is at 4.08%, compared to 3.96% the previous market day and 2.47% last year. This is higher than the long term average of 3.16%. The 2 Year Treasury Rate is the yield received for investing in a US government issued treasury security that has a maturity of 2 years.\\nThought:']), 'kwargs': {'stop': ['\\nObservation:', '\\n\\tObservation:']}}\n",
      "json_result: generations=[[Generation(text=' I need to calculate the amount of money I will have in 2 years\\nAction: Calculator\\nAction Input: 1000 * (1 + 0.0408)^2', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 308, 'total_tokens': 342, 'completion_tokens': 34}, 'model_name': 'text-davinci-003'}\n",
      "\u001b[32;1m\u001b[1;3m I need to calculate the amount of money I will have in 2 years\n",
      "Action: Calculator\n",
      "Action Input: 1000 * (1 + 0.0408)^2\u001b[0mjson_call: {'module': 'langchain.chains.base', 'name': 'run', 'args': (LLMMathChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, verbose=False, llm=AzureOpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, deployment_name='text-davinci-003'), prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into Python code that can be executed in Python 3 REPL. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```python\\n${{Code that solves the problem and prints the solution}}\\n```\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n\\n```python\\nprint(37593 * 67)\\n```\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), input_key='question', output_key='answer'), '1000 * (1 + 0.0408)^2'), 'kwargs': {}}\n",
      "json_call: {'module': 'langchain.llms.openai', 'name': '_generate', 'args': (AzureOpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, deployment_name='text-davinci-003'), ['Translate a math problem into Python code that can be executed in Python 3 REPL. Use the output of running this code to answer the question.\\n\\nQuestion: ${Question with math problem.}\\n```python\\n${Code that solves the problem and prints the solution}\\n```\\n```output\\n${Output of running the code}\\n```\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n\\n```python\\nprint(37593 * 67)\\n```\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 1000 * (1 + 0.0408)^2\\n']), 'kwargs': {'stop': ['```output']}}\n",
      "json_result: generations=[[Generation(text='\\n```python\\nprint(1000 * (1 + 0.0408)**2)\\n```\\n', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 144, 'total_tokens': 168, 'completion_tokens': 24}, 'model_name': 'text-davinci-003'}\n",
      "json_call: {'module': 'langchain.python', 'name': 'run', 'args': (PythonREPL(globals={}, locals={}), '\\nprint(1000 * (1 + 0.0408)**2)'), 'kwargs': {}}\n",
      "json_result: 1083.2646399999999\n",
      "\n",
      "json_result: Answer: 1083.2646399999999\n",
      "\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1083.2646399999999\n",
      "\u001b[0m\n",
      "Thought:json_call: {'module': 'langchain.llms.openai', 'name': '_generate', 'args': (AzureOpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, deployment_name='text-davinci-003'), ['Answer the following questions as best you can. You have access to the following tools:\\n\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: I have 1000 dollars. given today\\'s governement bond rate, how much money would I have in 2 years if I invested them in 2 year government bonds?\\nThought: I need to know the interest rate of the bonds\\nAction: Search\\nAction Input: \"2 year government bond rate\"\\nObservation: 2 Year Treasury Rate is at 4.08%, compared to 3.96% the previous market day and 2.47% last year. This is higher than the long term average of 3.16%. The 2 Year Treasury Rate is the yield received for investing in a US government issued treasury security that has a maturity of 2 years.\\nThought: I need to calculate the amount of money I will have in 2 years\\nAction: Calculator\\nAction Input: 1000 * (1 + 0.0408)^2\\nObservation: Answer: 1083.2646399999999\\n\\nThought:']), 'kwargs': {'stop': ['\\nObservation:', '\\n\\tObservation:']}}\n",
      "json_result: generations=[[Generation(text=' I now know the final answer\\nFinal Answer: In two years, I will have 1083.26 dollars if I invest 1000 dollars in 2 year government bonds.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 360, 'total_tokens': 393, 'completion_tokens': 33}, 'model_name': 'text-davinci-003'}\n",
      "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: In two years, I will have 1083.26 dollars if I invest 1000 dollars in 2 year government bonds.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In two years, I will have 1083.26 dollars if I invest 1000 dollars in 2 year government bonds.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI, AzureOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import openai, os\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=\"text-davinci-003\", model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "agent.run(\"I have 1000 dollars. given today's governement bond rate, how much money would I have in 2 years if I invested them in 2 year government bonds?\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Retrieval Augmented Generation\"-Style Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "class CognitiveSearchRetriever(BaseRetriever):\n",
    "    def __init__(self, endpoint: str, index_name: str, searchkey: str, top: int = 3):\n",
    "        self.endpoint = endpoint\n",
    "        self.index_name = index_name\n",
    "        self.searchkey = searchkey\n",
    "        self.top = top\n",
    "        self.client = SearchClient(endpoint=endpoint, index_name=index_name, credential=AzureKeyCredential(searchkey))\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        docs = []\n",
    "        for i in self.client.search(query, top=self.top):\n",
    "            docs.append(Document(page_content=i['content'], metadata={\"sourcefile\": i['sourcefile']}))\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cog_search = CognitiveSearchRetriever(endpoint=os.getenv(\"COG_SEARCH_ENDPOINT\"), index_name=\"amldocs\", searchkey=os.environ[\"COG_SEARCH_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='   > You can publish multiple pipelines to a single endpoint. Each pipeline in a given endpoint is given a version number, which you can specify when you call the pipeline endpoint.\\n\\n1. Select **Publish**.\\n\\n## Retrain your model\\n\\nNow that you have a published training pipeline, you can use it to retrain your model on new data. You can submit jobs from a pipeline endpoint from the studio workspace or programmatically.\\n\\n### Submit jobs by using the studio portal\\n\\nUse the following steps to submit a parameterized pipeline endpoint job from the studio portal:\\n\\n1. Go to the **Endpoints** page in your studio workspace.\\n1. Select the **Pipeline endpoints** tab. Then, select your pipeline endpoint.\\n1. Select the **Published pipelines** tab. Then, select the pipeline version that you want to run.\\n1. Select **Submit**.\\n1. In the setup dialog box, you can specify the parameters values for the job. For this example, update the data path to train your model using a non-US dataset.\\n\\n![Screenshot that shows how to set up a parameterized pipeline job in the designer](./media/how-to-retrain-designer/published-pipeline-run.png)\\n\\n### Submit jobs by using code\\n\\nYou can find the REST endpoint of a published pipeline in the overview panel. By calling the endpoint, you can retrain the published pipeline.\\n\\nTo make a REST call, you need an OAuth 2.0 bearer-type authentication header. For information about setting up authentication to your workspace and making a parameterized REST call, see [Build an Azure Machine Learning pipeline for batch scoring](tutorial-pipeline-batch-scoring-classification.md#publish-and-run-from-a-rest-endpoint).\\n\\n## Next steps\\n\\nIn this article, you learned how to create a parameterized training pipeline endpoint using the designer.\\n\\nFor a complete walkthrough of how you can deploy a model to make predictions, see the [designer tutorial](tutorial-designer-automobile-price-train-score.md) to train and deploy a regression model.\\n\\nFor how to publish and submit a job to pipeline endpoint using the SDK v1, see [this article](v1/how-to-deploy-pipelines.md).\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-retrain-designer-74.md'}),\n",
       " Document(page_content='\\n# Create and run machine learning pipelines using components with the Azure Machine Learning studio\\n\\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\\n\\nIn this article, you\\'ll learn how to create and run [machine learning pipelines](concept-ml-pipelines.md) by using the Azure Machine Learning studio and [Components](concept-component.md). You can create pipelines without using components, but components offer better amount of flexibility and reuse. Azure ML Pipelines may be defined in YAML and [run from the CLI](how-to-create-component-pipelines-cli.md), [authored in Python](how-to-create-component-pipeline-python.md), or composed in Azure ML Studio Designer with a drag-and-drop UI. This document focuses on the AzureML studio designer UI.\\n\\n## Prerequisites\\n\\n* If you don\\'t have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\\n\\n* An Azure Machine Learning workspace[Create workspace resources](quickstart-create-resources.md).\\n\\n* [Install and set up the Azure CLI extension for Machine Learning](how-to-configure-cli.md).\\n\\n* Clone the examples repository:\\n\\n    ```azurecli-interactive\\n    git clone https://github.com/Azure/azureml-examples --depth 1\\n    cd azureml-examples/cli/jobs/pipelines-with-components/\\n    ```\\n\\n## Register component in your workspace\\n\\n>[!Note]\\n> Designer supports two type of components, classic prebuilt components and custom components. These two types of components are not compatible.  \\n>\\n>Classic prebuilt components provides prebuilt components majorly for data processing and traditional machine learning tasks like regression and classification. This type of component continues to be supported but will not have any new components added.\\n>\\n>\\n>Custom components allow you to provide your own code as a component. It supports sharing across workspaces and seamless authoring across Studio, CLI, and SDK interfaces.\\n>\\n>This article applies to custom components. \\n\\nTo build pipeline using components in UI, you need to register components to your workspace first. You can use CLI or SDK to register components to your workspace, so that you can share and reuse the component within the workspace. Registered components support automatic versioning so you can update the component but assure that pipelines that require an older version will continue to work.  \\n\\nIn the example below take using CLI for example. If you want to learn more about how to build a component, see [Create and run pipelines using components with  CLI](how-to-create-component-pipelines-cli.md).\\n\\n1. From the `cli/jobs/pipelines-with-components/basics` directory of the [`azureml-examples` repository](https://github.com/Azure/azureml-examples), navigate to the `1b_e2e_registered_components` subdirectory.\\n\\n1. Register the components to AzureML workspace using following commands. Learn more about [ML components](concept-component.md).\\n\\n    ```CLI\\n    az ml component create --file train.yml\\n    az ml component create --file score.yml\\n    az ml component create --file eval.yml\\n    ```\\n\\n1. After register component successfully, you can see your component in the studio UI.\\n\\n:::image type=\"content\" source=\"./media/how-to-create-component-pipelines-ui/component-page.png\" alt-text=\"Screenshot showing registered component in component page.\" lightbox =\"./media/how-to-create-component-pipelines-ui/component-page.png\":::\\n\\n## Create pipeline using registered component\\n\\n1. Create a new pipeline in the designer.\\n\\n    :::image type=\"content\" source=\"./media/how-to-create-component-pipelines-ui/new-pipeline.png\" alt-text=\"Screenshot showing creating new pipeline in designer homepage.\" lightbox =\"./media/how-to-create-component-pipelines-ui/new-pipeline.png\":::\\n\\n1. Set the default compute target of the pipeline. \\n\\n    Select the **Gear icon** ![Screenshot of the gear icon that is in the UI.](./media/tutorial-designer-automobile-price-train-score/gear-icon.png) at the top right of the canvas to open the **Settings** pane. Select the default compute target for your pipeline.\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-component-pipelines-ui-0.md'}),\n",
       " Document(page_content='\\n# Use GitHub Actions with Azure Machine Learning\\n[!INCLUDE [v2](../../includes/machine-learning-dev-v2.md)]\\nGet started with [GitHub Actions](https://docs.github.com/en/actions) to train a model on Azure Machine Learning. \\n\\nThis article will teach you how to create a GitHub Actions workflow that builds and deploys a machine learning model to [Azure Machine Learning](./overview-what-is-azure-machine-learning.md). You\\'ll train a [scikit-learn](https://scikit-learn.org/) linear regression model on the NYC Taxi dataset. \\n\\nGitHub Actions uses a workflow YAML (.yml) file in the `/.github/workflows/` path in your repository. This definition contains the various steps and parameters that make up the workflow.\\n\\n\\n## Prerequisites\\n\\n[!INCLUDE [sdk](../../includes/machine-learning-sdk-v2-prereqs.md)]\\n\\n* A GitHub account. If you don\\'t have one, sign up for [free](https://github.com/join).  \\n\\n## Step 1. Get the code\\n\\nFork the following repo at GitHub:\\n\\n```\\nhttps://github.com/azure/azureml-examples\\n```\\n\\n## Step 2. Authenticate with Azure\\n\\nYou\\'ll need to first define how to authenticate with Azure. You can use a [service principal](../active-directory/develop/app-objects-and-service-principals.md#service-principal-object) or [OpenID Connect](https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect). \\n\\n### Generate deployment credentials\\n\\n[!INCLUDE [include](~/articles/reusable-content/github-actions/generate-deployment-credentials.md)]\\n\\n### Create secrets\\n\\n[!INCLUDE [include](~/articles/reusable-content/github-actions/create-secrets-with-openid.md)]\\n\\n## Step 3. Update `setup.sh` to connect to your Azure Machine Learning workspace\\n\\nYou\\'ll need to update the CLI setup file variables to match your workspace. \\n\\n1. In your cloned repository, go to `azureml-examples/cli/`. \\n1. Edit `setup.sh` and update these variables in the file. \\n   \\n    |Variable  | Description  |\\n    |---------|---------|\\n    |GROUP     |      Name of resource group    |\\n    |LOCATION     |    Location of your workspace (example: `eastus2`)    |\\n    |WORKSPACE     |     Name of Azure ML workspace     | \\n\\n## Step 4. Update `pipeline.yml` with your compute cluster name\\n\\nYou\\'ll use a `pipeline.yml` file to deploy your Azure ML pipeline. This is a machine learning pipeline and not a DevOps pipeline. You only need to make this update if you\\'re using a name other than `cpu-cluster` for your computer cluster name. \\n\\n1. In your cloned repository, go to `azureml-examples/cli/jobs/pipelines/nyc-taxi/pipeline.yml`. \\n1. Each time you see `compute: azureml:cpu-cluster`, update the value of `cpu-cluster` with your compute cluster name. For example, if your cluster is named `my-cluster`, your new value would be `azureml:my-cluster`. There are five updates.\\n\\n## Step 5: Run your GitHub Actions workflow\\n\\nYour workflow authenticates with Azure, sets up the Azure Machine Learning CLI, and uses the CLI to train a model in Azure Machine Learning. \\n\\n# [Service principal](#tab/userlevel)\\n\\n\\nYour workflow file is made up of a trigger section and jobs:\\n- A trigger starts the workflow in the `on` section. The workflow runs by default on a cron schedule and when a pull request is made from matching branches and paths. Learn more about [events that trigger workflows](https://docs.github.com/actions/using-workflows/events-that-trigger-workflows). \\n- In the jobs section of the workflow, you checkout code and log into Azure with your service principal secret.\\n- The jobs section also includes a setup action that installs and sets up the [Machine Learning CLI (v2)](how-to-configure-cli.md). Once the CLI is installed, the run job action runs your Azure Machine Learning `pipeline.yml` file to train a model with NYC taxi data.\\n\\n\\n### Enable your workflow\\n\\n1. In your cloned repository, open `.github/workflows/cli-jobs-pipelines-nyc-taxi-pipeline.yml` and verify that your workflow looks like this. \\n\\n    ```yaml\\n    name: cli-jobs-pipelines-nyc-taxi-pipeline\\n    on:\\n      workflow_dispatch:\\n      schedule:\\n        - cron: \"0 0/4 * * *\"\\n      pull_request:\\n        branches:\\n          - main\\n          - sdk-preview\\n        paths:\\n          - cli/jobs/pipelines/nyc-taxi/**\\n          - .github/workflows/cli-jobs-pipelines-nyc-taxi-pipeline.yml\\n          - cli/run-pipeline-jobs.sh\\n          - cli/setup.sh\\n    jobs:\\n      build:\\n        runs-on: ubuntu-latest\\n        steps:\\n        - name: check out repo\\n          uses: actions/checkout@v2\\n        - name: azure login\\n          uses: azure/login@v1\\n          with:\\n            creds: ${{secrets.AZ_CREDS}}\\n        - name: setup\\n          run: bash setup.sh\\n          working-directory: cli\\n          continue-on-error: true\\n        - name: run job\\n          run: bash -x ../../../run-job.sh pipeline.yml\\n          working-directory: cli/jobs/pipelines/nyc-taxi\\n    ```\\n', metadata={'sourcefile': 'UI/2023-04-06_191207_UTC/simple-4000-100/how-to-github-actions-machine-learning-0.md'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cog_search.get_relevant_documents(\"pipelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                    chain_type=\"stuff\",\n",
    "                                    retriever=cog_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_call: {'module': 'langchain.chat_models.openai', 'name': '_generate', 'args': (AzureChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fa0217edfc0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='', request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='gpt-35-turbo', openai_api_type='azure', openai_api_base='', openai_api_version='2023-03-15-preview'), [SystemMessage(content='Use the following pieces of context to answer the users question. \\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n----------------\\n\\n# MLOps: Model management, deployment, and monitoring with Azure Machine Learning\\n\\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\\n\\n> [!div class=\"op_single_selector\" title1=\"Select the version of Azure Machine Learning developer platform you are using:\"]\\n> * [v1](./v1/concept-model-management-and-deployment.md)\\n> * [v2 (current version)](concept-model-management-and-deployment.md)\\n\\nIn this article, learn how to apply Machine Learning Operations (MLOps) practices in Azure Machine Learning for the purpose of managing the lifecycle of your models. Applying MLOps practices can improve the quality and consistency of your machine learning solutions. \\n\\n## What is MLOps?\\n\\nMLOps is based on [DevOps](https://azure.microsoft.com/overview/what-is-devops/) principles and practices that increase the efficiency of workflows. Examples include continuous integration, delivery, and deployment. MLOps applies these principles to the machine learning process, with the goal of:\\n\\n* Faster experimentation and development of models.\\n* Faster deployment of models into production.\\n* Quality assurance and end-to-end lineage tracking.\\n\\n## MLOps in Machine Learning\\n\\nMachine Learning provides the following MLOps capabilities:\\n\\n- **Create reproducible machine learning pipelines.** Use machine learning pipelines to define repeatable and reusable steps for your data preparation, training, and scoring processes.\\n- **Create reusable software environments.** Use these environments for training and deploying models.\\n- **Register, package, and deploy models from anywhere.** You can also track associated metadata required to use the model.\\n- **Capture the governance data for the end-to-end machine learning lifecycle.** The logged lineage information can include who is publishing models and why changes were made. It can also include when models were deployed or used in production.\\n- **Notify and alert on events in the machine learning lifecycle.** Event examples include experiment completion, model registration, model deployment, and data drift detection.\\n- **Monitor machine learning applications for operational and machine learning-related issues.** Compare model inputs between training and inference. Explore model-specific metrics. Provide monitoring and alerts on your machine learning infrastructure.\\n- **Automate the end-to-end machine learning lifecycle with Machine Learning and Azure Pipelines.** By using pipelines, you can frequently update models. You can also test new models. You can continually roll out new machine learning models alongside your other applications and services.\\n\\nFor more information on MLOps, see [Machine learning DevOps](/azure/cloud-adoption-framework/ready/azure-best-practices/ai-machine-learning-mlops).\\n\\n## Create reproducible machine learning pipelines\\n\\nUse machine learning pipelines from Machine Learning to stitch together all the steps in your model training process.\\n\\nA machine learning pipeline can contain steps from data preparation to feature extraction to hyperparameter tuning to model evaluation. For more information, see [Machine learning pipelines](concept-ml-pipelines.md).\\n\\nIf you use the [designer](concept-designer.md) to create your machine learning pipelines, you can at any time select the **...** icon in the upper-right corner of the designer page. Then select **Clone**. When you clone your pipeline, you iterate your pipeline design without losing your old versions.\\n\\n## Create reusable software environments\\n\\nBy using Machine Learning environments, you can track and reproduce your projects\\' software dependencies as they evolve. You can use environments to ensure that builds are reproducible without manual software configurations.\\n\\nEnvironments describe the pip and conda dependencies for your projects. You can use them for training and deployment of models. For more information, see [What are Machine Learning environments?](concept-environments.md).\\n\\n## Register, package, and deploy models from anywhere\\n\\nThe following sections discuss how to register, package, and deploy models.\\n\\n\\n* [Configure a development environment for Azure Machine Learning](how-to-configure-environment.md)\\n\\n### Submit a command\\n\\nA generic training job with Azure Machine Learning can be defined using the [command()](/python/api/azure-ai-ml/azure.ai.ml#azure-ai-ml-command). The command is then used, along with your training script(s) to train a model on the specified compute target.\\n\\nYou may start with a command for your local computer, and then switch to one for a cloud-based compute target as needed. When changing the compute target, you only change the compute parameter in the command that you use. A run also logs information about the training job, such as the inputs, outputs, and logs.\\n\\n* [Tutorial: Train your first ML model](tutorial-1st-experiment-sdk-train.md)\\n* [Examples: Jupyter Notebook and Python examples of training models](https://github.com/Azure/azureml-examples)\\n\\n### Automated Machine Learning\\n\\nDefine the iterations, hyperparameter settings, featurization, and other settings. During training, Azure Machine Learning tries different algorithms and parameters in parallel. Training stops once it hits the exit criteria you defined.\\n\\n> [!TIP]\\n> In addition to the Python SDK, you can also use Automated ML through [Azure Machine Learning studio](https://ml.azure.com).\\n\\n* [What is automated machine learning?](concept-automated-ml.md)\\n* [Tutorial: Create your first classification model with automated machine learning](tutorial-first-experiment-automated-ml.md)\\n* [How to: Configure automated ML experiments in Python](how-to-configure-auto-train.md)\\n* [How to: Create, explore, and deploy automated machine learning experiments with Azure Machine Learning studio](how-to-use-automated-ml-for-ml-models.md)\\n\\n### Machine learning pipeline\\n\\nMachine learning pipelines can use the previously mentioned training methods. Pipelines are more about creating a workflow, so they encompass more than just the training of models. \\n\\n* [What are ML pipelines in Azure Machine Learning?](concept-ml-pipelines.md)\\n* [Tutorial: Create production ML pipelines with Python SDK v2 in a Jupyter notebook](tutorial-pipeline-python-sdk.md)\\n\\n\\n### Understand what happens when you submit a training job\\n\\nThe Azure training lifecycle consists of:\\n\\n1. Zipping the files in your project folder and upload to the cloud.\\n    \\n    > [!TIP]\\n    > [!INCLUDE [amlinclude-info](../../includes/machine-learning-amlignore-gitignore.md)]\\n\\n1. Scaling up your compute cluster \\n1. Building or downloading the dockerfile to the compute node \\n    1. The system calculates a hash of: \\n        - The base image \\n        - Custom docker steps (see [Deploy a model using a custom Docker base image](./how-to-deploy-custom-container.md))\\n        - The conda definition YAML (see [Manage Azure Machine Learning environments with the CLI (v2)](how-to-manage-environments-v2.md)))\\n    1. The system uses this hash as the key in a lookup of the workspace Azure Container Registry (ACR)\\n    1. If it is not found, it looks for a match in the global ACR\\n    1. If it is not found, the system builds a new image (which will be cached and registered with the workspace ACR)\\n1. Downloading your zipped project file to temporary storage on the compute node\\n1. Unzipping the project file\\n1. The compute node executing `python <entry script> <arguments>`\\n1. Saving logs, model files, and other files written to `./outputs` to the storage account associated with the workspace\\n1. Scaling down compute, including removing temporary storage \\n\\n\\n## Azure Machine Learning designer\\n\\nThe designer lets you train models using a drag and drop interface in your web browser.\\n\\n+ [What is the designer?](concept-designer.md)\\n+ [Tutorial: Predict automobile price](tutorial-designer-automobile-price-train-score.md)\\n\\n## Azure CLI\\n\\nThe machine learning CLI is an extension for the Azure CLI. It provides cross-platform CLI commands for working with Azure Machine Learning. Typically, you use the CLI to automate tasks, such as training a machine learning model.\\n\\n\\nThe Azure cloud provides several types of pipeline, each with a different purpose. The following table lists the different pipelines and what they\\'re used for:\\n\\n| Scenario | Primary persona | Azure offering | OSS offering | Canonical pipe | Strengths |\\n| -------- | --------------- | -------------- | ------------ | -------------- | --------- |\\n| Model orchestration (Machine learning) | Data scientist | Azure Machine Learning Pipelines | Kubeflow Pipelines | Data -> Model | Distribution, caching, code-first, reuse | \\n| Data orchestration (Data prep) | Data engineer | [Azure Data Factory pipelines](../data-factory/concepts-pipelines-activities.md) | Apache Airflow | Data -> Data | Strongly typed movement, data-centric activities |\\n| Code & app orchestration (CI/CD) | App Developer / Ops | [Azure Pipelines](https://azure.microsoft.com/services/devops/pipelines/) | Jenkins | Code + Model -> App/Service | Most open and flexible activity support, approval queues, phases with gating |\\n\\n## Next steps\\n\\nAzure Machine Learning pipelines are a powerful facility that begins delivering value in the early development stages.\\n\\n+ [Define pipelines with the Azure ML CLI v2](./how-to-create-component-pipelines-cli.md)\\n+ [Define pipelines with the Azure ML SDK v2](./how-to-create-component-pipeline-python.md)\\n+ [Define pipelines with Designer](./how-to-create-component-pipelines-ui.md)\\n+ Try out [CLI v2 pipeline example](https://github.com/Azure/azureml-examples/tree/sdk-preview/cli/jobs/pipelines-with-components)\\n+ Try out [Python SDK v2 pipeline example](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/pipelines)\\n', additional_kwargs={}), HumanMessage(content='What is a pipeline?', additional_kwargs={})]), 'kwargs': {'stop': None}}\n",
      "json_result: generations=[ChatGeneration(text='A pipeline in machine learning is a collection of repeatable and reusable steps used for data preparation, model training, and evaluation. Machine learning pipelines run automatically to train, tune, and deploy models. These pipelines enable easier management of the workflow and helps in creating reproducible results.', generation_info=None, message=AIMessage(content='A pipeline in machine learning is a collection of repeatable and reusable steps used for data preparation, model training, and evaluation. Machine learning pipelines run automatically to train, tune, and deploy models. These pipelines enable easier management of the workflow and helps in creating reproducible results.', additional_kwargs={}))] llm_output={'token_usage': <OpenAIObject at 0x7fa00849aa20> JSON: {\n",
      "  \"completion_tokens\": 55,\n",
      "  \"prompt_tokens\": 2059,\n",
      "  \"total_tokens\": 2114\n",
      "}, 'model_name': 'gpt-3.5-turbo'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is a pipeline?',\n",
       " 'result': 'A pipeline in machine learning is a collection of repeatable and reusable steps used for data preparation, model training, and evaluation. Machine learning pipelines run automatically to train, tune, and deploy models. These pipelines enable easier management of the workflow and helps in creating reproducible results.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"What is a pipeline?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
