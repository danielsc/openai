[
    {
        "query": "How to update an existing azureml environment using AzureML SDK V2?",
        "result": "To update an existing AzureML environment using AzureML SDK V2, you can use the `Environment.update()` method. First, retrieve the environment object using `Environment.get()` method, then modify the environment object as needed, and finally call the `update()` method to update the environment. Here's an example:\n\n```python\nfrom azureml.core import Environment, Workspace\n\nws = Workspace.from_config()\n\n# Get the environment object\nenv = Environment.get(workspace=ws, name=\"myenv\", version=\"1\")\n\n# Modify the environment object\nenv.python.conda_dependencies.add_pip_package(\"scikit-learn==1.0.0\")\n\n# Update the environment\nenv.update(workspace=ws)\n```\n\nThis example updates the `scikit-learn` package to version 1.0.0 in the `myenv` environment.",
        "context": [
            {
                "page_content": "\n*Applies to: all scenarios*\n\nIf you're using a YAML for your conda specification, specify a version for pip\n\n```yaml\nname: project_environment\ndependencies:\n  - python=3.8\n  - pip=22.3.1\n  - pip:\n      - numpy=1.24.1\nchannels:\n  - anaconda\n  - conda-forge\n```\n\n**Resources**\n* See [conda package pinning](https://aka.ms/azureml/environment/how-to-pin-conda-packages)\n\n## *Miscellaneous environment issues*\n### R section is deprecated\n<!--issueDescription-->\n**Potential causes:**\n* You specified an R section in your environment definition\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nThe AzureML SDK for R was deprecated at the end of 2021 to make way for an improved R training and deployment experience using the Azure CLI v2\n\n*Applies to: Python SDK v1*\n\nRemove the R section from your environment definition\n\n```python\nenv.r = None\n```\n\n*Applies to: all scenarios*\n\nSee the [samples repository](https://aka.ms/azureml/environment/train-r-models-cli-v2) to get started training R models using the Azure CLI v2\n\n### No definition exists for environment\n<!--issueDescription-->\n**Potential causes:**\n* You specified an environment that doesn't exist or hasn't been registered\n* There was a misspelling or syntactical error in the way you specified your environment name or environment version\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nEnsure that you are specifying your environment name correctly, along with the correct version\n* `path-to-resource:version-number`\n\nThe 'latest' version of your environment is specified in a slightly different way\n* `path-to-resource@latest`\n\n## **Image build problems**\n\n## *ACR issues*\n### ACR unreachable\n<!--issueDescription-->\nThis issue can happen by failing to access a workspace's associated Azure Container Registry (ACR) resource.\n\n**Potential causes:**\n* Workspace's ACR is behind a virtual network (VNet) (private endpoint or service endpoint), and no compute cluster is used to build images.\n* Workspace's ACR is behind a virtual network (private endpoint or service endpoint), and the compute cluster used for building images have no access to the workspace's ACR.\n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n* Pipeline job failures.\n* Model deployment failures.\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nUpdate the workspace image build compute property using SDK:\n\n```python\nfrom azureml.core import Workspace\nws = Workspace.from_config()\nws.update(image_build_compute = 'mycomputecluster')\n```\n\n*Applies to: Azure CLI extensions v1 & v2*\n\nUpdate the workspace image build compute property using Azure CLI:\n\n```\naz ml workspace update --name myworkspace --resource-group myresourcegroup --image-build-compute mycomputecluster\n```\n\n> [!NOTE]\n> * Only Azure Machine Learning compute clusters are supported. Compute, Azure Kubernetes Service (AKS), or other instance types are not supported for image build compute.\n> * Make sure the compute cluster's VNet that's used for the image build compute has access to the workspace's ACR.\n> * Make sure the compute cluster is CPU based.\n\n**Resources**\n* [Enable Azure Container Registry (ACR)](https://aka.ms/azureml/environment/acr-private-endpoint)\n* [How To Use Environments](https://aka.ms/azureml/environment/how-to-use-environments)\n\n## *Docker pull issues*\n### Failed to pull Docker image\n<!--issueDescription-->\nThis issue can happen when a Docker image pull fails during an image build.\n\n**Potential causes:**\n* The path name to the container registry is incorrect\n* A container registry behind a virtual network is using a private endpoint in an [unsupported region](https://aka.ms/azureml/environment/private-link-availability)\n* The image you're trying to reference doesn't exist in the container registry you specified \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-1072.md"
                }
            },
            {
                "page_content": "\n\n# Troubleshoot \"cannot import name 'SerializationError'\"\n\nWhen using Azure Machine Learning, you may receive one of the following errors:\n\n* `cannot import name 'SerializationError'`\n* `cannot import name 'SerializationError' from 'azure.core.exceptions'`\n\nThis error may occur when using an Azure Machine Learning environment. For example, when submitting a training job or using AutoML.\n\n## Cause\n\nThis problem is caused by a bug in the Azure Machine Learning SDK version 1.42.0.\n\n## Resolution\n\nUpdate the affected environment to use SDK version 1.42.0.post1 or greater. For a local development environment or compute instance, use the following command:\n\n```bash\npip install azureml-sdk[automl,explain,notebooks]>=1.42.0\n```\n\nFor more information on updating an Azure Machine Learning environment (for training or deployment), see the following articles:\n\n* [Manage environments in studio](how-to-manage-environments-in-studio.md#rebuild-an-environment)\n* [Create & use software environments (SDK v1)](how-to-use-environments.md#update-an-existing-environment)\n* [Create & manage environments (CLI v2)](how-to-manage-environments-v2.md#update)\n\nTo verify the version of your installed SDK, use the following command:\n\n```bash\npip show azureml-core\n```\n\n## Next steps\n\nFor more information on updating an Azure Machine Learning environment (for training or deployment), see the following articles:\n\n* [Manage environments in studio](how-to-manage-environments-in-studio.md#rebuild-an-environment)\n* [Create & use software environments (SDK v1)](how-to-use-environments.md#update-an-existing-environment)\n* [Create & manage environments (CLI v2)](how-to-manage-environments-v2.md#update)",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-serialization-error-0.md"
                }
            },
            {
                "page_content": "\n# Troubleshoot `descriptors cannot not be created directly` error\n\nWhen using Azure Machine Learning, you may receive the following error:\n\n```\nTypeError: Descriptors cannot not be created directly. If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\u201d It is followed by the proposition to install the appropriate version of protobuf library.\n\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n```\n\nYou may notice this error specifically when using AutoML.\n\n## Cause\n\nThis problem is caused by breaking changes introduced in protobuf 4.0.0. For more information, see [https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates](https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates).\n\n## Resolution\n\nFor a local development environment or compute instance, install the Azure Machine Learning SDK version 1.42.0.post1 or greater.\n\n```bash\npip install azureml-sdk[automl,explain,notebooks]>=1.42.0\n```\n\nFor more information on updating an Azure Machine Learning environment (for training or deployment), see the following articles:\n\n* [Manage environments in studio](how-to-manage-environments-in-studio.md#rebuild-an-environment)\n* [Create & use software environments (SDK v1)](how-to-use-environments.md#update-an-existing-environment)\n* [Create & manage environments (CLI v2)](how-to-manage-environments-v2.md#update)\n\nTo verify the version of your installed SDK, use the following command:\n\n```bash\npip show azureml-core\n```\n\nThis command should return information similar to `Version: 1.42.0.post1`.\n\n> [!TIP]\n> If you can't upgrade your Azure Machine Learning SDK installation, you can pin the protobuf version in your environment to `3.20.1`. The following example is a `conda.yml` file that demonstrates how to pin the version:\n>\n> ```yml\n> name: model-env\n> channels:\n>   - conda-forge\n> dependencies:\n>   - python=3.8\n>   - numpy=1.21.2\n>   - pip=21.2.4\n>   - scikit-learn=0.24.2\n>   - scipy=1.7.1\n>   - pandas>=1.1,<1.2\n>   - pip:\n>     - inference-schema[numpy-support]==1.3.0\n>     - xlrd==2.0.1\n>     - mlflow== 1.26.0\n>     - azureml-mlflow==1.41.0\n>     - protobuf==3.20.1\n> ```\n\n## Next steps\n\nFor more information on the breaking changes in protobuf 4.0.0, see [https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates](https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates). \n\nFor more information on updating an Azure Machine Learning environment (for training or deployment), see the following articles:\n\n* [Manage environments in studio](how-to-manage-environments-in-studio.md#rebuild-an-environment)\n* [Create & use software environments (SDK v1)](how-to-use-environments.md#update-an-existing-environment)\n* [Create & manage environments (CLI v2)](how-to-manage-environments-v2.md#update)",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-protobuf-descriptor-error-0.md"
                }
            }
        ]
    },
    {
        "query": "Generate a Python Code Sample to create an environment from conda specification using AzureML SDK V2?",
        "result": "Here's a Python code sample to create an environment from a conda specification using AzureML SDK V2:\n\n```python\nfrom azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nmyenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies.create(conda_packages=['scikit-learn','pandas'],\n                                     pip_packages=['azureml-defaults'])\nmyenv.python.conda_dependencies = conda_dep\n```\n\nThis code creates a new environment named \"myenv\" and specifies the conda and pip packages to be installed in the environment.",
        "context": [
            {
                "page_content": "To create a new environment, you must use one of the following approaches (see [DockerSection](https://aka.ms/azureml/environment/environment-docker-section)):\n* Base image\n    * Provide base image name, repository from which to pull it, and credentials if needed\n    * Provide a conda specification\n* Base Dockerfile \n    * Provide a Dockerfile\n    * Provide a conda specification\n* Docker build context\n    * Provide the location of the build context (URL)\n    * The build context must contain at least a Dockerfile, but may contain other files as well\n\n*Applies to: Azure CLI & Python SDK v2*\n\nTo create a new environment, you must use one of the following approaches:\n* Docker image\n    * Provide the image URI of the image hosted in a registry such as Docker Hub or Azure Container Registry\n    * [Sample here](https://aka.ms/azureml/environment/create-env-docker-image-v2)\n* Docker build context\n    * Specify the directory that will serve as the build context\n    * The directory should contain a Dockerfile and any other files needed to build the image\n    * [Sample here](https://aka.ms/azureml/environment/create-env-build-context-v2)\n* Conda specification \n    * You must specify a base Docker image for the environment; the conda environment will be built on top of the Docker image provided\n    * Provide the relative path to the conda file\n    * [Sample here](https://aka.ms/azureml/environment/create-env-conda-spec-v2)\n\n### Missing Docker definition\n*Applies to: Python SDK v1*\n<!--issueDescription-->\nThis issue can happen when your environment definition is missing a `DockerSection.` This section configures settings related to the final Docker image built from your environment specification.\n \n**Potential causes:**\n* The `DockerSection` of your environment definition isn't defined (null)\n \n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n \n**Troubleshooting steps**\n\n Add a `DockerSection` to your environment definition, specifying either a base image, base dockerfile, or docker build context.\n\n```python\nfrom azureml.core import Environment\nmyenv = Environment(name=\"myenv\")\n# Specify docker steps as a string.\ndockerfile = r'''\nFROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\nRUN echo \"Hello from custom container!\"\n'''\n\nmyenv.docker.base_dockerfile = dockerfile\n```\n \n**Resources**\n* [DockerSection](https://aka.ms/azureml/environment/environment-docker-section)\n\n### Too many Docker options\n<!--issueDescription-->\n \n**Potential causes:**\n\n*Applies to: Python SDK v1*\n\nYou have more than one of these Docker options specified in your environment definition\n* `base_image`\n* `base_dockerfile`\n* `build_context`\n* See [DockerSection](https://aka.ms/azureml/environment/docker-section-class)\n\n*Applies to: Azure CLI & Python SDK v2*\n\nYou have more than one of these Docker options specified in your environment definition\n* `image`\n* `build`\n* See [azure.ai.ml.entities.Environment](https://aka.ms/azureml/environment/environment-class-v2)\n \n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n \n**Troubleshooting steps**\n\nChoose which Docker option you'd like to use to build your environment. Then set all other specified options to None.\n\n*Applies to: Python SDK v1*\n\n```python\nfrom azureml.core import Environment\nmyenv = Environment(name=\"myEnv\")\ndockerfile = r'''\nFROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\nRUN echo \"Hello from custom container!\"\n'''\nmyenv.docker.base_dockerfile = dockerfile\nmyenv.docker.base_image = \"pytorch/pytorch:latest\"\n\n# Having both base dockerfile and base image set will cause failure. Delete the one you won't use.\nmyenv.docker.base_image = None\n```\n\n### Missing Docker option\n<!--issueDescription-->\n \n**Potential causes:**\n\n*Applies to: Python SDK v1*\n\nYou didn't specify one of the following options in your environment definition\n* `base_image`\n* `base_dockerfile`\n* `build_context`\n* See [DockerSection](https://aka.ms/azureml/environment/docker-section-class)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-128.md"
                }
            },
            {
                "page_content": "* See [how to create a conda file manually](https://aka.ms/azureml/environment/how-to-create-conda-file)\n* See [CondaDependencies class](https://aka.ms/azureml/environment/conda-dependencies-class)\n* See [how to set a conda specification on the environment definition](https://aka.ms/azureml/environment/set-conda-spec-on-environment-definition)\n\n### Invalid conda dependencies\n<!--issueDescription-->\n**Potential causes:**\n* The conda dependencies specified in your environment definition aren't formatted correctly\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nEnsure that `conda_dependencies` is a JSONified version of the conda dependencies YAML structure\n\n```json\n\"condaDependencies\": {\n    \"channels\": [\n\t\"anaconda\",\n        \"conda-forge\"\n    ],\n    \"dependencies\": [\n        \"python=3.8\",\n        {\n            \"pip\": [\n                \"azureml-defaults\"\n            ]\n        }\n    ],\n    \"name\": \"project_environment\"\n}\n```\n\nConda dependencies can also be specified using the `add_conda_package` method\n\n```python\nfrom azureml.core.environment import CondaDependencies\n\nenv = Environment(name=\"env\")\nconda_dep = CondaDependencies()\nconda_dep.add_conda_package(\"python==3.8\")\nenv.python.conda_dependencies = conda_dep\n```\n\n*Applies to: Azure CLI & Python SDK v2*\n\nYou must specify a base Docker image for the environment, and the conda environment will be built on top of that image\n* Provide the relative path to the conda file\n* See how to [create an environment from a conda specification](https://aka.ms/azureml/environment/create-env-conda-spec-v2)\n\n**Resources**\n* See [more extensive examples](https://github.com/Azure/MachineLearningNotebooks/blob/9b1e130d18d3c61d41dc225488a4575904897c85/how-to-use-azureml/training/using-environments/using-environments.ipynb)\n* See [how to create a conda file manually](https://aka.ms/azureml/environment/how-to-create-conda-file)\n* See [CondaDependencies class](https://aka.ms/azureml/environment/conda-dependencies-class)\n* See [how to set a conda specification on the environment definition](https://aka.ms/azureml/environment/set-conda-spec-on-environment-definition)\n\n### Missing conda channels\n<!--issueDescription-->\n**Potential causes:**\n* You haven't specified conda channels in your environment definition\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nFor reproducibility of your environment, specify channels from which to pull dependencies. If no conda channel is specified, conda will use defaults that might change.\n\n*Applies to: Python SDK v1*\n\nAdd a conda channel using the Python SDK\n\n```python\nfrom azureml.core.environment import CondaDependencies\n\nenv = Environment(name=\"env\")\nconda_dep = CondaDependencies()\nconda_dep.add_channel(\"conda-forge\")\nenv.python.conda_dependencies = conda_dep\n```\n\n*Applies to: all scenarios*\n\nIf you're using a YAML for your conda specification, include the conda channel(s) you'd like to use\n\n```yaml\nname: project_environment\ndependencies:\n  - python=3.8\n  - pip:\n      - azureml-defaults\nchannels:\n  - anaconda\n  - conda-forge\n```\n\n**Resources**\n* See [how to set a conda specification on the environment definition v1](https://aka.ms/azureml/environment/set-conda-spec-on-environment-definition)\n* See [CondaDependencies class](https://aka.ms/azureml/environment/conda-dependencies-class)\n* See how to [create an environment from a conda specification v2](https://aka.ms/azureml/environment/create-env-conda-spec-v2)\n* See [how to create a conda file manually](https://aka.ms/azureml/environment/how-to-create-conda-file)\n\n### Base conda environment not recommended\n<!--issueDescription-->\n**Potential causes:**\n* You specified a base conda environment in your environment definition\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-827.md"
                }
            },
            {
                "page_content": "\n# What are Azure Machine Learning environments?\n\nAzure Machine Learning environments are an encapsulation of the environment where your machine learning training happens. They specify the Python packages, environment variables, and software settings around your training and scoring scripts. They also specify runtimes (Python, Spark, or Docker). The environments are managed and versioned entities within your Machine Learning workspace that enable reproducible, auditable, and portable machine learning workflows across a variety of compute targets.\n\nYou can use an `Environment` object on your local compute to:\n* Develop your training script.\n* Reuse the same environment on Azure Machine Learning Compute for model training at scale.\n* Deploy your model with that same environment.\n* Revisit the environment in which an existing model was trained.\n\nThe following diagram illustrates how you can use a single `Environment` object in both your job configuration (for training) and your inference and deployment configuration (for web service deployments).\n\n![Diagram of an environment in machine learning workflow](./media/concept-environments/ml-environment.png)\n\nThe environment, compute target and training script together form the job configuration: the full specification of a training job.\n\n## Types of environments\n\nEnvironments can broadly be divided into three categories: *curated*, *user-managed*, and *system-managed*.\n\nCurated environments are provided by Azure Machine Learning and are available in your workspace by default. Intended to be used as is, they contain collections of Python packages and settings to help you get started with various machine learning frameworks. These pre-created environments also allow for faster deployment time. For a full list, see the [curated environments article](resource-curated-environments.md).\n\nIn user-managed environments, you're responsible for setting up your environment and installing every package that your training script needs on the compute target. Also be sure to include any dependencies needed for model deployment.\n\nYou use system-managed environments when you want [conda](https://conda.io/docs/) to manage the Python environment for you. A new conda environment is materialized from your conda specification on top of a base docker image.\n\n## Create and manage environments\n\nYou can create environments from clients like the AzureML Python SDK, Azure Machine Learning CLI, Environments page in Azure Machine Learning studio, and [VS Code extension](how-to-manage-resources-vscode.md#create-environment). Every client allows you to customize the base image, Dockerfile, and Python layer if needed.\n\nFor specific code samples, see the \"Create an environment\" section of [How to use environments](how-to-manage-environments-v2.md#create-an-environment). \n\nEnvironments are also easily managed through your workspace, which allows you to:\n\n* Register environments.\n* Fetch environments from your workspace to use for training or deployment.\n* Create a new instance of an environment by editing an existing one.\n* View changes to your environments over time, which ensures reproducibility.\n* Build Docker images automatically from your environments.\n\n\"Anonymous\" environments are automatically registered in your workspace when you submit an experiment. They will not be listed but may be retrieved by version.\n\nFor code samples, see the \"Manage environments\" section of [How to use environments](how-to-manage-environments-v2.md#manage-environments).\n\n## Environment building, caching, and reuse\n\nAzure Machine Learning builds environment definitions into Docker images and conda environments. It also caches the environments so they can be reused in subsequent training jobs and service endpoint deployments. Running a training script remotely requires the creation of a Docker image, but a local job can use a conda environment directly. \n\n### Submitting a job using an environment\n\nWhen you first submit a remote job using an environment, the Azure Machine Learning service invokes an [ACR Build Task](../container-registry/container-registry-tasks-overview.md) on the Azure Container Registry (ACR) associated with the Workspace. The built Docker image is then cached on the Workspace ACR. Curated environments are backed by Docker images that are cached in Global ACR. At the start of the job execution, the image is retrieved by the compute target from the relevant ACR.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-environments-0.md"
                }
            }
        ]
    },
    {
        "query": "How to use Tensorboard to see job output using AzureML SDK V2?",
        "result": "To use TensorBoard to see job output using AzureML SDK V2, you need to define the interactive services you want to use for your job, including TensorBoard. You can do this by importing the `JobService` class from the `azure.ai.ml.entities` package and specifying the `job_service_type` as `tensor_board` in the `services` section of your job configuration. You also need to specify the relative path of TensorBoard logs (same as in your training script) in the `properties` section. Once you submit the job, you can access TensorBoard from the job details page or by connecting to the job endpoints.",
        "context": [
            {
                "page_content": "  > If you use `sleep infinity`, you will need to manually [cancel the job](./how-to-interactive-jobs.md#end-job) to let go of the compute resource (and stop billing). \n\n5. Select the training applications you want to use to interact with the job.\n\n  :::image type=\"content\" source=\"./media/interactive-jobs/select-training-apps.png\" alt-text=\"Screenshot of selecting a training application for the user to use for a job.\":::\n\n6. Review and create the job.\n\nIf you don't see the above options, make sure you have enabled the \"Debug & monitor your training jobs\" flight via the [preview panel](./how-to-enable-preview-features.md#how-do-i-enable-preview-features).\n\n# [Python SDK](#tab/python)\n1. Define the interactive services you want to use for your job. Make sure to replace `your compute name` with your own value. If you want to use your own custom environment, follow the examples in [this tutorial](how-to-manage-environments-v2.md) to create a custom environment. \n\n   Note that you have to import the `JobService` class from the `azure.ai.ml.entities` package to configure interactive services via the SDKv2. \n\n    ```python\n    command_job = command(...\n        code=\"./src\",  # local path where the code is stored\n        command=\"python main.py\", # you can add a command like \"sleep 1h\" to reserve the compute resource is reserved after the script finishes running\n        environment=\"AzureML-tensorflow-2.7-ubuntu20.04-py38-cuda11-gpu@latest\",\n        compute=\"<name-of-compute>\",\n        services={\n          \"My_jupyterlab\": JobService(\n            job_service_type=\"jupyter_lab\",\n            nodes=\"all\" # For distributed jobs, use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node. Values are \"all\", or compute node index (for ex. \"0\", \"1\" etc.)\n          ),\n          \"My_vscode\": JobService(\n            job_service_type=\"vs_code\",\n            nodes=\"all\"\n          ),\n          \"My_tensorboard\": JobService(\n            job_service_type=\"tensor_board\",\n            nodes=\"all\",\n            properties={\n                \"logDir\": \"output/tblogs\"  # relative path of Tensorboard logs (same as in your training script)\n            }          \n          ),\n          \"My_ssh\": JobService(\n            job_service_type=\"ssh\",\n            sshPublicKeys=\"<add-public-key>\",\n            nodes=\"all\"\n            properties={\n                \"sshPublicKeys\":\"<add-public-key>\"\n            }    \n          ),\n        }\n    )\n\n    # submit the command\n    returned_job = ml_client.jobs.create_or_update(command_job)\n    ```\n\n    The `services` section specifies the training applications you want to interact with.  \n\n    You can put `sleep <specific time>` at the end of your command to specify the amount of time you want to reserve the compute resource. The format follows: \n    * sleep 1s\n    * sleep 1m\n    * sleep 1h\n    * sleep 1d\n\n    You can also use the `sleep infinity` command that would keep the job alive indefinitely. \n    \n    > [!NOTE]\n    > If you use `sleep infinity`, you will need to manually [cancel the job](./how-to-interactive-jobs.md#end-job) to let go of the compute resource (and stop billing). \n\n2. Submit your training job. For more details on how to train with the Python SDKv2, check out this [article](./how-to-train-model.md).\n\n# [Azure CLI](#tab/azurecli)\n\n1. Create a job yaml `job.yaml` with below sample content. Make sure to replace `your compute name` with your own value. If you want to use custom environment, follow the examples in [this tutorial](how-to-manage-environments-v2.md) to create a custom environment. \n    ```dotnetcli\n    code: src \n    command: \n      python train.py \n      # you can add a command like \"sleep 1h\" to reserve the compute resource is reserved after the script finishes running.\n    environment: azureml:AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11-gpu:41\n    compute: azureml:<your compute name>\n    services:\n        my_vs_code:\n          job_service_type: vs_code\n          nodes: all # For distributed jobs, use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node. Values are \"all\", or compute node index (for ex. \"0\", \"1\" etc.)\n        my_tensor_board:\n          job_service_type: tensor_board\n          properties:\n            logDir: \"output/tblogs\" # relative path of Tensorboard logs (same as in your training script)\n          nodes: all\n        my_jupyter_lab:\n          job_service_type: jupyter_lab\n          nodes: all\n        my_ssh:\n         job_service_type: ssh\n         properties:\n           sshPublicKeys: <paste the entire pub key content>\n         nodes: all\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-interactive-jobs-50.md"
                }
            },
            {
                "page_content": "- [AzureML registered environment asset](https://github.com/Azure/azureml-examples/tree/sdk-preview/cli/jobs/pipelines-with-components/basics/5b_env_registered). It's referenced in component following `azureml:<environment-name>:<environment-version>` syntax.\n- [public docker image](https://github.com/Azure/azureml-examples/tree/sdk-preview/cli/jobs/pipelines-with-components/basics/5a_env_public_docker_image)\n- [conda file](https://github.com/Azure/azureml-examples/tree/sdk-preview/cli/jobs/pipelines-with-components/basics/5c_env_conda_file) Conda file needs to be used together with a base image.\n\n## Register component for reuse and sharing\n\nWhile some components will be specific to a particular pipeline, the real benefit of components comes from reuse and sharing. Register a component in your Machine Learning workspace to make it available for reuse. Registered components support automatic versioning so you can update the component but assure that pipelines that require an older version will continue to work.  \n\nIn the azureml-examples repository, navigate to the `cli/jobs/pipelines-with-components/basics/1b_e2e_registered_components` directory. \n\nTo register a component, use the `az ml component create` command:\n\n```azurecli\naz ml component create --file train.yml\naz ml component create --file score.yml\naz ml component create --file eval.yml\n```\n\nAfter these commands run to completion, you can see the components in Studio, under Asset -> Components:\n\n:::image type=\"content\" source=\"./media/how-to-create-component-pipelines-cli/registered-components.png\" alt-text=\"Screenshot of Studio showing the components that were just registered.\" lightbox =\"./media/how-to-create-component-pipelines-cli/registered-components.png\":::\n\nSelect a component. You'll see detailed information for each version of the component.\n\nUnder **Details** tab, you'll see basic information of the component like name, created by, version etc. You'll see editable fields for Tags and Description. The tags can be used for adding rapidly searched keywords. The description field supports Markdown formatting and should be used to describe your component's functionality and basic use.\n\nUnder **Jobs** tab, you'll see the history of all jobs that use this component.\n\n:::image type=\"content\" source=\"./media/how-to-create-component-pipelines-cli/registered-components.png\" alt-text=\"Screenshot of the component tab showing 3 components.\" lightbox =\"./media/how-to-create-component-pipelines-cli/registered-components.png\":::\n\n### Use registered components in a pipeline job YAML file\n\nLet's use `1b_e2e_registered_components` to demo how to use registered component in pipeline YAML. Navigate to `1b_e2e_registered_components` directory, open the `pipeline.yml` file. The keys and values in the `inputs` and `outputs` fields are similar to those already discussed. The only significant difference is the value of the `component` field in the `jobs.<JOB_NAME>.component` entries. The `component` value is of the form `azureml:<COMPONENT_NAME>:<COMPONENT_VERSION>`. The `train-job` definition, for instance, specifies the latest version of the registered component `my_train` should be used:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\ndisplay_name: 1b_e2e_registered_components\ndescription: E2E dummy train-score-eval pipeline with registered components\n\ninputs:\n  pipeline_job_training_max_epocs: 20\n  pipeline_job_training_learning_rate: 1.8\n  pipeline_job_learning_rate_schedule: 'time-based'\n\noutputs: \n  pipeline_job_trained_model:\n    mode: upload\n  pipeline_job_scored_data:\n    mode: upload\n  pipeline_job_evaluation_report:\n    mode: upload\n\nsettings:\n default_compute: azureml:cpu-cluster\n\njobs:\n  train_job:\n    type: command\n    component: azureml:my_train@latest\n    inputs:\n      training_data: \n        type: uri_folder \n        path: ./data      \n      max_epocs: ${{parent.inputs.pipeline_job_training_max_epocs}}\n      learning_rate: ${{parent.inputs.pipeline_job_training_learning_rate}}\n      learning_rate_schedule: ${{parent.inputs.pipeline_job_learning_rate_schedule}}\n    outputs:\n      model_output: ${{parent.outputs.pipeline_job_trained_model}}\n    services: \n      my_vscode: \n        job_service_type: vs_code\n      my_jupyter_lab: \n        job_service_type: jupyter_lab\n      my_tensorboard:\n        job_service_type: tensor_board\n        properties:\n          logDir: \"outputs/tblogs\"\n    #  my_ssh:\n    #    job_service_type: tensor_board\n    #    properties:\n    #      sshPublicKeys: <paste the entire pub key content>\n    #    nodes: all # Use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node.\n  \n  score_job:\n    type: command\n    component: azureml:my_score@latest\n    inputs:\n      model_input: ${{parent.jobs.train_job.outputs.model_output}}\n      test_data: \n        type: uri_folder \n        path: ./data\n    outputs:\n      score_output: ${{parent.outputs.pipeline_job_scored_data}}\n\n  evaluate_job:\n    type: command\n    component: azureml:my_eval@latest\n    inputs:\n      scoring_result: ${{parent.jobs.score_job.outputs.score_output}}\n    outputs:\n      eval_output: ${{parent.outputs.pipeline_job_evaluation_report}}\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-component-pipelines-cli-216.md"
                }
            },
            {
                "page_content": "When you click on the endpoints to interact when your job, you're taken to the user container under your working directory, where you can access your code, inputs, outputs, and logs. If you run into any issues while connecting to the applications, the interactive capability and applications logs can be found from **system_logs->interactive_capability** under **Outputs + logs** tab.\n\n:::image type=\"content\" source=\"./media/interactive-jobs/interactive-logs.png\" alt-text=\"Screenshot of interactive jobs interactive logs panel location.\":::\n\n- You can open a terminal from Jupyter Lab and start interacting within the job container. You can also directly iterate on your training script with Jupyter Lab. \n\n  :::image type=\"content\" source=\"./media/interactive-jobs/jupyter-lab.png\" alt-text=\"Screenshot of interactive jobs Jupyter lab content panel.\":::\n\n- You can also interact with the job container within VS Code. To attach a debugger to a job during job submission and pause execution, [navigate here](./how-to-interactive-jobs.md#attach-a-debugger-to-a-job).\n\n  :::image type=\"content\" source=\"./media/interactive-jobs/vs-code-open.png\" alt-text=\"Screenshot of interactive jobs VS Code panel when first opened. This shows the sample python file that was created to print two lines.\":::\n\n- If you have logged tensorflow events for your job, you can use TensorBoard to monitor the metrics when your job is running.\n\n  :::image type=\"content\" source=\"./media/interactive-jobs/tensorboard-open.png\" alt-text=\"Screenshot of interactive jobs tensorboard panel when first opened. This information will vary depending upon customer data\":::\n  \nIf you don't see the above options, make sure you have enabled the \"Debug & monitor your training jobs\" flight via the [preview panel](./how-to-enable-preview-features.md#how-do-i-enable-preview-features).\n\n### End job\nOnce you're done with the interactive training, you can also go to the job details page to cancel the job which will release the compute resource. Alternatively, use `az ml job cancel -n <your job name>` in the CLI or `ml_client.job.cancel(\"<job name>\")` in the SDK. \n\n:::image type=\"content\" source=\"./media/interactive-jobs/cancel-job.png\" alt-text=\"Screenshot of interactive jobs cancel job option and its location for user selection\":::\n\n## Attach a debugger to a job\nTo submit a job with a debugger attached and the execution paused, you can use debugpy and VS Code (`debugpy` must be installed in your job environment). \n\n1. During job submission (either through the UI, the CLIv2 or the SDKv2) use the debugpy command to run your python script. For example, the below screenshot shows a sample command that uses debugpy to attach the debugger for a tensorflow script (`tfevents.py` can be replaced with the name of your training script).\n   \n:::image type=\"content\" source=\"./media/interactive-jobs/use-debugpy.png\" alt-text=\"Screenshot of interactive jobs configuration of debugpy\":::\n\n2. Once the job has been submitted, [connect to the VS Code](./how-to-interactive-jobs.md#connect-to-endpoints), and click on the in-built debugger.\n   \n   :::image type=\"content\" source=\"./media/interactive-jobs/open-debugger.png\" alt-text=\"Screenshot of interactive jobs location of open debugger on the left side panel\":::\n\n3. Use the \"Remote Attach\" debug configuration to attach to the submitted job and pass in the path and port you configured in your job submission command. You can also find this information on the job details page.\n   \n   :::image type=\"content\" source=\"./media/interactive-jobs/debug-path-and-port.png\" alt-text=\"Screenshot of interactive jobs completed jobs\":::\n\n   :::image type=\"content\" source=\"./media/interactive-jobs/remote-attach.png\" alt-text=\"Screenshot of interactive jobs add a remote attach button\":::\n\n4. Set breakpoints and walk through your job execution as you would in your local debugging workflow. \n   \n    :::image type=\"content\" source=\"./media/interactive-jobs/set-breakpoints.png\" alt-text=\"Screenshot of location of an example breakpoint that is set in the Visual Studio Code editor\":::\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-interactive-jobs-194.md"
                }
            }
        ]
    },
    {
        "query": "How do I deploy a model in AzureML using CLI V2 and Managed Inferencing ?",
        "result": "To deploy a model in AzureML using CLI V2 and Managed Inferencing, you can follow these steps:\n\n1. Create or configure a secure Kubernetes inferencing environment.\n2. Deploy AzureML extension.\n3. Attach the Kubernetes cluster to the workspace.\n4. Use CLI v2 to deploy the model to a Kubernetes online endpoint. You can find examples on the AzureML GitHub repository.",
        "context": [
            {
                "page_content": "|CLI/SDK v2 | No | Yes|\n|Training | No | Yes|\n|Real-time inference | Yes | Yes |\n|Batch inference | No | Yes |\n|Real-time inference new features | No new features development | Active roadmap |\n\nWith these key differences and overall AzureML evolution to use SDK/CLI v2, AzureML recommends you to use Kubernetes compute target to deploy models if you decide to use AKS for model deployment.\n\n## Next steps\n\n- [Step 1: Deploy AzureML extension](how-to-deploy-kubernetes-extension.md)\n- [Step 2: Attach Kubernetes cluster to workspace](how-to-attach-kubernetes-to-workspace.md)\n- [Create and manage instance types](how-to-manage-kubernetes-instance-types.md)\n\n### Other resources\n\n- [Kubernetes version and region availability](./reference-kubernetes.md#supported-kubernetes-version-and-region)\n- [Work with custom data storage](./reference-kubernetes.md#azureml-jobs-connect-with-custom-data-storage)\n\n\n### Examples\n\nAll AzureML examples can be found in [https://github.com/Azure/azureml-examples.git](https://github.com/Azure/azureml-examples).\n\nFor any AzureML example, you only need to update the compute target name to your Kubernetes compute target, then you're all done. \n* Explore training job samples with CLI v2 - [https://github.com/Azure/azureml-examples/tree/main/cli/jobs](https://github.com/Azure/azureml-examples/tree/main/cli/jobs)\n* Explore model deployment with online endpoint samples with CLI v2 - [https://github.com/Azure/azureml-examples/tree/main/cli/endpoints/online/kubernetes](https://github.com/Azure/azureml-examples/tree/main/cli/endpoints/online/kubernetes)\n* Explore batch endpoint samples with CLI v2 - [https://github.com/Azure/azureml-examples/tree/main/cli/endpoints/batch](https://github.com/Azure/azureml-examples/tree/main/cli/endpoints/batch)\n* Explore training job samples with SDK v2 -[https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs)\n* Explore model deployment with online endpoint samples with SDK v2 -[https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/online/kubernetes](https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/online/kubernetes)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-attach-kubernetes-anywhere-69.md"
                }
            },
            {
                "page_content": "\n# Upgrade steps for Azure Container Instances web services to managed online endpoints\n\n[Managed online endpoints](concept-endpoints.md#what-are-online-endpoints) help to deploy your ML models in a turnkey manner. Managed online endpoints work with powerful CPU and GPU machines in Azure in a scalable, fully managed way. Managed online endpoints take care of serving, scaling, securing, and monitoring your models, freeing you from the overhead of setting up and managing the underlying infrastructure. Details can be found on [Deploy and score a machine learning model by using an online endpoint](how-to-deploy-online-endpoints.md).\n\nYou can deploy directly to the new compute target with your previous models and environments, or use the [scripts](https://aka.ms/moeonboard) provided by us to export the current services and then deploy to the new compute without affecting your existing services. If you regularly create and delete Azure Container Instances (ACI) web services, we strongly recommend the deploying directly and not using the scripts. \n\n> [!IMPORTANT]\n> **The scoring URL will be changed after upgrade**. For example, the scoring url for ACI web service is like `http://aaaaaa-bbbbb-1111.westus.azurecontainer.io/score`. The scoring URI for a managed online endpoint is like `https://endpoint-name.westus.inference.ml.azure.com/score`.\n\n## Supported scenarios and differences\n\n### Auth mode\nNo auth isn't supported for managed online endpoint. If you use the upgrade scripts, it will convert it to key auth.\nFor key auth, the original keys will be used. Token-based auth is also supported.\n\n### TLS\nFor ACI service secured with HTTPS, you don't need to provide your own certificates anymore, all the managed online endpoints are protected by TLS.\n\nCustom DNS name **isn't** supported.\n\n### Resource requirements\n[ContainerResourceRequirements](/python/api/azureml-core/azureml.core.webservice.aci.containerresourcerequirements) isn't supported, you can choose the proper [SKU](reference-managed-online-endpoints-vm-sku-list.md) for your inferencing.\nThe upgrade tool will map the CPU/Memory requirement to corresponding SKU. If you choose to redeploy manually through CLI/SDK V2, we also suggest the corresponding SKU for your new deployment.\n\n| CPU request | Memory request in GB | Suggested SKU |\n| :----| :---- | :---- |\n| (0, 1] | (0, 1.2] | DS1 V2 |\n| (1, 2] | (1.2, 1.7] | F2s V2 |\n| (1, 2] | (1.7, 4.7] | DS2 V2 |\n| (1, 2] | (4.7, 13.7] | E2s V3 |\n| (2, 4] | (0, 5.7] | F4s V2 |\n| (2, 4] | (5.7, 11.7] | DS3 V2 |\n| (2, 4] | (11.7, 16] | E4s V3 |\n\n\"(\" means greater than and \"]\" means less than or equal to. For example, \u201c(0, 1]\u201d means \u201cgreater than 0 and less than or equal to 1\u201d.\n\n> [!IMPORTANT]\n> When upgrading from ACI, there will be some changes in how you'll be charged. See [our blog](https://aka.ms/acimoemigration) for a rough cost comparison to help you choose the right VM SKUs for your workload.\n\n### Network isolation\nFor private workspace and VNet scenarios, see [Use network isolation with managed online endpoints](how-to-secure-online-endpoint.md?tabs=model).\n\n> [!IMPORTANT]\n> As there are many settings for your workspace and VNet, we strongly suggest that redeploy through the Azure CLI extension v2 for machine learning instead of the script tool.\n\n## Not supported\n+ [EncryptionProperties](/python/api/azureml-core/azureml.core.webservice.aci.encryptionproperties) for ACI container isn't supported.\n+ ACI web services deployed through deploy_from_model and deploy_from_image isn't supported by the upgrade tool. Redeploy manually through CLI/SDK V2.\n\n## Upgrade steps\n\n### With our [CLI](how-to-deploy-online-endpoints.md) or [SDK preview](how-to-deploy-managed-online-endpoint-sdk-v2.md)\nRedeploy manually with your model files and environment definition.\nYou can find our examples on [azureml-examples](https://github.com/Azure/azureml-examples). Specifically, this is the [SDK example for managed online endpoint](https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/online/managed).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-managed-online-endpoints-0.md"
                }
            },
            {
                "page_content": "\n# Secure an Azure Machine Learning inferencing environment with virtual networks\n\n> [!div class=\"op_single_selector\" title1=\"Select the Azure Machine Learning SDK or CLI version you are using:\"]\n> * [SDK/CLI v1](v1/how-to-secure-inferencing-vnet.md)\n> * [SDK/CLI v2 (current version)](how-to-secure-inferencing-vnet.md)\n\nIn this article, you learn how to secure inferencing environments (online endpoints) with a virtual network in Azure Machine Learning. There are two inference options that can be secured using a VNet:\n\n* Azure Machine Learning managed online endpoints\n* Azure Kubernetes Service \n\n> [!TIP]\n> This article is part of a series on securing an Azure Machine Learning workflow. See the other articles in this series:\n>\n> * [Virtual network overview](how-to-network-security-overview.md)\n> * [Secure the workspace resources](how-to-secure-workspace-vnet.md)\n> * [Secure the training environment](how-to-secure-training-vnet.md)\n> * [Enable studio functionality](how-to-enable-studio-virtual-network.md)\n> * [Use custom DNS](how-to-custom-dns.md)\n> * [Use a firewall](how-to-access-azureml-behind-firewall.md)\n>\n> For a tutorial on creating a secure workspace, see [Tutorial: Create a secure workspace](tutorial-create-secure-workspace.md) or [Tutorial: Create a secure workspace using a template](tutorial-create-secure-workspace-template.md).\n\n## Prerequisites\n\n+ Read the [Network security overview](how-to-network-security-overview.md) article to understand common virtual network scenarios and overall virtual network architecture.\n\n+ An existing virtual network and subnet, that is used to secure the Azure Machine Learning workspace.\n\n+ To deploy resources into a virtual network or subnet, your user account must have permissions to the following actions in Azure role-based access control (Azure RBAC):\n\n    - \"Microsoft.Network/virtualNetworks/join/action\" on the virtual network resource.\n    - \"Microsoft.Network/virtualNetworks/subnet/join/action\" on the subnet resource.\n\n    For more information on Azure RBAC with networking, see the [Networking built-in roles](../role-based-access-control/built-in-roles.md#networking).\n\n+ If using Azure Kubernetes Service (AKS), you must have an existing AKS cluster secured as described in the [Secure Azure Kubernetes Service inference environment](how-to-secure-kubernetes-inferencing-environment.md) article.\n\n## Secure managed online endpoints\n\nFor information on securing managed online endpoints, see the [Use network isolation with managed online endpoints](how-to-secure-online-endpoint.md) article.\n\n## Secure Azure Kubernetes Service online endpoints\n\nTo use Azure Kubernetes Service cluster for secure inference, use the following steps:\n\n1. Create or configure a [secure Kubernetes inferencing environment](how-to-secure-kubernetes-inferencing-environment.md).\n2. Deploy [AzureML extension](how-to-deploy-kubernetes-extension.md).\n3. [Attach the Kubernetes cluster to the workspace](how-to-attach-kubernetes-anywhere.md).\n4. Model deployment with Kubernetes online endpoint can be done using CLI v2, Python SDK v2 and Studio UI.\n \n   * CLI v2 - https://github.com/Azure/azureml-examples/tree/main/cli/endpoints/online/kubernetes\n   * Python SDK V2 - https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/online/kubernetes\n   * Studio UI - Follow the steps in [managed online endpoint deployment](how-to-use-managed-online-endpoint-studio.md) through the Studio. After entering the __Endpoint name__ select __Kubernetes__ as the compute type instead of __Managed__ \n\n\n## Limit outbound connectivity from the virtual network\n\nIf you don't want to use the default outbound rules and you do want to limit the outbound access of your virtual network, you must allow access to Azure Container Registry. For example, make sure that your Network Security Groups (NSG) contains a rule that allows access to the __AzureContainerRegistry.RegionName__ service tag where `{RegionName} is the name of an Azure region.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-secure-inferencing-vnet-0.md"
                }
            }
        ]
    },
    {
        "query": "generate a python code samples to debug a failed job using Vscode and AZureML SDK V2?",
        "result": "To debug a failed job using VS Code and AzureML SDK V2, you can use the `parallel_run_function` method to process tabular data. You can set the `logging_level` to \"DEBUG\" and the `mini_batch_error_threshold` to 5. You can also set the `retry_settings` to a maximum of 2 retries and a timeout of 60 seconds. Additionally, you can set the `task_overhead_timeout` to 1200 seconds, the `progress_update_timeout` to 600 seconds, and the `first_task_creation_timeout` to 600 seconds. Finally, you can set the `copy_logs_to_parent` to True and the `resource_monitor_interval` to 20 seconds.",
        "context": [
            {
                "page_content": "The examples in this article can be found in the [Debug online endpoints locally in Visual Studio Code](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/online/managed/debug-online-endpoints-locally-in-visual-studio-code.ipynb) notebook within the[azureml-examples](https://github.com/azure/azureml-examples) repository. To run the code locally, clone the repo and then change directories to the notebook's parent directory `sdk/endpoints/online/managed`. \n\n```azurecli\ngit clone https://github.com/Azure/azureml-examples --depth 1\ncd azureml-examples\ncd sdk/python/endpoints/online/managed\n```\n\nImport the required modules: \n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import (\n    ManagedOnlineEndpoint,\n    ManagedOnlineDeployment,\n    Model,\n    CodeConfiguration,\n    Environment,\n)\nfrom azure.identity import DefaultAzureCredential\n``` \n\nSet up variables for the workspace and endpoint: \n\n```python \nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace_name = \"<AML_WORKSPACE_NAME>\"\n\nendpoint_name = \"<ENDPOINT_NAME>\"\n``` \n\n\n## Launch development container\n\n# [Azure CLI](#tab/cli)\n\nAzure Machine Learning local endpoints use Docker and VS Code development containers (dev container) to build and configure a local debugging environment. With dev containers, you can take advantage of VS Code features from inside a Docker container. For more information on dev containers, see [Create a development container](https://code.visualstudio.com/docs/remote/create-dev-container).\n\nTo debug online endpoints locally in VS Code, use the `--vscode-debug` flag when creating or updating and Azure Machine Learning online deployment. The following command uses a deployment example from the examples repo:\n\n```azurecli\naz ml online-deployment create --file endpoints/online/managed/sample/blue-deployment.yml --local --vscode-debug\n```\n\n> [!IMPORTANT]\n> On Windows Subsystem for Linux (WSL), you'll need to update your PATH environment variable to include the path to the VS Code executable or use WSL interop. For more information, see [Windows interoperability with Linux](/windows/wsl/interop).\n\nA Docker image is built locally. Any environment configuration or model file errors are surfaced at this stage of the process.\n\n> [!NOTE]\n> The first time you launch a new or updated dev container it can take several minutes.\n\nOnce the image successfully builds, your dev container opens in a VS Code window.\n\nYou'll use a few VS Code extensions to debug your deployments in the dev container. Azure Machine Learning automatically installs these extensions in your dev container.\n\n- Inference Debug\n- [Pylance](https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance)\n- [Jupyter](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter)\n- [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python)\n\n> [!IMPORTANT]\n> Before starting your debug session, make sure that the VS Code extensions have finished installing in your dev container.  \n\n\n# [Python](#tab/python)\n\nAzure Machine Learning local endpoints use Docker and VS Code development containers (dev container) to build and configure a local debugging environment. With dev containers, you can take advantage of VS Code features from inside a Docker container. For more information on dev containers, see [Create a development container](https://code.visualstudio.com/docs/remote/create-dev-container).\n\nGet a handle to the workspace: \n\n```python \ncredential = DefaultAzureCredential()\nml_client = MLClient(\n    credential,\n    subscription_id=subscription_id,\n    resource_group_name=resource_group,\n    workspace_name=workspace_name,\n)\n``` \n\nTo debug online endpoints locally in VS Code, set the `vscode-debug` and `local` flags when creating or updating an Azure Machine Learning online deployment. The following code mirrors a deployment example from the examples repo:\n\n```python\ndeployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=endpoint_name,\n    model=Model(path=\"../model-1/model/sklearn_regression_model.pkl\"),\n    code_configuration=CodeConfiguration(\n        code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n    ),\n    environment=Environment(\n        conda_file=\"../model-1/environment/conda.yml\",\n        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n    ),\n    instance_type=\"Standard_DS3_v2\",\n    instance_count=1,\n)\n\ndeployment = ml_client.online_deployments.begin_create_or_update(\n    deployment, local=True, vscode_debug=True\n).result()\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-debug-managed-online-endpoints-visual-studio-code-77.md"
                }
            },
            {
                "page_content": "| overhead timeout | integer | The timeout in second for initialization of each mini-batch. For example, load mini-batch data and pass it to run() function. | (0, 259200] | 600 | N/A | --task_overhead_timeout |\n| progress update timeout | integer | The timeout in second for monitoring the progress of mini-batch execution. If no progress updates receive within this timeout setting, the parallel job will be marked as failed. | (0, 259200] | Dynamically calculated by other settings. | N/A | --progress_update_timeout |\n| first task creation timeout | integer | The timeout in second for monitoring the time between the job start to the run of first mini-batch. | (0, 259200] | 600 | N/A | --first_task_creation_timeout |\n| logging level | string | Define which level of logs will be dumped to user log files. | INFO, WARNING, or DEBUG | INFO | logging_level | N/A |\n| append row to | string | Aggregate all returns from each run of mini-batch and output it into this file. May reference to one of the outputs of parallel job by using the expression ${{outputs.<output_name>}} |  |  | task.append_row_to | N/A |\n| copy logs to parent | string | Boolean option to whether copy the job progress, overview, and logs to the parent pipeline job. | True or False | False | N/A | --copy_logs_to_parent |\n| resource monitor interval | integer | The time interval in seconds to dump node resource usage(for example, cpu, memory) to log folder under \"logs/sys/perf\" path.<br><br>Note: Frequent dump resource logs will slightly slow down the execution speed of your mini-batch. Set this value to \"0\" to stop dumping resource usage. | [0, int.max] | 600 | N/A | --resource_monitor_interval |\n\nSample code to update these settings:\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\n\ndisplay_name: iris-batch-prediction-using-parallel\ndescription: The hello world pipeline job with inline parallel job\ntags:\n  tag: tagvalue\n  owner: sdkteam\n\nsettings:\n  default_compute: azureml:cpu-cluster\n\njobs:\n  batch_prediction:\n    type: parallel\n    compute: azureml:cpu-cluster\n    inputs:\n      input_data: \n        type: mltable\n        path: ./neural-iris-mltable\n        mode: direct\n      score_model: \n        type: uri_folder\n        path: ./iris-model\n        mode: download\n    outputs:\n      job_output_file:\n        type: uri_file\n        mode: rw_mount\n\n    input_data: ${{inputs.input_data}}\n    mini_batch_size: \"10kb\"\n    resources:\n        instance_count: 2\n    max_concurrency_per_instance: 2\n\n    logging_level: \"DEBUG\"\n    mini_batch_error_threshold: 5\n    retry_settings:\n      max_retries: 2\n      timeout: 60\n\n    task:\n      type: run_function\n      code: \"./script\"\n      entry_script: iris_prediction.py\n      environment:\n        name: \"prs-env\"\n        version: 1\n        image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n        conda_file: ./environment/environment_parallel.yml\n      program_arguments: >-\n        --model ${{inputs.score_model}}\n        --error_threshold 5\n        --allowed_failed_percent 30\n        --task_overhead_timeout 1200\n        --progress_update_timeout 600\n        --first_task_creation_timeout 600\n        --copy_logs_to_parent True\n        --resource_monitor_interva 20\n      append_row_to: ${{outputs.job_output_file}}\n\n```\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\n# parallel task to process tabular data\ntabular_batch_inference = parallel_run_function(\n    name=\"batch_score_with_tabular_input\",\n    display_name=\"Batch Score with Tabular Dataset\",\n    description=\"parallel component for batch score\",\n    inputs=dict(\n        job_data_path=Input(\n            type=AssetTypes.MLTABLE,\n            description=\"The data to be split and scored in parallel\",\n        ),\n        score_model=Input(\n            type=AssetTypes.URI_FOLDER, description=\"The model for batch score.\"\n        ),\n    ),\n    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n    input_data=\"${{inputs.job_data_path}}\",\n    instance_count=2,\n    max_concurrency_per_instance=2,\n    mini_batch_size=\"100\",\n    mini_batch_error_threshold=5,\n    logging_level=\"DEBUG\",\n    retry_settings=dict(max_retries=2, timeout=60),\n    task=RunFunction(\n        code=\"./src\",\n        entry_script=\"tabular_batch_inference.py\",\n        environment=Environment(\n            image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n            conda_file=\"./src/environment_parallel.yml\",\n        ),\n        program_arguments=\"--model ${{inputs.score_model}} \"\n        \"--job_output_path ${{outputs.job_output_path}} \"\n        \"--error_threshold 5 \"\n        \"--allowed_failed_percent 30 \"\n        \"--task_overhead_timeout 1200 \"\n        \"--progress_update_timeout 600 \"\n        \"--first_task_creation_timeout 600 \"\n        \"--copy_logs_to_parent True \"\n        \"--resource_monitor_interva 20 \",\n        append_row_to=\"${{outputs.job_output_path}}\",\n    ),\n)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-parallel-job-in-pipeline-423.md"
                }
            },
            {
                "page_content": "\n# Set up the Visual Studio Code Azure Machine Learning extension (preview)\n\nLearn how to set up the Azure Machine Learning Visual Studio Code extension for your machine learning workflows.\n\nThe Azure Machine Learning extension for VS Code provides a user interface to:\n\n- Manage Azure Machine Learning resources (experiments, virtual machines, models, deployments, etc.)\n- Develop locally using remote compute instances\n- Train machine learning models\n- Debug machine learning experiments locally\n- Schema-based language support, autocompletion and diagnostics for specification file authoring\n\n## Prerequisites\n\n- Azure subscription. If you don't have one, sign up to try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n- Visual Studio Code. If you don't have it, [install it](https://code.visualstudio.com/docs/setup/setup-overview).\n- [Python](https://www.python.org/downloads/)\n- (Optional) To create resources using the extension, you need to install the CLI (v2). For setup instructions, see [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n- Clone the community driven repository\n    ```bash\n        git clone https://github.com/Azure/azureml-examples.git --depth 1\n    ```\n\n## Install the extension\n\n1. Open Visual Studio Code.\n1. Select **Extensions** icon from the **Activity Bar** to open the Extensions view.\n1. In the Extensions view search bar, type \"Azure Machine Learning\" and select the first extension.\n1. Select **Install**.\n\n    > [!div class=\"mx-imgBorder\"]\n    > ![Install Azure Machine Learning VS Code Extension](./media/how-to-setup-vs-code/install-aml-vscode-extension.PNG)\n\n> [!NOTE]\n> The Azure Machine Learning VS Code extension uses the CLI (v2) by default. To switch to the 1.0 CLI, set the `azureML.CLI Compatibility Mode` setting in Visual Studio Code to `1.0`. For more information on modifying your settings in Visual Studio, see the [user and workspace settings documentation](https://code.visualstudio.com/docs/getstarted/settings).\n\n## Sign in to your Azure Account\n\nIn order to provision resources and job workloads on Azure, you have to sign in with your Azure account credentials. To assist with account management, Azure Machine Learning automatically installs the Azure Account extension. Visit the following site to [learn more about the Azure Account extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.azure-account).\n\nTo sign into you Azure account, select the **Azure: Sign In** button in the bottom right corner on the Visual Studio Code status bar to start the sign in process.\n\n## Choose your default workspace\n\nChoosing a default Azure Machine Learning workspace enables the following when authoring CLI (v2) YAML specification files:\n\n- Schema validation\n- Autocompletion\n- Diagnostics\n\nIf you don't have a workspace, create one. For more information, see [manage Azure Machine Learning resources with the VS Code extension](how-to-manage-resources-vscode.md).\n\nTo choose your default workspace, select the **Set Azure ML Workspace** button on the Visual Studio Code status bar and follow the prompts to set your workspace.\n\nAlternatively, use the `> Azure ML: Set Default Workspace` command in the command palette and follow the prompts to set your workspace.\n\n## Next Steps\n\n- [Manage your Azure Machine Learning resources](how-to-manage-resources-vscode.md)\n- [Develop on a remote compute instance locally](how-to-set-up-vs-code-remote.md)\n- [Use a compute instances as a remote Jupyter server](how-to-set-up-vs-code-remote.md)\n- [Train an image classification model using the Visual Studio Code extension](tutorial-train-deploy-image-classification-model-vscode.md)\n- [Run and debug machine learning experiments locally](how-to-debug-visual-studio-code.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-setup-vs-code-0.md"
                }
            }
        ]
    },
    {
        "query": "How can I see progress of an online endpoint deploying in AzureML using CLI V2 ?",
        "result": "You can use the `az ml endpoint deployment show` command to see the status of an online endpoint deployment in AzureML using CLI V2. This command shows the status of the deployment, including the endpoint name, deployment name, status, and other details. You can also use the `--watch` flag to continuously monitor the deployment status until it is complete.",
        "context": [
            {
                "page_content": "\n# Debug online endpoints locally in Visual Studio Code\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\nLearn how to use the Visual Studio Code (VS Code) debugger to test and debug online endpoints locally before deploying them to Azure.\n\nAzure Machine Learning local endpoints help you test and debug your scoring script, environment configuration, code configuration, and machine learning model locally.\n\n## Online endpoint local debugging\n\nDebugging endpoints locally before deploying them to the cloud can help you catch errors in your code and configuration earlier. You have different options for debugging endpoints locally with VS Code.\n\n- [Azure Machine Learning inference HTTP server (Preview)](how-to-inference-server-http.md)\n- Local endpoint\n\nThis guide focuses on local endpoints.\n\nThe following table provides an overview of scenarios to help you choose what works best for you.\n\n| Scenario | Inference HTTP Server | Local endpoint |\n|--|--|--|\n| Update local Python environment, **without** Docker image rebuild | Yes | No |\n| Update scoring script | Yes | Yes |\n| Update deployment configurations (deployment, environment, code, model) | No | Yes |\n| VS Code Debugger integration | Yes | Yes |\n\n## Prerequisites\n\n# [Azure CLI](#tab/cli)\n\nThis guide assumes you have the following items installed locally on your PC.\n\n- [Docker](https://docs.docker.com/engine/install/)\n- [VS Code](https://code.visualstudio.com/#alt-downloads)\n- [Azure CLI](/cli/azure/install-azure-cli)\n- [Azure CLI `ml` extension (v2)](how-to-configure-cli.md)\n\nFor more information, see the guide on [how to prepare your system to deploy online endpoints](how-to-deploy-online-endpoints.md#prepare-your-system).\n\nThe examples in this article are based on code samples contained in the [azureml-examples](https://github.com/azure/azureml-examples) repository. To run the commands locally without having to copy/paste YAML and other files, clone the repo and then change directories to the `cli` directory in the repo:\n\n```azurecli\ngit clone https://github.com/Azure/azureml-examples --depth 1\ncd azureml-examples\ncd cli\n```\n\nIf you haven't already set the defaults for the Azure CLI, save your default settings. To avoid passing in the values for your subscription, workspace, and resource group multiple times, use the following commands. Replace the following parameters with values for your specific configuration:\n\n* Replace `<subscription>` with your Azure subscription ID.\n* Replace `<workspace>` with your Azure Machine Learning workspace name.\n* Replace `<resource-group>` with the Azure resource group that contains your workspace.\n* Replace `<location>` with the Azure region that contains your workspace.\n\n> [!TIP]\n> You can see what your current defaults are by using the `az configure -l` command.\n\n```azurecli\naz account set --subscription <subscription>\naz configure --defaults workspace=<workspace> group=<resource-group> location=<location>\n```\n\n# [Python](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nThis guide assumes you have the following items installed locally on your PC.\n\n- [Docker](https://docs.docker.com/engine/install/)\n- [VS Code](https://code.visualstudio.com/#alt-downloads)\n- [Azure CLI](/cli/azure/install-azure-cli)\n- [Azure CLI `ml` extension (v2)](how-to-configure-cli.md)\n- [Azure ML Python SDK (v2)](https://aka.ms/sdk-v2-install)\n\nFor more information, see the guide on [how to prepare your system to deploy online endpoints](how-to-deploy-online-endpoints.md#prepare-your-system).\n\nThe examples in this article can be found in the [Debug online endpoints locally in Visual Studio Code](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/online/managed/debug-online-endpoints-locally-in-visual-studio-code.ipynb) notebook within the[azureml-examples](https://github.com/azure/azureml-examples) repository. To run the code locally, clone the repo and then change directories to the notebook's parent directory `sdk/endpoints/online/managed`. \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-debug-managed-online-endpoints-visual-studio-code-0.md"
                }
            },
            {
                "page_content": "\n# Upgrade steps for Azure Container Instances web services to managed online endpoints\n\n[Managed online endpoints](concept-endpoints.md#what-are-online-endpoints) help to deploy your ML models in a turnkey manner. Managed online endpoints work with powerful CPU and GPU machines in Azure in a scalable, fully managed way. Managed online endpoints take care of serving, scaling, securing, and monitoring your models, freeing you from the overhead of setting up and managing the underlying infrastructure. Details can be found on [Deploy and score a machine learning model by using an online endpoint](how-to-deploy-online-endpoints.md).\n\nYou can deploy directly to the new compute target with your previous models and environments, or use the [scripts](https://aka.ms/moeonboard) provided by us to export the current services and then deploy to the new compute without affecting your existing services. If you regularly create and delete Azure Container Instances (ACI) web services, we strongly recommend the deploying directly and not using the scripts. \n\n> [!IMPORTANT]\n> **The scoring URL will be changed after upgrade**. For example, the scoring url for ACI web service is like `http://aaaaaa-bbbbb-1111.westus.azurecontainer.io/score`. The scoring URI for a managed online endpoint is like `https://endpoint-name.westus.inference.ml.azure.com/score`.\n\n## Supported scenarios and differences\n\n### Auth mode\nNo auth isn't supported for managed online endpoint. If you use the upgrade scripts, it will convert it to key auth.\nFor key auth, the original keys will be used. Token-based auth is also supported.\n\n### TLS\nFor ACI service secured with HTTPS, you don't need to provide your own certificates anymore, all the managed online endpoints are protected by TLS.\n\nCustom DNS name **isn't** supported.\n\n### Resource requirements\n[ContainerResourceRequirements](/python/api/azureml-core/azureml.core.webservice.aci.containerresourcerequirements) isn't supported, you can choose the proper [SKU](reference-managed-online-endpoints-vm-sku-list.md) for your inferencing.\nThe upgrade tool will map the CPU/Memory requirement to corresponding SKU. If you choose to redeploy manually through CLI/SDK V2, we also suggest the corresponding SKU for your new deployment.\n\n| CPU request | Memory request in GB | Suggested SKU |\n| :----| :---- | :---- |\n| (0, 1] | (0, 1.2] | DS1 V2 |\n| (1, 2] | (1.2, 1.7] | F2s V2 |\n| (1, 2] | (1.7, 4.7] | DS2 V2 |\n| (1, 2] | (4.7, 13.7] | E2s V3 |\n| (2, 4] | (0, 5.7] | F4s V2 |\n| (2, 4] | (5.7, 11.7] | DS3 V2 |\n| (2, 4] | (11.7, 16] | E4s V3 |\n\n\"(\" means greater than and \"]\" means less than or equal to. For example, \u201c(0, 1]\u201d means \u201cgreater than 0 and less than or equal to 1\u201d.\n\n> [!IMPORTANT]\n> When upgrading from ACI, there will be some changes in how you'll be charged. See [our blog](https://aka.ms/acimoemigration) for a rough cost comparison to help you choose the right VM SKUs for your workload.\n\n### Network isolation\nFor private workspace and VNet scenarios, see [Use network isolation with managed online endpoints](how-to-secure-online-endpoint.md?tabs=model).\n\n> [!IMPORTANT]\n> As there are many settings for your workspace and VNet, we strongly suggest that redeploy through the Azure CLI extension v2 for machine learning instead of the script tool.\n\n## Not supported\n+ [EncryptionProperties](/python/api/azureml-core/azureml.core.webservice.aci.encryptionproperties) for ACI container isn't supported.\n+ ACI web services deployed through deploy_from_model and deploy_from_image isn't supported by the upgrade tool. Redeploy manually through CLI/SDK V2.\n\n## Upgrade steps\n\n### With our [CLI](how-to-deploy-online-endpoints.md) or [SDK preview](how-to-deploy-managed-online-endpoint-sdk-v2.md)\nRedeploy manually with your model files and environment definition.\nYou can find our examples on [azureml-examples](https://github.com/Azure/azureml-examples). Specifically, this is the [SDK example for managed online endpoint](https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/online/managed).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-managed-online-endpoints-0.md"
                }
            },
            {
                "page_content": "\n# Progressive rollout of MLflow models to Online Endpoints\n\nIn this article, you'll learn how you can progressively update and deploy MLflow models to Online Endpoints without causing service disruption. You'll use blue-green deployment, also known as a safe rollout strategy, to introduce a new version of a web service to production. This strategy will allow you to roll out your new version of the web service to a small subset of users or requests before rolling it out completely.\n\n## About this example\n\nOnline Endpoints have the concept of __Endpoint__ and __Deployment__. An endpoint represents the API that customers use to consume the model, while the deployment indicates the specific implementation of that API. This distinction allows users to decouple the API from the implementation and to change the underlying implementation without affecting the consumer. This example will use such concepts to update the deployed model in endpoints without introducing service disruption. \n\nThe model we will deploy is based on the [UCI Heart Disease Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). The database contains 76 attributes, but we are using a subset of 14 of them. The model tries to predict the presence of heart disease in a patient. It is integer valued from 0 (no presence) to 1 (presence). It has been trained using an `XGBBoost` classifier and all the required preprocessing has been packaged as a `scikit-learn` pipeline, making this model an end-to-end pipeline that goes from raw data to predictions.\n\nThe information in this article is based on code samples contained in the [azureml-examples](https://github.com/azure/azureml-examples) repository. To run the commands locally without having to copy/paste files, clone the repo, and then change directories to `sdk/using-mlflow/deploy`.\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in the following notebooks. In the cloned repository, open the notebook: [mlflow_sdk_online_endpoints_progresive.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/using-mlflow/deploy/mlflow_sdk_online_endpoints_progresive.ipynb).\n\n## Prerequisites\n\nBefore following the steps in this article, make sure you have the following prerequisites:\n\n- An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n- Azure role-based access controls (Azure RBAC) are used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned the owner or contributor role for the Azure Machine Learning workspace, or a custom role allowing Microsoft.MachineLearningServices/workspaces/onlineEndpoints/*. For more information, see [Manage access to an Azure Machine Learning workspace](how-to-assign-roles.md).\n\nAdditionally, you will need to:\n\n# [Azure CLI](#tab/cli)\n\n- Install the Azure CLI and the ml extension to the Azure CLI. For more information, see [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n\n# [Python (Azure ML SDK)](#tab/sdk)\n\n- Install the Azure Machine Learning SDK for Python\n    \n    ```bash\n    pip install azure-ai-ml\n    ```\n    \n# [Python (MLflow SDK)](#tab/mlflow)\n\n- Install the Mlflow SDK package `mlflow` and the Azure Machine Learning plug-in for MLflow `azureml-mlflow`.\n\n    ```bash\n    pip install mlflow azureml-mlflow\n    ```\n\n- If you are not running in Azure Machine Learning compute, configure the MLflow tracking URI or MLflow's registry URI to point to the workspace you are working on. See [Configure MLflow for Azure Machine Learning](how-to-use-mlflow-configure-tracking.md) for more details.\n\n\n### Connect to your workspace\n\nFirst, let's connect to Azure Machine Learning workspace where we are going to work on.\n\n# [Azure CLI](#tab/cli)\n\n```azurecli\naz account set --subscription <subscription>\naz configure --defaults workspace=<workspace> group=<resource-group> location=<location>\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-mlflow-models-online-progressive-0.md"
                }
            }
        ]
    },
    {
        "query": "How can I setup an online endpoint in AzureML inside my virtual network using CLI V2 ?",
        "result": "To set up an online endpoint in AzureML inside your virtual network using CLI V2, you can use the `public_network_access` flag for the endpoint to `disabled`. You can use the following Azure CLI command to create a managed online endpoint: \n\n```azurecli\naz ml online-endpoint create -f endpoint.yml --set public_network_access=disabled\n```\n\nYou can also use the Azure Machine Learning studio to create the deployment setup wizard and disable the `Public network access` flag.",
        "context": [
            {
                "page_content": "> The end-to-end example in this article comes from the files in the __azureml-examples__ GitHub repository. To clone the samples repository and switch to the repository's `cli/` directory, use the following commands: \n>\n> ```azurecli\n> git clone https://github.com/Azure/azureml-examples\n> cd azureml-examples/cli\n> ```\n\n## Limitations\n\n* The `v1_legacy_mode` flag must be disabled (false) on your Azure Machine Learning workspace. If this flag is enabled, you won't be able to create a managed online endpoint. For more information, see [Network isolation with v2 API](how-to-configure-network-isolation-with-v2.md).\n\n* If your Azure Machine Learning workspace has a private endpoint that was created before May 24, 2022, you must recreate the workspace's private endpoint before configuring your online endpoints to use a private endpoint. For more information on creating a private endpoint for your workspace, see [How to configure a private endpoint for Azure Machine Learning workspace](how-to-configure-private-link.md).\n\n* Secure outbound communication creates three private endpoints per deployment. One to the Azure Blob storage, one to the Azure Container Registry, and one to your workspace.\n\n* When you use network isolation with a deployment, Azure Log Analytics is partially supported. All metrics and the `AMLOnlineEndpointTrafficLog` table are supported via Azure Log Analytics. `AMLOnlineEndpointConsoleLog` and `AMLOnlineEndpointEventLog` tables are currently not supported. As a workaround, you can use the [az ml online-deployment get_logs](/cli/azure/ml/online-deployment#az-ml-online-deployment-get-logs) CLI command, the [OnlineDeploymentOperations.get_logs()](/python/api/azure-ai-ml/azure.ai.ml.operations.onlinedeploymentoperations#azure-ai-ml-operations-onlinedeploymentoperations-get-logs) Python SDK, or the Deployment log tab in the Azure Machine Learning studio instead. For more information, see [Monitoring online endpoints](how-to-monitor-online-endpoints.md).\n\n* You can configure public access to a __managed online endpoint__ (_inbound_ and _outbound_). You can also configure [public access to an Azure Machine Learning workspace](how-to-configure-private-link.md#enable-public-access).\n\n    Outbound communication from a managed online endpoint deployment is to the _workspace API_. When the endpoint is configured to use __public outbound__, then the workspace must be able to accept that public communication (allow public access).\n\n> [!NOTE]\n> Requests to create, update, or retrieve the authentication keys are sent to the Azure Resource Manager over the public network.\n \n## Inbound (scoring)\n\nTo secure scoring requests to the online endpoint to your virtual network, set the `public_network_access` flag for the endpoint to `disabled`:\n\n# [Azure CLI](#tab/cli)\n\n```azurecli\naz ml online-endpoint create -f endpoint.yml --set public_network_access=disabled\n```\n\n# [Python](#tab/python)\n\n```python\nfrom azure.ai.ml.entities import ManagedOnlineEndpoint\n\nendpoint = ManagedOnlineEndpoint(name='my-online-endpoint',  \n                         description='this is a sample online endpoint', \n                         tags={'foo': 'bar'}, \n                         auth_mode=\"key\", \n                         public_network_access=\"disabled\" \n                         # public_network_access=\"enabled\" \n)\n```\n\n# [Studio](#tab/azure-studio)\n\n1. Go to the [Azure Machine Learning studio](https://ml.azure.com).\n1. Select the **Workspaces** page from the left navigation bar.\n1. Enter a workspace by clicking its name.\n1. Select the **Endpoints** page from the left navigation bar.\n1. Select **+ Create** to open the **Create deployment** setup wizard.\n1. Disable the **Public network access** flag at the **Create endpoint** step.\n\n    :::image type=\"content\" source=\"media/how-to-secure-online-endpoint/endpoint-disable-public-network-access.png\" alt-text=\"A screenshot of how to disable public network access for an endpoint.\" lightbox=\"media/how-to-secure-online-endpoint/endpoint-disable-public-network-access.png\":::\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-secure-online-endpoint-38.md"
                }
            },
            {
                "page_content": "* The Azure Storage blob that is the default storage for the workspace\n* The Azure Container Registry for the workspace\n\nWhen you configure the `egress_public_network_access` to `disabled`, a new private endpoint is created per deployment, per service. For example, if you set the flag to `disabled` for three deployments to an online endpoint, nine private endpoints are created. Each deployment would have three private endpoints to communicate with the workspace, blob, and container registry.\n\n## Scenarios\n\nThe following table lists the supported configurations when configuring inbound and outbound communications for an online endpoint:\n\n| Configuration | Inbound </br> (Endpoint property) | Outbound </br> (Deployment property) | Supported? |\n| -------- | -------------------------------- | --------------------------------- | --------- |\n| secure inbound with secure outbound | `public_network_access` is disabled | `egress_public_network_access` is disabled   | Yes |\n| secure inbound with public outbound | `public_network_access` is disabled</br>The workspace must also allow public access. | `egress_public_network_access` is enabled  | Yes |\n| public inbound with secure outbound | `public_network_access` is enabled | `egress_public_network_access` is disabled    | Yes |\n| public inbound with public outbound | `public_network_access` is enabled</br>The workspace must also allow public access. | `egress_public_network_access` is enabled  | Yes |\n\n> [!IMPORTANT]\n> Outbound communication from managed online endpoint deployment is to the _workspace API_. When the endpoint is configured to use __public outbound__, then the workspace must be able to accept that public communication (allow public access).\n\n## End-to-end example\n\nUse the information in this section to create an example configuration that uses private endpoints to secure online endpoints.\n\n> [!TIP]\n> In this example, and Azure Virtual Machine is created inside the VNet. You connect to the VM using SSH, and run the deployment from the VM. This configuration is used to simplify the steps in this example, and does not represent a typical secure configuration. For example, in a production environment you would most likely use a VPN client or Azure ExpressRoute to directly connect clients to the virtual network.\n\n### Create workspace and secured resources\n\nThe steps in this section use an Azure Resource Manager template to create the following Azure resources:\n\n* Azure Virtual Network\n* Azure Machine Learning workspace\n* Azure Container Registry\n* Azure Key Vault\n* Azure Storage account (blob & file storage)\n\nPublic access is disabled for all the services. While the Azure Machine Learning workspace is secured behind a vnet, it's configured to allow public network access. For more information, see [CLI 2.0 secure communications](how-to-configure-cli.md#secure-communications). A scoring subnet is created, along with outbound rules that allow communication with the following Azure services:\n\n* Azure Active Directory\n* Azure Resource Manager\n* Azure Front Door\n* Microsoft Container Registries\n\nThe following diagram shows the different components created in this architecture:\n\nThe following diagram shows the overall architecture of this example:\n\n:::image type=\"content\" source=\"./media/how-to-secure-online-endpoint/endpoint-network-isolation-diagram.png\" alt-text=\"Diagram of the services created.\":::\n\nTo create the resources, use the following Azure CLI commands. To create a resource group. Replace `<my-resource-group>` and `<my-location>` with the desierd values.  \n\n```azurecli\n# create resource group\naz group create --name <my-resource-group> --location <my-location>\n```\n\nClone the example files for the deployment, use the following command:\n\n```azurecli\n#Clone the example files\ngit clone https://github.com/Azure/azureml-examples\n```\n\nTo create the resources, use the following Azure CLI commands. Replace `<UNIQUE_SUFFIX>` with a unique suffix for the resources that are created.\n\n```azurecli\naz deployment group create --template-file endpoints/online/managed/vnet/setup_ws/main.bicep --parameters suffix=$SUFFIX --resource-group <my-resource-group>\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-secure-online-endpoint-149.md"
                }
            },
            {
                "page_content": "> With the previous extension (`azure-cli-ml`, sometimes called 'CLI v1'), only some of the commands communicate with the Azure Resource Manager. Specifically, commands that create, update, delete, list, or show Azure resources. Operations such as submitting a training job communicate directly with the Azure Machine Learning workspace. If your workspace is [secured with a private endpoint](how-to-configure-private-link.md), that is enough to secure commands provided by the `azure-cli-ml` extension.\n\n# [Public workspace](#tab/public)\n\nIf your Azure Machine Learning workspace is public (that is, not behind a virtual network), then there is no additional configuration required. Communications are secured using HTTPS/TLS 1.2\n\n# [Private workspace](#tab/private)\n\nIf your Azure Machine Learning workspace uses a private endpoint and virtual network, choose one of the following configurations to use:\n\n* If you are __OK__ with the CLI v2 communication over the public internet, use the following `--public-network-access` parameter for the `az ml workspace update` command to enable public network access. For example, the following command updates a workspace for public network access:\n\n    ```azurecli\n    az ml workspace update --name myworkspace --public-network-access enabled\n    ```\n\n* If you are __not OK__ with the CLI v2 communication over the public internet, you can use an Azure Private Link to increase security of the communication. Use the following links to secure communications with Azure Resource Manager by using Azure Private Link.\n\n    1. [Secure your Azure Machine Learning workspace inside a virtual network using a private endpoint](how-to-configure-private-link.md).\n    2. [Create a Private Link for managing Azure resources](../azure-resource-manager/management/create-private-link-access-portal.md). \n    3. [Create a private endpoint](../azure-resource-manager/management/create-private-link-access-portal.md#create-private-endpoint) for the Private Link created in the previous step.\n\n    > [!IMPORTANT]\n    > To configure the private link for Azure Resource Manager, you must be the _subscription owner_ for the Azure subscription, and an _owner_ or _contributor_ of the root management group. For more information, see [Create a private link for managing Azure resources](../azure-resource-manager/management/create-private-link-access-portal.md).\n\n\n## Next steps\n\n- [Train models using CLI (v2)](how-to-train-model.md)\n- [Set up the Visual Studio Code Azure Machine Learning extension](how-to-setup-vs-code.md)\n- [Train an image classification TensorFlow model using the Azure Machine Learning Visual Studio Code extension](tutorial-train-deploy-image-classification-model-vscode.md)\n- [Explore Azure Machine Learning with examples](samples-notebooks.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-configure-cli-998.md"
                }
            }
        ]
    },
    {
        "query": "Code Samples to create an MLTable and use it in a Pipeline Job using AzureML SDK V2?",
        "result": "Here is an example of how to create an MLTable and use it in a Pipeline Job using AzureML SDK V2:\n\n```python\nfrom azureml.core import Workspace, Dataset, Datastore, Environment, ScriptRunConfig, Experiment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.data import OutputFileDatasetConfig\nfrom azureml.data.dataset_consumption_config import DatasetConsumptionConfig\nfrom azureml.data.datapath import DataPath\n\n# Create a workspace object from the existing workspace\nws = Workspace.from_config()\n\n# Create a datastore object from the existing datastore\ndatastore = Datastore.get(ws, \"mydatastore\")\n\n# Create an MLTable object from a CSV file in the datastore\ndataset = Dataset.Tabular.from_delimited_files(\n    path=(datastore, \"mydataset.csv\"),\n    separator=\",\",\n    header=True,\n    infer_column_types=True,\n    set_column_types=None,\n    support_multi_line=False,\n    empty_as_string=False,\n    encoding=\"utf8\",\n)\n\n# Create a PipelineData object to hold the output of the pipeline step\noutput_data = PipelineData(\n    \"output_data\", datastore=datastore, output_name=\"output_data\"\n)\n\n# Create a PipelineParameter object to hold a parameter for the pipeline step\nparameter = PipelineParameter(name=\"parameter\", default_value=\"default_value\")\n\n# Create a Pipeline object\npipeline = Pipeline(workspace=ws, steps=[])\n\n# Create a PythonScriptStep object to run a Python script in the pipeline\nstep = PythonScriptStep(\n    name=\"step\",\n    script_name=\"script.py\",\n    arguments=[\"--parameter\", parameter, \"--input_data\", dataset.as_named_input(\"input_data\"), \"--output_data\", output_data],\n    inputs=[dataset.as_named_input(\"input_data\")],\n    outputs=[output_data],\n    compute_target=\"mycompute\",\n    source_directory=\".\",\n    runconfig=RunConfiguration(),\n    allow_reuse=True,\n)\n\n# Add the step to the pipeline\npipeline.steps.append(step)\n\n# Submit the pipeline to run\npipeline_run = Experiment(ws, \"myexperiment\").submit(pipeline)\n```\n\nIn this example, we create an MLTable object from a CSV file in a datastore, create a PipelineData object to hold the output of the pipeline step, create a PipelineParameter object to hold a parameter for the pipeline step, create a Pipeline object, create a PythonScriptStep object to run a Python script in the pipeline, add the step to the pipeline, and submit the pipeline to run.",
        "context": [
            {
                "page_content": "\nFinally use your parallel job as a step in your pipeline and bind its inputs/outputs with other steps:\n\n```python\n@pipeline()\ndef parallel_in_pipeline(pipeline_job_data_path, pipeline_score_model):\n\n    prepare_file_tabular_data = prepare_data(input_data=pipeline_job_data_path)\n    # output of file & tabular data should be type MLTable\n    prepare_file_tabular_data.outputs.file_output_data.type = AssetTypes.MLTABLE\n    prepare_file_tabular_data.outputs.tabular_output_data.type = AssetTypes.MLTABLE\n\n    batch_inference_with_file_data = file_batch_inference(\n        job_data_path=prepare_file_tabular_data.outputs.file_output_data\n    )\n    # use eval_mount mode to handle file data\n    batch_inference_with_file_data.inputs.job_data_path.mode = (\n        InputOutputModes.EVAL_MOUNT\n    )\n    batch_inference_with_file_data.outputs.job_output_path.type = AssetTypes.MLTABLE\n\n    batch_inference_with_tabular_data = tabular_batch_inference(\n        job_data_path=prepare_file_tabular_data.outputs.tabular_output_data,\n        score_model=pipeline_score_model,\n    )\n    # use direct mode to handle tabular data\n    batch_inference_with_tabular_data.inputs.job_data_path.mode = (\n        InputOutputModes.DIRECT\n    )\n\n    return {\n        \"pipeline_job_out_file\": batch_inference_with_file_data.outputs.job_output_path,\n        \"pipeline_job_out_tabular\": batch_inference_with_tabular_data.outputs.job_output_path,\n    }\n\n\npipeline_job_data_path = Input(\n    path=\"./dataset/\", type=AssetTypes.MLTABLE, mode=InputOutputModes.RO_MOUNT\n)\npipeline_score_model = Input(\n    path=\"./model/\", type=AssetTypes.URI_FOLDER, mode=InputOutputModes.DOWNLOAD\n)\n# create a pipeline\npipeline_job = parallel_in_pipeline(\n    pipeline_job_data_path=pipeline_job_data_path,\n    pipeline_score_model=pipeline_score_model,\n)\npipeline_job.outputs.pipeline_job_out_tabular.type = AssetTypes.URI_FILE\n\n# set pipeline level compute\npipeline_job.settings.default_compute = \"cpu-cluster\"\n```\n\n\n## Submit pipeline job and check parallel step in Studio UI\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nYou can submit your pipeline job with parallel step by using the CLI command:\n\n```azurecli\naz ml job create --file pipeline.yml\n```\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nYou can submit your pipeline job with parallel step by using `jobs.create_or_update` function of ml_client:\n\n```python\npipeline_job = ml_client.jobs.create_or_update(\n    pipeline_job, experiment_name=\"pipeline_samples\"\n)\npipeline_job\n```\n\n\nOnce you submit your pipeline job, the SDK or CLI widget will give you a web URL link to the Studio UI. The link will guide you to the pipeline graph view by default. Double select the parallel step to open the right panel of your parallel job.\n\nTo check the settings of your parallel job, navigate to **Parameters** tab, expand **Run settings**, and check **Parallel** section:\n\n:::image type=\"content\" source=\"./media/how-to-use-parallel-job-in-pipeline/screenshot-for-parallel-job-settings.png\" alt-text=\"Screenshot of Azure ML studio on the jobs tab showing the parallel job settings.\" lightbox =\"./media/how-to-use-parallel-job-in-pipeline/screenshot-for-parallel-job-settings.png\":::\n\nTo debug the failure of your parallel job, navigate to **Outputs + Logs** tab, expand **logs** folder from output directories on the left, and check **job_result.txt** to understand why the parallel job is failed. For more detail about logging structure of parallel job, see the **readme.txt** under the same folder.\n\n:::image type=\"content\" source=\"./media/how-to-use-parallel-job-in-pipeline/screenshot-for-parallel-job-result.png\" alt-text=\"Screenshot of Azure ML studio on the jobs tab showing the parallel job results.\" lightbox =\"./media/how-to-use-parallel-job-in-pipeline/screenshot-for-parallel-job-result.png\":::\n\n## Parallel job in pipeline examples\n\n- Azure CLI + YAML:\n    - [Iris prediction using parallel](https://github.com/Azure/azureml-examples/tree/sdk-preview/cli/jobs/pipelines/iris-batch-prediction-using-parallel) (tabular input)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-parallel-job-in-pipeline-703.md"
                }
            },
            {
                "page_content": "\n# How to use parallel job in pipeline (V2)\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\nParallel job lets users accelerate their job execution by distributing repeated tasks on powerful multi-nodes compute clusters. For example, take the scenario where you're running an object detection model on large set of images. With Azure ML Parallel job, you can easily distribute your images to run custom code in parallel on a specific compute cluster. Parallelization could significantly reduce the time cost. Also by using Azure ML parallel job you can simplify and automate your process to make it more efficient.\n\n## Prerequisite\n\nAzure ML parallel job can only be used as one of steps in a pipeline job. Thus, it's important to be familiar with using pipelines. To learn more about Azure ML pipelines, see the following articles.\n\n- Understand what is a [Azure Machine Learning pipeline](concept-ml-pipelines.md)\n- Understand how to use Azure ML pipeline with [CLI v2](how-to-create-component-pipelines-cli.md) and [SDK v2](how-to-create-component-pipeline-python.md).\n\n## Why are parallel jobs needed?\n\nIn the real world, ML engineers always have scale requirements on their training or inferencing tasks. For example, when a data scientist provides a single script to train a sales prediction model, ML engineers need to apply this training task to each individual store. During this scale out process, some challenges are:\n\n- Delay pressure caused by long execution time.\n- Manual intervention to handle unexpected issues to keep the task proceeding.\n\nThe core value of Azure ML parallel job is to split a single serial task into mini-batches and dispatch those mini-batches to multiple computes to execute in parallel. By using parallel jobs, we can:\n\n - Significantly reduce end-to-end execution time.\n - Use Azure ML parallel job's automatic error handling settings.\n\nYou should consider using Azure ML Parallel job if:\n\n - You plan to train many models on top of your partitioned data.\n - You want to accelerate your large scale batch inferencing task.\n\n## Prepare for parallel job\n\nUnlike other types of jobs, a parallel job requires preparation. Follow the next sections to prepare for creating your parallel job.\n\n### Declare the inputs to be distributed and partition setting\n\nParallel job requires only one **major input data** to be split and processed with parallel. The major input data can be either tabular data or a set of files. Different input types can have a different partition method.\n\nThe following table illustrates the relation between input data and partition setting:\n\n| Data format | AML input type | AML input mode | Partition method |\n|: ---------- |: ------------- |: ------------- |: --------------- |\n| File list | `mltable` or<br>`uri_folder` | ro_mount or<br>download | By size (number of files) |\n| Tabular data | `mltable` | direct | By size (estimated physical size) |\n\nYou can declare your major input data with `input_data` attribute in parallel job YAML or Python SDK. And you can bind it with one of your defined `inputs` of your parallel job by using `${{inputs.<input name>}}`. Then to define the partition method for your major input.\n\nFor example, you could set numbers to `mini_batch_size` to partition your data **by size**.\n\n- When using file list input, this value defines the number of files for each mini-batch.\n- When using tabular input, this value defines the estimated physical size for each mini-batch.\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\n\ndisplay_name: iris-batch-prediction-using-parallel\ndescription: The hello world pipeline job with inline parallel job\ntags:\n  tag: tagvalue\n  owner: sdkteam\n\nsettings:\n  default_compute: azureml:cpu-cluster\n\njobs:\n  batch_prediction:\n    type: parallel\n    compute: azureml:cpu-cluster\n    inputs:\n      input_data: \n        type: mltable\n        path: ./neural-iris-mltable\n        mode: direct\n      score_model: \n        type: uri_folder\n        path: ./iris-model\n        mode: download\n    outputs:\n      job_output_file:\n        type: uri_file\n        mode: rw_mount\n\n    input_data: ${{inputs.input_data}}\n    mini_batch_size: \"10kb\"\n    resources:\n        instance_count: 2\n    max_concurrency_per_instance: 2\n\n    logging_level: \"DEBUG\"\n    mini_batch_error_threshold: 5\n    retry_settings:\n      max_retries: 2\n      timeout: 60\n\n    task:\n      type: run_function\n      code: \"./script\"\n      entry_script: iris_prediction.py\n      environment:\n        name: \"prs-env\"\n        version: 1\n        image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n        conda_file: ./environment/environment_parallel.yml\n      program_arguments: >-\n        --model ${{inputs.score_model}}\n        --error_threshold 5\n        --allowed_failed_percent 30\n        --task_overhead_timeout 1200\n        --progress_update_timeout 600\n        --first_task_creation_timeout 600\n        --copy_logs_to_parent True\n        --resource_monitor_interva 20\n      append_row_to: ${{outputs.job_output_file}}\n\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-parallel-job-in-pipeline-0.md"
                }
            },
            {
                "page_content": "\n## Use parallel step in pipeline\n\n* SDK v1\n\n    ```python\n    # Run pipeline with parallel run step\n    from azureml.core import Experiment\n    \n    pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n    experiment = Experiment(ws, 'digit_identification')\n    pipeline_run = experiment.submit(pipeline)\n    pipeline_run.wait_for_completion(show_output=True)\n    ```\n\n* SDK v2\n\n    ```python\n    @pipeline()\n    def parallel_in_pipeline(pipeline_job_data_path, pipeline_score_model):\n    \n        prepare_file_tabular_data = prepare_data(input_data=pipeline_job_data_path)\n        # output of file & tabular data should be type MLTable\n        prepare_file_tabular_data.outputs.file_output_data.type = AssetTypes.MLTABLE\n        prepare_file_tabular_data.outputs.tabular_output_data.type = AssetTypes.MLTABLE\n    \n        batch_inference_with_file_data = file_batch_inference(\n            job_data_path=prepare_file_tabular_data.outputs.file_output_data\n        )\n        # use eval_mount mode to handle file data\n        batch_inference_with_file_data.inputs.job_data_path.mode = (\n            InputOutputModes.EVAL_MOUNT\n        )\n        batch_inference_with_file_data.outputs.job_output_path.type = AssetTypes.MLTABLE\n    \n        batch_inference_with_tabular_data = tabular_batch_inference(\n            job_data_path=prepare_file_tabular_data.outputs.tabular_output_data,\n            score_model=pipeline_score_model,\n        )\n        # use direct mode to handle tabular data\n        batch_inference_with_tabular_data.inputs.job_data_path.mode = (\n            InputOutputModes.DIRECT\n        )\n    \n        return {\n            \"pipeline_job_out_file\": batch_inference_with_file_data.outputs.job_output_path,\n            \"pipeline_job_out_tabular\": batch_inference_with_tabular_data.outputs.job_output_path,\n        }\n    \n    pipeline_job_data_path = Input(\n        path=\"./dataset/\", type=AssetTypes.MLTABLE, mode=InputOutputModes.RO_MOUNT\n    )\n    pipeline_score_model = Input(\n        path=\"./model/\", type=AssetTypes.URI_FOLDER, mode=InputOutputModes.DOWNLOAD\n    )\n    # create a pipeline\n    pipeline_job = parallel_in_pipeline(\n        pipeline_job_data_path=pipeline_job_data_path,\n        pipeline_score_model=pipeline_score_model,\n    )\n    pipeline_job.outputs.pipeline_job_out_tabular.type = AssetTypes.URI_FILE\n    \n    # set pipeline level compute\n    pipeline_job.settings.default_compute = \"cpu-cluster\"\n    \n    # run pipeline job\n    pipeline_job = ml_client.jobs.create_or_update(\n        pipeline_job, experiment_name=\"pipeline_samples\"\n    )\n    ```\n    \n## Mapping of key functionality in SDK v1 and SDK v2\n\n|Functionality in SDK v1|Rough mapping in SDK v2|\n|-|-|\n|[azureml.pipeline.steps.parallelrunconfig](/python/api/azureml-pipeline-steps/azureml.pipeline.steps.parallelrunconfig)<br>[azureml.pipeline.steps.parallelrunstep](/python/api/azureml-pipeline-steps/azureml.pipeline.steps.parallelrunstep)|[azure.ai.ml.parallel](/python/api/azure-ai-ml/azure.ai.ml.parallel)|\n|[OutputDatasetConfig](/python/api/azureml-core/azureml.data.output_dataset_config.outputdatasetconfig)|[Output](/python/api/azure-ai-ml/azure.ai.ml.output)|\n|[dataset as_mount](/python/api/azureml-core/azureml.data.filedataset#azureml-data-filedataset-as-mount)|[Input](/python/api/azure-ai-ml/azure.ai.ml.input)|\n\n## Parallel job configurations and settings mapping\n\n| SDK v1| SDK v2| Description |\n|-------|-------|-------------|\n|ParallelRunConfig.environment|parallel_run_function.task.environment|Environment that training job will run in. |\n|ParallelRunConfig.entry_script|parallel_run_function.task.entry_script |User script that will be run in parallel on multiple nodes. |\n|ParallelRunConfig.error_threshold| parallel_run_function.error_threshold |The number of failed mini batches that could be ignored in this parallel job. If the count of failed mini-batch is higher than this threshold, the parallel job will be marked as failed.<br><br>\"-1\" is the default number, which means to ignore all failed mini-batch during parallel job.|\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-execution-parallel-run-step-82.md"
                }
            }
        ]
    },
    {
        "query": "How can I create an online endpoint from an MLFlow model using AzureML CLI V2?",
        "result": "To create an online endpoint from an MLFlow model using AzureML CLI V2, you can use the `az ml endpoint create` command. You will need to specify the name of the endpoint, the model name and version, and the deployment configuration file. The deployment configuration file specifies the authentication mode and other properties of the endpoint. Additionally, you can specify the compute target where the endpoint will be deployed. Once the endpoint is created, you can use the `az ml endpoint show` command to get information about the endpoint, including its URL and authentication keys.",
        "context": [
            {
                "page_content": "\n# Progressive rollout of MLflow models to Online Endpoints\n\nIn this article, you'll learn how you can progressively update and deploy MLflow models to Online Endpoints without causing service disruption. You'll use blue-green deployment, also known as a safe rollout strategy, to introduce a new version of a web service to production. This strategy will allow you to roll out your new version of the web service to a small subset of users or requests before rolling it out completely.\n\n## About this example\n\nOnline Endpoints have the concept of __Endpoint__ and __Deployment__. An endpoint represents the API that customers use to consume the model, while the deployment indicates the specific implementation of that API. This distinction allows users to decouple the API from the implementation and to change the underlying implementation without affecting the consumer. This example will use such concepts to update the deployed model in endpoints without introducing service disruption. \n\nThe model we will deploy is based on the [UCI Heart Disease Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). The database contains 76 attributes, but we are using a subset of 14 of them. The model tries to predict the presence of heart disease in a patient. It is integer valued from 0 (no presence) to 1 (presence). It has been trained using an `XGBBoost` classifier and all the required preprocessing has been packaged as a `scikit-learn` pipeline, making this model an end-to-end pipeline that goes from raw data to predictions.\n\nThe information in this article is based on code samples contained in the [azureml-examples](https://github.com/azure/azureml-examples) repository. To run the commands locally without having to copy/paste files, clone the repo, and then change directories to `sdk/using-mlflow/deploy`.\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in the following notebooks. In the cloned repository, open the notebook: [mlflow_sdk_online_endpoints_progresive.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/using-mlflow/deploy/mlflow_sdk_online_endpoints_progresive.ipynb).\n\n## Prerequisites\n\nBefore following the steps in this article, make sure you have the following prerequisites:\n\n- An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n- Azure role-based access controls (Azure RBAC) are used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned the owner or contributor role for the Azure Machine Learning workspace, or a custom role allowing Microsoft.MachineLearningServices/workspaces/onlineEndpoints/*. For more information, see [Manage access to an Azure Machine Learning workspace](how-to-assign-roles.md).\n\nAdditionally, you will need to:\n\n# [Azure CLI](#tab/cli)\n\n- Install the Azure CLI and the ml extension to the Azure CLI. For more information, see [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n\n# [Python (Azure ML SDK)](#tab/sdk)\n\n- Install the Azure Machine Learning SDK for Python\n    \n    ```bash\n    pip install azure-ai-ml\n    ```\n    \n# [Python (MLflow SDK)](#tab/mlflow)\n\n- Install the Mlflow SDK package `mlflow` and the Azure Machine Learning plug-in for MLflow `azureml-mlflow`.\n\n    ```bash\n    pip install mlflow azureml-mlflow\n    ```\n\n- If you are not running in Azure Machine Learning compute, configure the MLflow tracking URI or MLflow's registry URI to point to the workspace you are working on. See [Configure MLflow for Azure Machine Learning](how-to-use-mlflow-configure-tracking.md) for more details.\n\n\n### Connect to your workspace\n\nFirst, let's connect to Azure Machine Learning workspace where we are going to work on.\n\n# [Azure CLI](#tab/cli)\n\n```azurecli\naz account set --subscription <subscription>\naz configure --defaults workspace=<workspace> group=<resource-group> location=<location>\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-mlflow-models-online-progressive-0.md"
                }
            },
            {
                "page_content": "\n# [Python (MLflow SDK)](#tab/mlflow)\n\n```python\nmodel_name = 'sklearn-diabetes'\nmodel_local_path = \"sklearn-diabetes/model\"\n\nregistered_model = mlflow_client.create_model_version(\n    name=model_name, source=f\"file://{model_local_path}\"\n)\nversion = registered_model.version\n```\n\n# [Studio](#tab/studio)\n\nTo create a model in Azure Machine Learning, open the Models page in Azure Machine Learning. Select **Register model** and select where your model is located. Fill out the required fields, and then select __Register__.\n\n:::image type=\"content\" source=\"./media/how-to-manage-models/register-model-as-asset.png\" alt-text=\"Screenshot of the UI to register a model.\" lightbox=\"./media/how-to-manage-models/register-model-as-asset.png\":::\n\n\nAlternatively, if your model was logged inside of a run, you can register it directly.\n\n> [!TIP]\n> To register the model, you will need to know the location where the model has been stored. If you are using `autolog` feature of MLflow, the path will depend on the type and framework of the model being used. We recommend to check the jobs output to identify which is the name of this folder. You can look for the folder that contains a file named `MLModel`. If you are logging your models manually using `log_model`, then the path is the argument you pass to such method. As an example, if you log the model using `mlflow.sklearn.log_model(my_model, \"classifier\")`, then the path where the model is stored is `classifier`.\n\n# [Azure CLI](#tab/cli)\n\nUse the Azure ML CLI v2 to create a model from a training job output. In the following example, a model named `$MODEL_NAME` is registered using the artifacts of a job with ID `$RUN_ID`. The path where the model is stored is `$MODEL_PATH`.\n\n```bash\naz ml model create --name $MODEL_NAME --path azureml://jobs/$RUN_ID/outputs/artifacts/$MODEL_PATH\n```\n\n> [!NOTE]\n> The path `$MODEL_PATH` is the location where the model has been stored in the run.\n\n# [Python (Azure ML SDK)](#tab/sdk)\n\n```python\nmodel_name = 'sklearn-diabetes'\n\nml_client.models.create_or_update(\n    Model(\n        path=f\"azureml://jobs/{RUN_ID}/outputs/artifacts/{MODEL_PATH}\"\n        name=model_name,\n        type=AssetTypes.MLFLOW_MODEL\n    )\n) \n```\n\n> [!NOTE]\n> The path `MODEL_PATH` is the location where the model has been stored in the run.\n\n# [Python (MLflow SDK)](#tab/mlflow)\n\n```python\nmodel_name = 'sklearn-diabetes'\n\nregistered_model = mlflow_client.create_model_version(\n    name=model_name, source=f\"runs://{RUN_ID}/{MODEL_PATH}\"\n)\nversion = registered_model.version\n```\n\n> [!NOTE]\n> The path `MODEL_PATH` is the location where the model has been stored in the run.\n\n# [Studio](#tab/studio)\n\n:::image type=\"content\" source=\"media/how-to-deploy-mlflow-models-online-endpoints/mlflow-register-model-output.gif\" lightbox=\"media/how-to-deploy-mlflow-models-online-endpoints/mlflow-register-model-output.gif\" alt-text=\"Screenshot showing how to download Outputs and logs from Experimentation run\":::\n\n\n## Deploy an MLflow model to an online endpoint\n\n1. First. we need to configure the endpoint where the model will be deployed. The following example configures the name and authentication mode of the endpoint:\n    \n    # [Azure CLI](#tab/cli)\n    \n    __endpoint.yaml__\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\nname: my-endpoint\nauth_mode: key\n```\n\n    # [Python (Azure ML SDK)](#tab/sdk)\n\n    ```python\n    endpoint_name = \"sklearn-diabetes-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n\n    endpoint = ManagedOnlineEndpoint(\n        name=endpoint_name,\n        description=\"An online endpoint to generate predictions for the diabetes dataset\",\n        auth_mode=\"key\",\n        tags={\"foo\": \"bar\"},\n    )\n    ```\n\n    # [Python (MLflow SDK)](#tab/mlflow)\n\n    We can configure the properties of this endpoint using a configuration file. In this case, we are configuring the authentication mode of the endpoint to be \"key\".\n    \n    ```python\n    endpoint_config = {\n        \"auth_mode\": \"key\",\n        \"identity\": {\n            \"type\": \"system_assigned\"\n        }\n    }\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-mlflow-models-online-endpoints-157.md"
                }
            },
            {
                "page_content": "\n# Deploy MLflow models to online endpoints\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n> [!div class=\"op_single_selector\" title1=\"Select the version of Azure Machine Learning CLI extension you are using:\"]\n> * [v1](./v1/how-to-deploy-mlflow-models.md)\n> * [v2 (current version)](how-to-deploy-mlflow-models-online-endpoints.md)\n\nIn this article, learn how to deploy your [MLflow](https://www.mlflow.org) model to an [online endpoint](concept-endpoints.md) for real-time inference. When you deploy your MLflow model to an online endpoint, you don't need to indicate a scoring script or an environment. This characteristic is usually referred as __no-code deployment__. \n\nFor no-code-deployment, Azure Machine Learning \n\n* Dynamically installs Python packages provided in the `conda.yaml` file, this means the dependencies are installed during container runtime.\n* Provides a MLflow base image/curated environment that contains the following items:\n    * [`azureml-inference-server-http`](how-to-inference-server-http.md) \n    * [`mlflow-skinny`](https://github.com/mlflow/mlflow/blob/master/README_SKINNY.rst)\n    * A scoring script to perform inference.\n\n> [!WARNING]\n> __Workspaces without public network access:__ Azure Machine Learning performs dynamic installation of packages when deploying MLflow models with no-code deployment. As a consequence, deploying MLflow models to online endpoints with no-code deployment in a private network without egress connectivity is not supported by the moment. If that's your case, either enable egress connectivity or indicate the environment to use in the deployment as explained in [Customizing MLflow model deployments](#customizing-mlflow-model-deployments).\n\n\n## About this example\n\nThis example shows how you can deploy an MLflow model to an online endpoint to perform predictions. This example uses an MLflow model based on the [Diabetes dataset](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html). This dataset contains ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements obtained from n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline (regression).\n\nThe model has been trained using an `scikit-learn` regressor and all the required preprocessing has been packaged as a pipeline, making this model an end-to-end pipeline that goes from raw data to predictions.\n\nThe information in this article is based on code samples contained in the [azureml-examples](https://github.com/azure/azureml-examples) repository. To run the commands locally without having to copy/paste YAML and other files, clone the repo and then change directories to the `cli/endpoints/online` if you are using the Azure CLI or `sdk/endpoints/online` if you are using our SDK for Python.\n\n```azurecli\ngit clone https://github.com/Azure/azureml-examples --depth 1\ncd azureml-examples/cli/endpoints/online\n```\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in the following notebooks. In the cloned repository, open the notebook: [mlflow_sdk_online_endpoints_progresive.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/using-mlflow/deploy/mlflow_sdk_online_endpoints.ipynb).\n\n## Prerequisites\n\nBefore following the steps in this article, make sure you have the following prerequisites:\n\n- An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n- Azure role-based access controls (Azure RBAC) are used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned the owner or contributor role for the Azure Machine Learning workspace, or a custom role allowing Microsoft.MachineLearningServices/workspaces/onlineEndpoints/*. For more information, see [Manage access to an Azure Machine Learning workspace](how-to-assign-roles.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-mlflow-models-online-endpoints-0.md"
                }
            }
        ]
    },
    {
        "query": "How can I create a compute instance with auto-shutdown on it using CLI V2 ?",
        "result": "You can create a compute instance with auto-shutdown using CLI V2 by adding the `idle_time_before_shutdown_minutes` parameter to the YAML definition. For example, to create a compute instance with auto-shutdown set to 30 minutes, you can use the following YAML definition:\n\n```YAML\n$schema: https://azuremlschemas.azureedge.net/latest/computeInstance.schema.json \nname: my-ci\ntype: computeinstance\nsize: STANDARD_DS3_v2\nidle_time_before_shutdown_minutes: 30\n```\n\nThen, run the following command to create the compute instance:\n\n```azurecli\naz ml compute create -f my-ci.yml\n```",
        "context": [
            {
                "page_content": "Activity on custom applications installed on the compute instance isn't considered. There are also some basic bounds around inactivity time periods; compute instance must be inactive for a minimum of 15 mins and a maximum of three days. \n\nAlso, if a compute instance has already been idle for a certain amount of time, if idle shutdown settings are updated to  an amount of time shorter than the current idle duration, the idle time clock will be reset to 0. For example, if the compute instance has already been idle for 20 minutes, and the shutdown settings are updated to 15 minutes, the idle time clock will be reset to 0.\n\nUse **Manage preview features** to access this feature.\n\n1. In the workspace toolbar, select the **Manage preview features** image.\n1. Scroll down until you see **Configure auto-shutdown for idle compute instances**.\n1. Toggle the switch to enable the feature.\n\n:::image type=\"content\" source=\"media/how-to-create-manage-compute-instance/enable-preview.png\" alt-text=\"Screenshot: Enable auto-shutdown.\":::\n\nOnce enabled, the setting can be configured during compute instance creation or for existing compute instances via the following interfaces:\n\n# [Python SDK](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nWhen creating a new compute instance, add the `idle_time_before_shutdown_minutes` parameter.\n\n```Python\n# Note that idle_time_before_shutdown has been deprecated.\nComputeInstance(name=ci_basic_name, size=\"STANDARD_DS3_v2\", idle_time_before_shutdown_minutes=\"30\")\n```\n\nYou cannot change the idle time of an existing compute instance with the Python SDK.\n\n# [Azure CLI](#tab/azure-cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nWhen creating a new compute instance, add `idle_time_before_shutdown_minutes` to the YAML definition.\n\n```YAML\n# Note that this is just a snippet for the idle shutdown property. Refer to the \"Create\" Azure CLI section for more information.\n# Note that idle_time_before_shutdown has been deprecated.\nidle_time_before_shutdown_minutes: 30\n```\n\nYou cannot change the idle time of an existing compute instance with the CLI.\n\n# [Studio](#tab/azure-studio)\n\n* When creating a new compute instance:\n\n    1. Select **Advanced** after completing required settings.  \n    1. Select **Enable idle shutdown**\n\n        :::image type=\"content\" source=\"media/how-to-create-manage-compute-instance/enable-idle-shutdown.png\" alt-text=\"Screenshot: Enable compute instance idle shutdown.\" lightbox=\"media/how-to-create-manage-compute-instance/enable-idle-shutdown.png\":::\n\n    1. Specify the shutdown period.\n\n* For an existing compute instance:\n\n    1. In the left navigation bar, select **Compute**\n    1. In the list, select the compute instance you wish to change\n    1. Select the **Edit** pencil in the **Schedules** section.\n\n        :::image type=\"content\" source=\"media/how-to-create-manage-compute-instance/edit-idle-time.png\" alt-text=\"Screenshot: Edit idle time for a compute instance.\" lightbox=\"media/how-to-create-manage-compute-instance/edit-idle-time.png\":::\n\n\nYou can also change the idle time using:\n\n* REST API\n\n    Endpoint:\n    ```\n    POST https://management.azure.com/subscriptions/{SUB_ID}/resourceGroups/{RG_NAME}/providers/Microsoft.MachineLearningServices/workspaces/{WS_NAME}/computes/{CI_NAME}/updateIdleShutdownSetting?api-version=2021-07-01\n    ```\n\n    Body:\n    ```JSON\n    {\n        \"idleTimeBeforeShutdown\": \"PT30M\" // this must be a string in ISO 8601 format\n    }\n    ```\n\n\n* ARM Templates: only configurable during new compute instance creation\n\n    ```JSON\n    // Note that this is just a snippet for the idle shutdown property in an ARM template\n    {\n        \"idleTimeBeforeShutdown\":\"PT30M\" // this must be a string in ISO 8601 format\n    }\n    ```\n\n### Azure policy support\nAdministrators can use a built-in [Azure Policy](./../governance/policy/overview.md) definition to enforce auto-stop on all compute instances in a given subscription/resource-group. \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-185.md"
                }
            },
            {
                "page_content": "Administrators can use a built-in [Azure Policy](./../governance/policy/overview.md) definition to enforce auto-stop on all compute instances in a given subscription/resource-group. \n\n1. Navigate to Azure Policy in the Azure portal.\n2. Under \"Definitions\", look for the idle shutdown policy.\n\n      :::image type=\"content\" source=\"media/how-to-create-attach-studio/idle-shutdown-policy.png\" alt-text=\"Screenshot for the idle shutdown policy in Azure portal.\" lightbox=\"media/how-to-create-attach-studio/idle-shutdown-policy.png\":::\n\n3. Assign policy to the necessary scope.\n\nYou can also create your own custom Azure policy. For example, if the below policy is assigned, all new compute instances will have auto-stop configured with a 60-minute inactivity period. \n\n```json\n{\n  \"mode\": \"All\",\n  \"policyRule\": {\n    \"if\": {\n      \"allOf\": [\n        {\n          \"field\": \"type\",\n          \"equals\": \"Microsoft.MachineLearningServices/workspaces/computes\"\n        },\n        {\n          \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/computeType\",\n          \"equals\": \"ComputeInstance\"\n        },\n        {\n          \"anyOf\": [\n            {\n              \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/idleTimeBeforeShutdown\",\n              \"exists\": false\n            },\n            {\n              \"value\": \"[empty(field('Microsoft.MachineLearningServices/workspaces/computes/idleTimeBeforeShutdown'))]\",\n              \"equals\": true\n            }\n          ]\n        }\n      ]\n    },\n    \"then\": {\n      \"effect\": \"append\",\n      \"details\": [\n        {\n          \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/idleTimeBeforeShutdown\",\n          \"value\": \"PT60M\"\n        }\n      ]\n    }\n  },\n  \"parameters\": {}\n}\n```\n\n## Schedule automatic start and stop\n\nDefine multiple schedules for auto-shutdown and auto-start. For instance, create a schedule to start at 9 AM and stop at 6 PM from Monday-Thursday, and a second schedule to start at 9 AM and stop at 4 PM for Friday.  You can create a total of four schedules per compute instance.\n\nSchedules can also be defined for [create on behalf of](#create-on-behalf-of-preview) compute instances. You can create a schedule that creates the compute instance in a stopped state. Stopped compute instances are useful when you create a compute instance on behalf of another user.\n\nPrior to a scheduled shutdown, users will see a notification alerting them that the Compute Instance is about to shut down. At that point, the user can choose to dismiss the upcoming shutdown event, if for example they are in the middle of using their Compute Instance.\n\n### Create a schedule in studio\n\n1. [Fill out the form](?tabs=azure-studio#create).\n1. On the second page of the form, open **Show advanced settings**.\n1. Select **Add schedule** to add a new schedule.\n\n    :::image type=\"content\" source=\"media/how-to-create-attach-studio/create-schedule.png\" alt-text=\"Screenshot: Add schedule in advanced settings.\":::\n\n1. Select **Start compute instance** or **Stop compute instance**.\n1. Select the **Time zone**.\n1. Select the **Startup time** or **Shutdown time**.\n1. Select the days when this schedule is active.\n\n    :::image type=\"content\" source=\"media/how-to-create-attach-studio/stop-compute-schedule.png\" alt-text=\"Screenshot: schedule a compute instance to shut down.\":::\n\n1. Select **Add schedule** again if you want to create another schedule.\n\nOnce the compute instance is created, you can view, edit, or add new schedules from the compute instance details section.\n\n\n> [!NOTE]\n> Timezone labels don't account for day light savings. For instance,  (UTC+01:00) Amsterdam, Berlin, Bern, Rome, Stockholm, Vienna is actually UTC+02:00 during day light savings.\n\n### Create a schedule with CLI\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```azurecli\naz ml compute create -f create-instance.yml\n```\n\nWhere the file *create-instance.yml* is:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/computeInstance.schema.json \nname: schedule-example-i\ntype: computeinstance\nsize: STANDARD_DS3_v2\nschedules:\n   compute_start_stop:\n      - action: stop\n        trigger:\n         type: cron\n         start_time: \"2021-03-10T21:21:07\"\n         time_zone: Pacific Standard Time\n         expression: 0 18 * * *\n         \n\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-273.md"
                }
            },
            {
                "page_content": "> The compute instance has 120GB OS disk. If you run out of disk space and get into an unusable state, please clear at least 5 GB disk space on OS disk (mounted on /) through the compute instance terminal by removing files/folders and then do `sudo reboot`. Temporary disk will be freed after restart; you do not need to clear space on temp disk manually. To access the terminal go to compute list page or compute instance details page and click on **Terminal** link. You can check available disk space by running `df -h` on the terminal. Clear at least 5 GB space before doing `sudo reboot`. Please do not stop or restart the compute instance through the Studio until 5 GB disk space has been cleared. Auto shutdowns, including scheduled start or stop as well as idle shutdowns(preview), will not work if the CI disk is full.\n\n## Next steps\n\n* [Quickstart: Create workspace resources you need to get started with Azure Machine Learning](quickstart-create-resources.md).\n* [Tutorial: Train your first ML model](tutorial-1st-experiment-sdk-train.md) shows how to use a compute instance with an integrated notebook.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-compute-instance-125.md"
                }
            }
        ]
    },
    {
        "query": "can you give me a sample code I can use to test my online endpoint deployment in AzureML ?",
        "result": "Sure, here is a sample code you can use to test your online endpoint deployment in AzureML:\n\n```python\nfrom azureml.core import Workspace\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import Model\nfrom azureml.core.authentication import InteractiveLoginAuthentication\n\n# Define the workspace and authentication\ninteractive_auth = InteractiveLoginAuthentication(tenant_id=\"your_tenant_id\")\nws = Workspace.from_config(auth=interactive_auth)\n\n# Define the deployed service\nservice_name = \"your_service_name\"\nservice = AciWebservice(workspace=ws, name=service_name)\n\n# Define the input data\ninput_data = {\n    \"data\": [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8]\n    ]\n}\n\n# Call the deployed service\noutput = service.run(input_data=input_data)\n\n# Print the output\nprint(output)\n```\n\nReplace `your_tenant_id` with your Azure tenant ID, `your_service_name` with the name of your deployed service, and `input_data` with your own input data.",
        "context": [
            {
                "page_content": "Though `green` has 0% of traffic allocated, you can still invoke the endpoint and deployment with the [json](https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/online/model-2/sample-request.json) file.\n\n```python\nml_client.online_endpoints.invoke(\n    endpoint_name=online_endpoint_name,\n    deployment_name=\"green\",\n    request_file=\"../model-2/sample-request.json\",\n)\n```\n\n\n## Test the deployment with mirrored traffic (preview)\n[!INCLUDE [preview disclaimer](../../includes/machine-learning-preview-generic-disclaimer.md)]\n\nOnce you've tested your `green` deployment, you can 'mirror' (or copy) a percentage of the live traffic to it. Mirroring traffic (also called shadowing) doesn't change the results returned to clients. Requests still flow 100% to the `blue` deployment. The mirrored percentage of the traffic is copied and submitted to the `green` deployment so you can gather metrics and logging without impacting your clients. Mirroring is useful when you want to validate a new deployment without impacting clients; for example, to check if latency is within acceptable bounds and that there are no HTTP errors. Testing the new deployment with traffic mirroring/shadowing is also known as [shadow testing](https://microsoft.github.io/code-with-engineering-playbook/automated-testing/shadow-testing/). The deployment receiving the mirrored traffic (in this case, the `green` deployment) can also be called the shadow deployment.\n\n> [!WARNING]\n> Mirroring traffic uses your [endpoint bandwidth quota](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints) (default 5 MBPS). Your endpoint bandwidth will be throttled if you exceed the allocated quota. For information on monitoring bandwidth throttling, see [Monitor managed online endpoints](how-to-monitor-online-endpoints.md#metrics-at-endpoint-scope).\n\n> [!IMPORTANT]\n> Mirrored traffic is supported for the CLI (v2) (version 2.4.0 or above) and Python SDK (v2) (version 1.0.0 or above). If you update the endpoint using an older version of CLI/SDK or Studio UI, the setting for mirrored traffic will be removed.\n\n# [Azure CLI](#tab/azure-cli)\n\nThe following command mirrors 10% of the traffic to the `green` deployment:\n\n```azurecli\nset -e\n\n# <set_endpoint_name>\nexport ENDPOINT_NAME=\"<YOUR_ENDPOINT_NAME>\"\n# </set_endpoint_name>\n\n#  endpoint name\nexport ENDPOINT_NAME=endpt-sr-`echo $RANDOM`\n\n# <create_endpoint>\naz ml online-endpoint create --name $ENDPOINT_NAME -f endpoints/online/managed/sample/endpoint.yml\n# </create_endpoint>\n\n# <create_blue>\naz ml online-deployment create --name blue --endpoint-name $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment.yml --all-traffic\n# </create_blue>\n\n# <test_blue>\naz ml online-endpoint invoke --name $ENDPOINT_NAME --request-file endpoints/online/model-1/sample-request.json\n# </test_blue>\n\n# <scale_blue>\naz ml online-deployment update --name blue --endpoint-name $ENDPOINT_NAME --set instance_count=2\n# </scale_blue>\n\n# <create_green>\naz ml online-deployment create --name green --endpoint-name $ENDPOINT_NAME -f endpoints/online/managed/sample/green-deployment.yml\n# </create_green>\n\n# <get_traffic>\naz ml online-endpoint show -n $ENDPOINT_NAME --query traffic\n# </get_traffic>\n\n# <test_green>\naz ml online-endpoint invoke --name $ENDPOINT_NAME --deployment-name green --request-file endpoints/online/model-2/sample-request.json\n# </test_green>\n\n# supress printing secret\nset +x\n\n# <test_green_using_curl_get_key>\n#get the key\nENDPOINT_KEY=$(az ml online-endpoint get-credentials -n $ENDPOINT_NAME -o tsv --query primaryKey)\n# </test_green_using_curl_get_key>\n\nset -x\n\n# <test_green_using_curl>\n# get the scoring uri\nSCORING_URI=$(az ml online-endpoint show -n $ENDPOINT_NAME -o tsv --query scoring_uri)\n# use curl to invoke the endpoint\ncurl --request POST \"$SCORING_URI\" --header \"Authorization: Bearer $ENDPOINT_KEY\" --header 'Content-Type: application/json' --header \"azureml-model-deployment: green\" --data @endpoints/online/model-2/sample-request.json\n# </test_green_using_curl>\n\n# <test_green_with_mirror_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --mirror-traffic \"green=10\"\n# </test_green_with_mirror_traffic>\n\n# <reset_mirror_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --mirror-traffic \"green=0\"\n# </reset_mirror_traffic>\n\n# <green_10pct_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --traffic \"blue=90 green=10\"\n# </green_10pct_traffic>\n\n# <green_100pct_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --traffic \"blue=0 green=100\"\n# </green_100pct_traffic>\n\n# <delete_blue>\naz ml online-deployment delete --name blue --endpoint $ENDPOINT_NAME --yes --no-wait\n# </delete_blue>\n\n# <delete_endpoint>\naz ml online-endpoint delete --name $ENDPOINT_NAME --yes --no-wait\n# </delete_endpoint>\n\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-safely-rollout-online-endpoints-1113.md"
                }
            },
            {
                "page_content": "\n## Deploy the model as an online endpoint\n\nYou can now deploy your model as an [online endpoint](concept-endpoints.md)\u2014that is, as a web service in the Azure cloud.\n\nTo deploy a machine learning service, you'll typically need:\n- The model assets that you want to deploy. These assets include the model's file and metadata that you already registered in your training job.\n- Some code to run as a service. The code executes the model on a given input request (an entry script). This entry script receives data submitted to a deployed web service and passes it to the model. After the model processes the data, the script returns the model's response to the client. The script is specific to your model and must understand the data that the model expects and returns. When you use an MLFlow model, AzureML automatically creates this script for you.\n\nFor more information about deployment, see [Deploy and score a machine learning model with managed online endpoint using Python SDK v2](how-to-deploy-managed-online-endpoint-sdk-v2.md).\n\n### Create a new online endpoint\n\nAs a first step to deploying your model, you need to create your online endpoint. The endpoint name must be unique in the entire Azure region. For this article, you'll create a unique name using a universally unique identifier (UUID).\n\n```python\nimport uuid\n\n# Creating a unique name for the endpoint\nonline_endpoint_name = \"aci-birds-endpoint-\" + str(uuid.uuid4())[:8]\n```\n\n```python\nfrom azure.ai.ml.entities import ManagedOnlineEndpoint\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"Classify turkey/chickens using transfer learning with PyTorch\",\n    auth_mode=\"key\",\n    tags={\"data\": \"birds\", \"method\": \"transfer learning\", \"framework\": \"pytorch\"},\n)\n\nendpoint = ml_client.begin_create_or_update(endpoint).result()\n\nprint(f\"Endpoint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")\n```\n\nOnce you've created the endpoint, you can retrieve it as follows:\n\n```python\nendpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n\nprint(\n    f'Endpint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n)\n```\n\n### Deploy the model to the endpoint\n\nAfter you've created the endpoint, you can deploy the model with the entry script. An endpoint can have multiple deployments. Using rules, the endpoint can then direct traffic to these deployments.\n\nIn the following code, you'll create a single deployment that handles 100% of the incoming traffic. We've specified an arbitrary color name (*aci-blue*) for the deployment. You could also use any other name such as *aci-green* or *aci-red* for the deployment.\nThe code to deploy the model to the endpoint does the following:\n\n- deploys the best version of the model that you registered earlier;\n- scores the model, using the `score.py` file; and\n- uses the curated environment (that you specified earlier) to perform inferencing.\n\n```python\nfrom azure.ai.ml.entities import (\n    ManagedOnlineDeployment,\n    Model,\n    Environment,\n    CodeConfiguration,\n)\n\nonline_deployment_name = \"aci-blue\"\n\n# create an online deployment.\nblue_deployment = ManagedOnlineDeployment(\n    name=online_deployment_name,\n    endpoint_name=online_endpoint_name,\n    model=model,\n    environment=curated_env_name,\n    code_configuration=CodeConfiguration(code=\"./score/\", scoring_script=\"score.py\"),\n    instance_type=\"Standard_NC6s_v3\",\n    instance_count=1,\n)\n\nblue_deployment = ml_client.begin_create_or_update(blue_deployment).result()\n```\n\n> [!NOTE]\n> Expect this deployment to take a bit of time to finish.\n\n### Test the deployed model\n\nNow that you've deployed the model to the endpoint, you can predict the output of the deployed model, using the `invoke` method on the endpoint.\n\nTo test the endpoint, let's use a sample image for prediction. First, let's display the image.\n\n```python\n# install pillow if PIL cannot imported\n%pip install pillow\nimport json\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.imshow(Image.open(\"test_img.jpg\"))\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-train-pytorch-295.md"
                }
            },
            {
                "page_content": "# [Azure CLI](#tab/azure-cli)\n\nYou can view the status of your existing endpoint and deployment by running:\n\n```azurecli\naz ml online-endpoint show --name $ENDPOINT_NAME \n\naz ml online-deployment show --name blue --endpoint $ENDPOINT_NAME \n```\n\nYou should see the endpoint identified by `$ENDPOINT_NAME` and, a deployment called `blue`.\n\n### Test the endpoint with sample data\n\nThe endpoint can be invoked using the `invoke` command. We'll send a sample request using a [json](https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/online/model-1/sample-request.json) file.\n\n```azurecli\nset -e\n\n# <set_endpoint_name>\nexport ENDPOINT_NAME=\"<YOUR_ENDPOINT_NAME>\"\n# </set_endpoint_name>\n\n#  endpoint name\nexport ENDPOINT_NAME=endpt-sr-`echo $RANDOM`\n\n# <create_endpoint>\naz ml online-endpoint create --name $ENDPOINT_NAME -f endpoints/online/managed/sample/endpoint.yml\n# </create_endpoint>\n\n# <create_blue>\naz ml online-deployment create --name blue --endpoint-name $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment.yml --all-traffic\n# </create_blue>\n\n# <test_blue>\naz ml online-endpoint invoke --name $ENDPOINT_NAME --request-file endpoints/online/model-1/sample-request.json\n# </test_blue>\n\n# <scale_blue>\naz ml online-deployment update --name blue --endpoint-name $ENDPOINT_NAME --set instance_count=2\n# </scale_blue>\n\n# <create_green>\naz ml online-deployment create --name green --endpoint-name $ENDPOINT_NAME -f endpoints/online/managed/sample/green-deployment.yml\n# </create_green>\n\n# <get_traffic>\naz ml online-endpoint show -n $ENDPOINT_NAME --query traffic\n# </get_traffic>\n\n# <test_green>\naz ml online-endpoint invoke --name $ENDPOINT_NAME --deployment-name green --request-file endpoints/online/model-2/sample-request.json\n# </test_green>\n\n# supress printing secret\nset +x\n\n# <test_green_using_curl_get_key>\n#get the key\nENDPOINT_KEY=$(az ml online-endpoint get-credentials -n $ENDPOINT_NAME -o tsv --query primaryKey)\n# </test_green_using_curl_get_key>\n\nset -x\n\n# <test_green_using_curl>\n# get the scoring uri\nSCORING_URI=$(az ml online-endpoint show -n $ENDPOINT_NAME -o tsv --query scoring_uri)\n# use curl to invoke the endpoint\ncurl --request POST \"$SCORING_URI\" --header \"Authorization: Bearer $ENDPOINT_KEY\" --header 'Content-Type: application/json' --header \"azureml-model-deployment: green\" --data @endpoints/online/model-2/sample-request.json\n# </test_green_using_curl>\n\n# <test_green_with_mirror_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --mirror-traffic \"green=10\"\n# </test_green_with_mirror_traffic>\n\n# <reset_mirror_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --mirror-traffic \"green=0\"\n# </reset_mirror_traffic>\n\n# <green_10pct_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --traffic \"blue=90 green=10\"\n# </green_10pct_traffic>\n\n# <green_100pct_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --traffic \"blue=0 green=100\"\n# </green_100pct_traffic>\n\n# <delete_blue>\naz ml online-deployment delete --name blue --endpoint $ENDPOINT_NAME --yes --no-wait\n# </delete_blue>\n\n# <delete_endpoint>\naz ml online-endpoint delete --name $ENDPOINT_NAME --yes --no-wait\n# </delete_endpoint>\n\n```\n\n# [Python](#tab/python)\n\nCheck the status to see whether the model was deployed without error:\n\n```python\nml_client.online_endpoints.get(name=online_endpoint_name)\n```\n\n### Test the endpoint with sample data\n\nUsing the `MLClient` created earlier, we'll get a handle to the endpoint. The endpoint can be invoked using the `invoke` command with the following parameters:\n\n* `endpoint_name` - Name of the endpoint\n* `request_file` - File with request data\n* `deployment_name` - Name of the specific deployment to test in an endpoint\n\nWe'll send a sample request using a [json](https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/online/model-1/sample-request.json) file.\n\n```python\n# test the blue deployment with some sample data\nml_client.online_endpoints.invoke(\n    endpoint_name=online_endpoint_name,\n    deployment_name=\"blue\",\n    request_file=\"../model-1/sample-request.json\",\n)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-safely-rollout-online-endpoints-500.md"
                }
            }
        ]
    },
    {
        "query": "How can I create a sweep job using AzureML SDK V2 ?",
        "result": "To create a sweep job using AzureML SDK V2, you can use the `sweep` method of a command job. First, define a command job that takes hyperparameters as inputs. Then, call the `sweep` method on the command job to sweep over the hyperparameters based on the search space and terminate policy you defined. Finally, set the limits for the sweep and submit the job. Here is an example code snippet:\n\n```python\nfrom azure.ai.ml import command, Input\nfrom azure.ai.ml.sweep import Choice, Uniform, MedianStoppingPolicy\n\ncommand_job = command(\n    code=\"./src\",\n    command=\"python main.py --iris-csv ${{inputs.iris_csv}} --learning-rate ${{inputs.learning_rate}} --boosting ${{inputs.boosting}}\",\n    environment=\"AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu@latest\",\n    inputs={\n        \"iris_csv\": Input(\n            type=\"uri_file\",\n            path=\"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\",\n        ),\n        \"learning_rate\": Uniform(min_value=0.01, max_value=0.9),\n        \"boosting\": Choice(values=[\"gbdt\", \"dart\"]),\n    },\n    compute=\"cpu-cluster\",\n)\n\nsweep_job = command_job.sweep(\n    compute=\"cpu-cluster\", \n    sampling_algorithm=\"random\",\n    primary_metric=\"test-multi_logloss\",\n    goal=\"Minimize\",\n)\n\nsweep_job.set_limits(max_total_trials=20, max_concurrent_trials=10, timeout=7200)\nsweep_job.early_termination = MedianStoppingPolicy(delay_evaluation=5, evaluation_interval=2)\n\nreturned_sweep_job = ml_client.create_or_update(sweep_job)\n```",
        "context": [
            {
                "page_content": "\n### Python SDK\n\nThe Python SDK example can be found in [azureml-example repo](https://github.com/Azure/azureml-examples). Navigate to *azureml-examples/sdk/jobs/pipelines/1c_pipeline_with_hyperparameter_sweep* to check the example.\n\nIn Azure Machine Learning Python SDK v2, you can enable hyperparameter tuning for any command component by calling `.sweep()` method.\n\nBelow code snippet shows how to enable sweep for `train_model`.\n\n```python\ntrain_component_func = load_component(source=\"./train.yml\")\nscore_component_func = load_component(source=\"./predict.yml\")\n\n# define a pipeline\n@pipeline()\ndef pipeline_with_hyperparameter_sweep():\n    \"\"\"Tune hyperparameters using sample components.\"\"\"\n    train_model = train_component_func(\n        data=Input(\n            type=\"uri_file\",\n            path=\"wasbs://datasets@azuremlexamples.blob.core.windows.net/iris.csv\",\n        ),\n        c_value=Uniform(min_value=0.5, max_value=0.9),\n        kernel=Choice([\"rbf\", \"linear\", \"poly\"]),\n        coef0=Uniform(min_value=0.1, max_value=1),\n        degree=3,\n        gamma=\"scale\",\n        shrinking=False,\n        probability=False,\n        tol=0.001,\n        cache_size=1024,\n        verbose=False,\n        max_iter=-1,\n        decision_function_shape=\"ovr\",\n        break_ties=False,\n        random_state=42,\n    )\n    sweep_step = train_model.sweep(\n        primary_metric=\"training_f1_score\",\n        goal=\"minimize\",\n        sampling_algorithm=\"random\",\n        compute=\"cpu-cluster\",\n    )\n    sweep_step.set_limits(max_total_trials=20, max_concurrent_trials=10, timeout=7200)\n\n    score_data = score_component_func(\n        model=sweep_step.outputs.model_output, test_data=sweep_step.outputs.test_data\n    )\n\n\npipeline_job = pipeline_with_hyperparameter_sweep()\n\n# set pipeline level compute\npipeline_job.settings.default_compute = \"cpu-cluster\"\n```\n\n We first load `train_component_func` defined in `train.yml` file. When creating `train_model`, we add `c_value`, `kernel` and `coef0` into search space(line 15-17). Line 30-35 defines the primary metric, sampling algorithm etc.\n\n## Check pipeline job with sweep step in Studio\n\nAfter you submit a pipeline job, the SDK or CLI widget will give you a web URL link to Studio UI. The link will guide you to the pipeline graph view by default.\n\nTo check details of the sweep step, double click the sweep step and navigate to the **child job** tab in the panel on the right.\n\n:::image type=\"content\" source=\"./media/how-to-use-sweep-in-pipeline/pipeline-view.png\" alt-text=\"Screenshot of the pipeline with child job and the train_model node highlighted.\" lightbox= \"./media/how-to-use-sweep-in-pipeline/pipeline-view.png\":::\n\nThis will link you to the sweep job page as seen in the below screenshot. Navigate to **child job** tab, here you can see the metrics of all child jobs and list of all child jobs.\n\n:::image type=\"content\" source=\"./media/how-to-use-sweep-in-pipeline/sweep-job.png\" alt-text=\"Screenshot of the job page on the child jobs tab.\" lightbox= \"./media/how-to-use-sweep-in-pipeline/sweep-job.png\":::\n\nIf a child jobs failed, select the name of that child job to enter detail page of that specific child job (see screenshot below). The useful debug information is under **Outputs + Logs**.\n\n:::image type=\"content\" source=\"./media/how-to-use-sweep-in-pipeline/child-run.png\" alt-text=\"Screenshot of the output + logs tab of a child run.\" lightbox= \"./media/how-to-use-sweep-in-pipeline/child-run.png\":::\n\n## Sample notebooks\n\n- [Build pipeline with sweep node](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/1c_pipeline_with_hyperparameter_sweep/pipeline_with_hyperparameter_sweep.ipynb)\n- [Run hyperparameter sweep on a command job](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/single-step/lightgbm/iris/lightgbm-iris-sweep.ipynb)\n\n## Next steps\n\n- [Track an experiment](how-to-log-view-metrics.md)\n- [Deploy a trained model](how-to-deploy-online-endpoints.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-sweep-in-pipeline-305.md"
                }
            },
            {
                "page_content": "\n# How to do hyperparameter tuning in pipeline (v2)\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\nIn this article, you'll learn how to do hyperparameter tuning in Azure Machine Learning pipeline.\n\n## Prerequisite\n\n1. Understand what is [hyperparameter tuning](how-to-tune-hyperparameters.md) and how to do hyperparameter tuning in Azure Machine Learning use SweepJob.\n2. Understand what is a [Azure Machine Learning pipeline](concept-ml-pipelines.md)\n3. Build a command component that takes hyperparameter as input.\n\n## How to do hyperparameter tuning in Azure Machine Learning pipeline\n\nThis section explains how to do hyperparameter tuning in Azure Machine Learning pipeline using CLI v2 and Python SDK. Both approaches share the same prerequisite: you already have a command component created and the command component takes hyperparameters as inputs. If you don't have a command component yet. Follow below links to create a command component first.\n\n- [AzureML CLI v2](how-to-create-component-pipelines-cli.md)\n- [AzureML Python SDK v2](how-to-create-component-pipeline-python.md)\n\n### CLI v2\n\nThe example used in this article can be found in [azureml-example repo](https://github.com/Azure/azureml-examples). Navigate to *[azureml-examples/cli/jobs/pipelines-with-components/pipeline_with_hyperparameter_sweep* to check the example.\n\nAssume you already have a command component defined in `train.yaml`. A two-step pipeline job (train and predict) YAML file looks like below.\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\ndisplay_name: pipeline_with_hyperparameter_sweep\ndescription: Tune hyperparameters using TF component\nsettings:\n    default_compute: azureml:cpu-cluster\njobs:\n  sweep_step:\n    type: sweep\n    inputs:\n      data: \n        type: uri_file\n        path: wasbs://datasets@azuremlexamples.blob.core.windows.net/iris.csv\n      degree: 3\n      gamma: \"scale\"\n      shrinking: False\n      probability: False\n      tol: 0.001\n      cache_size: 1024\n      verbose: False\n      max_iter: -1\n      decision_function_shape: \"ovr\"\n      break_ties: False\n      random_state: 42\n    outputs:\n      model_output:\n      test_data:\n    sampling_algorithm: random\n    trial: ./train.yml\n    search_space:\n      c_value:\n        type: uniform\n        min_value: 0.5\n        max_value: 0.9\n      kernel:\n        type: choice\n        values: [\"rbf\", \"linear\", \"poly\"]\n      coef0:\n        type: uniform\n        min_value: 0.1\n        max_value: 1\n    objective:\n      goal: minimize\n      primary_metric: training_f1_score\n    limits:\n      max_total_trials: 5\n      max_concurrent_trials: 3\n      timeout: 7200\n\n  predict_step:\n    type: command\n    inputs:\n      model: ${{parent.jobs.sweep_step.outputs.model_output}}\n      test_data: ${{parent.jobs.sweep_step.outputs.test_data}}\n    outputs:\n      predict_result:\n    component: ./predict.yml\n\n    \n```\n\nThe `sweep_step` is the step for hyperparameter tuning. Its type needs to be `sweep`.  And `trial` refers to the command component defined in `train.yaml`. From the `search space` field we can see three hyparmeters (`c_value`, `kernel`, and `coef`) are added to the search space. After you submit this pipeline job, Azure Machine Learning will run the trial component multiple times to sweep over hyperparameters based on the search space and terminate policy you defined in `sweep_step`. Check [sweep job YAML schema](reference-yaml-job-sweep.md) for full schema of sweep job.\n\nBelow is the trial component definition (train.yml file). \n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\ntype: command\n\nname: train_model\ndisplay_name: train_model\nversion: 1\n\ninputs: \n  data:\n    type: uri_folder\n  c_value:\n    type: number\n    default: 1.0\n  kernel:\n    type: string\n    default: rbf\n  degree:\n    type: integer\n    default: 3\n  gamma:\n    type: string\n    default: scale\n  coef0: \n    type: number\n    default: 0\n  shrinking:\n    type: boolean\n    default: false\n  probability:\n    type: boolean\n    default: false\n  tol:\n    type: number\n    default: 1e-3\n  cache_size:\n    type: number\n    default: 1024\n  verbose:\n    type: boolean\n    default: false\n  max_iter:\n    type: integer\n    default: -1\n  decision_function_shape:\n    type: string\n    default: ovr\n  break_ties:\n    type: boolean\n    default: false\n  random_state:\n    type: integer\n    default: 42\n\noutputs:\n  model_output:\n    type: mlflow_model\n  test_data:\n    type: uri_folder\n  \ncode: ./train-src\n\nenvironment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\n\ncommand: >-\n  python train.py \n  --data ${{inputs.data}}\n  --C ${{inputs.c_value}}\n  --kernel ${{inputs.kernel}}\n  --degree ${{inputs.degree}}\n  --gamma ${{inputs.gamma}}\n  --coef0 ${{inputs.coef0}}\n  --shrinking ${{inputs.shrinking}}\n  --probability ${{inputs.probability}}\n  --tol ${{inputs.tol}}\n  --cache_size ${{inputs.cache_size}}\n  --verbose ${{inputs.verbose}}\n  --max_iter ${{inputs.max_iter}}\n  --decision_function_shape ${{inputs.decision_function_shape}}\n  --break_ties ${{inputs.break_ties}}\n  --random_state ${{inputs.random_state}}\n  --model_output ${{outputs.model_output}}\n  --test_data ${{outputs.test_data}}\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-sweep-in-pipeline-0.md"
                }
            },
            {
                "page_content": "\n# Upgrade hyperparameter tuning to SDK v2\n\nIn SDK v2, tuning hyperparameters are consolidated into jobs.\n\nA job has a type. Most jobs are command jobs that run a `command`, like `python main.py`. What runs in a job is agnostic to any programming language, so you can run `bash` scripts, invoke `python` interpreters, run a bunch of `curl` commands, or anything else.\n\nA sweep job is another type of job, which defines sweep settings and can be initiated by calling the sweep method of command.\n\nTo upgrade, you'll need to change your code for defining and submitting your hyperparameter tuning experiment to SDK v2. What you run _within_ the job doesn't need to be upgraded to SDK v2. However, it's recommended to remove any code specific to Azure ML from your model training scripts. This separation allows for an easier transition between local and cloud and is considered best practice for mature MLOps. In practice, this means removing `azureml.*` lines of code. Model logging and tracking code should be replaced with MLflow. For more information, see [how to use MLflow in v2](how-to-use-mlflow-cli-runs.md).\n\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\n\n## Run hyperparameter tuning in an experiment\n\n* SDK v1\n\n    ```python\n    from azureml.core import ScriptRunConfig, Experiment, Workspace\n    from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n    from azureml.train.hyperdrive import choice, loguniform\n    \n    dataset = Dataset.get_by_name(ws, 'mnist-dataset')\n    \n    # list the files referenced by mnist dataset\n    dataset.to_path()\n    \n    #define the search space for your hyperparameters\n    param_sampling = RandomParameterSampling(\n        {\n            '--batch-size': choice(25, 50, 100),\n            '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n            '--second-layer-neurons': choice(10, 50, 200, 500),\n            '--learning-rate': loguniform(-6, -1)\n        }\n    )\n    \n    args = ['--data-folder', dataset.as_named_input('mnist').as_mount()]\n    \n    #Set up your script run\n    src = ScriptRunConfig(source_directory=script_folder,\n                          script='keras_mnist.py',\n                          arguments=args,\n                          compute_target=compute_target,\n                          environment=keras_env)\n    \n    # Set early stopping on this one\n    early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n    \n    # Define the configurations for your hyperparameter tuning experiment\n    hyperdrive_config = HyperDriveConfig(run_config=src,\n                                         hyperparameter_sampling=param_sampling,\n                                         policy=early_termination_policy,\n                                         primary_metric_name='Accuracy',\n                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                         max_total_runs=20,\n                                         max_concurrent_runs=4)\n    # Specify your experiment details                                     \n    experiment = Experiment(workspace, experiment_name)\n    \n    hyperdrive_run = experiment.submit(hyperdrive_config)\n    \n    #Find the best model\n    best_run = hyperdrive_run.get_best_run_by_primary_metric()\n    ```\n\n* SDK v2\n\n    ```python\n    from azure.ai.ml import MLClient\n    from azure.ai.ml import command, Input\n    from azure.ai.ml.sweep import Choice, Uniform, MedianStoppingPolicy\n    from azure.identity import DefaultAzureCredential\n    \n    # Create your command\n    command_job_for_sweep = command(\n        code=\"./src\",\n        command=\"python main.py --iris-csv ${{inputs.iris_csv}} --learning-rate ${{inputs.learning_rate}} --boosting ${{inputs.boosting}}\",\n        environment=\"AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu@latest\",\n        inputs={\n            \"iris_csv\": Input(\n                type=\"uri_file\",\n                path=\"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\",\n            ),\n            #define the search space for your hyperparameters\n            \"learning_rate\": Uniform(min_value=0.01, max_value=0.9),\n            \"boosting\": Choice(values=[\"gbdt\", \"dart\"]),\n        },\n        compute=\"cpu-cluster\",\n    )\n    \n    # Call sweep() on your command job to sweep over your parameter expressions\n    sweep_job = command_job_for_sweep.sweep(\n        compute=\"cpu-cluster\", \n        sampling_algorithm=\"random\",\n        primary_metric=\"test-multi_logloss\",\n        goal=\"Minimize\",\n    )\n    \n    # Define the limits for this sweep\n    sweep_job.set_limits(max_total_trials=20, max_concurrent_trials=10, timeout=7200)\n    \n    # Set early stopping on this one\n    sweep_job.early_termination = MedianStoppingPolicy(delay_evaluation=5, evaluation_interval=2)\n    \n    # Specify your experiment details\n    sweep_job.display_name = \"lightgbm-iris-sweep-example\"\n    sweep_job.experiment_name = \"lightgbm-iris-sweep-example\"\n    sweep_job.description = \"Run a hyperparameter sweep job for LightGBM on Iris dataset.\"\n    \n    # submit the sweep\n    returned_sweep_job = ml_client.create_or_update(sweep_job)\n    \n    # get a URL for the status of the job\n    returned_sweep_job.services[\"Studio\"].endpoint\n    \n    # Download best trial model output\n    ml_client.jobs.download(returned_sweep_job.name, output_name=\"model\")\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-execution-hyperdrive-0.md"
                }
            }
        ]
    },
    {
        "query": "What are the supported VM SKU List of AzureML ManagedOnlineEndpoint ?",
        "result": "The supported VM SKU list of AzureML ManagedOnlineEndpoint can be found in the reference documentation. For more information, see [Managed online endpoints SKU list](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas#azure-machine-learning-managed-online-endpoints).",
        "context": [
            {
                "page_content": "\n# [Python SDK](#tab/python)\n\n### Connect to Azure Machine Learning workspace\nConnect to Azure Machine Learning Workspace, configure workspace details, and get a handle to the workspace as follows:\n\n1. Import the required libraries:\n\n```python\n# import required libraries\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import (\n   ManagedOnlineEndpoint,\n   ManagedOnlineDeployment,\n   Model,\n   Environment,\n   CodeConfiguration,\n)\nfrom azure.identity import DefaultAzureCredential\n```\n\n2. Configure workspace details and get a handle to the workspace:\n\n```python\n# enter details of your AzureML workspace\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace = \"<AZUREML_WORKSPACE_NAME>\"\n\n# get a handle to the workspace\nml_client = MLClient(\n   DefaultAzureCredential(), subscription_id, resource_group, workspace\n)\n```\n\nFor more information, see [Deploy machine learning models to managed online endpoint using Python SDK v2](how-to-deploy-managed-online-endpoint-sdk-v2.md).\n\n### Configure online endpoint\n\n> [!TIP]\n> * `name`: The name of the endpoint. It must be unique in the Azure region. The name for an endpoint must start with an upper- or lowercase letter and only consist of '-'s and alphanumeric characters. For more information on the naming rules, see [managed online endpoint limits](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).\n> * `auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` doesn't expire, but `aml_token` does expire. For more information on authenticating, see [Authenticate to an online endpoint](how-to-authenticate-online-endpoint.md).\n\nOptionally, you can add description, tags to your endpoint.\n\n```python\n# Creating a unique endpoint name with current datetime to avoid conflicts\nimport datetime\n\nonline_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"this is a sample online endpoint\",\n    auth_mode=\"key\",\n    tags={\"foo\": \"bar\"},\n)\n```\n\n### Configure online deployment\n\nA deployment is a set of resources required for hosting the model that does the actual inferencing. We'll create a deployment for our endpoint using the `ManagedOnlineDeployment` class.\n\n> [!TIP]\n> - `name` - Name of the deployment.\n> - `endpoint_name` - Name of the endpoint to create the deployment under.\n> - `model` - The model to use for the deployment. This value can be either a reference to an existing versioned > model in the workspace or an inline model specification.\n> - `environment` - The environment to use for the deployment. This value can be either a reference to an existing > versioned environment in the workspace or an inline environment specification.\n> - `code_configuration` - the configuration for the source code and scoring script\n>     - `path`- Path to the source code directory for scoring the model\n>     - `scoring_script` - Relative path to the scoring file in the source code directory\n> - `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).\n> - `instance_count` - The number of instances to use for the deployment\n\n```python\n# create a blue deployment\nmodel = Model(name=\"tfserving-mounted\", version=\"1\", path=\"half_plus_two\")\n\nenv = Environment(\n    image=\"docker.io/tensorflow/serving:latest\",\n    inference_config={\n        \"liveness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n        \"readiness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n        \"scoring_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two:predict\"},\n    },\n)\n\nblue_deployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=online_endpoint_name,\n    model=model,\n    environment=env,\n    environment_variables={\n        \"MODEL_BASE_PATH\": \"/var/azureml-app/azureml-models/tfserving-mounted/1\",\n        \"MODEL_NAME\": \"half_plus_two\",\n    },\n    instance_type=\"Standard_DS2_v2\",\n    instance_count=1,\n)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-custom-container-812.md"
                }
            },
            {
                "page_content": "For more information about the YAML schema, see the [online endpoint YAML reference](reference-yaml-endpoint-online.md).\n\n> [!NOTE]\n> To use Kubernetes instead of managed endpoints as a compute target:\n> 1. Create and attach your Kubernetes cluster as a compute target to your Azure Machine Learning workspace by using [Azure Machine Learning studio](how-to-attach-kubernetes-to-workspace.md).\n> 1. Use the [endpoint YAML](https://github.com/Azure/azureml-examples/blob/main/cli/endpoints/online/kubernetes/kubernetes-endpoint.yml) to target Kubernetes instead of the managed endpoint YAML. You'll need to edit the YAML to change the value of `target` to the name of your registered compute target. You can use this [deployment.yaml](https://github.com/Azure/azureml-examples/blob/main/cli/endpoints/online/kubernetes/kubernetes-blue-deployment.yml) that has additional properties applicable to Kubernetes deployment.\n>\n> All the commands that are used in this article (except the optional SLA monitoring and Azure Log Analytics integration) can be used either with managed endpoints or with Kubernetes endpoints.\n\n# [Python](#tab/python)\n\nIn this article, we first define names of online endpoint and deployment for debug locally.\n\n1. Define endpoint (with name for local endpoint):\n      ```python\n    # Creating a local endpoint\n    import datetime\n\n    local_endpoint_name = \"local-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n\n    # create an online endpoint\n    endpoint = ManagedOnlineEndpoint(\n        name=local_endpoint_name, description=\"this is a sample local endpoint\"\n    )\n    ```\n\n1. Define deployment (with name for local deployment)\n\n    The example contains all the files needed to deploy a model on an online endpoint. To deploy a model, you must have:\n\n    * Model files (or the name and version of a model that's already registered in your workspace). In the example, we have a scikit-learn model that does regression.\n    * The code that's required to score the model. In this case, we have a score.py file.\n    * An environment in which your model runs. As you'll see, the environment might be a Docker image with Conda dependencies, or it might be a Dockerfile.\n    * Settings to specify the instance type and scaling capacity.\n\n    **Key aspects of deployment**\n    * `name` - Name of the deployment.\n    * `endpoint_name` - Name of the endpoint to create the deployment under.\n    * `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n    * `environment` - The environment to use for the deployment. This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification.\n    * `code_configuration` - the configuration for the source code and scoring script\n        * `path`- Path to the source code directory for scoring the model\n        * `scoring_script` - Relative path to the scoring file in the source code directory\n    * `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).\n    * `instance_count` - The number of instances to use for the deployment\n\n    ```python\n    model = Model(path=\"../model-1/model/sklearn_regression_model.pkl\")\n    env = Environment(\n        conda_file=\"../model-1/environment/conda.yml\",\n        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n    )\n\n    blue_deployment = ManagedOnlineDeployment(\n        name=\"blue\",\n        endpoint_name=local_endpoint_name,\n        model=model,\n        environment=env,\n        code_configuration=CodeConfiguration(\n            code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n        ),\n        instance_type=\"Standard_DS2_v2\",\n        instance_count=1,\n    )\n    ```\n\n# [ARM template](#tab/arm)\n\nThe Azure Resource Manager templates [online-endpoint.json](https://github.com/Azure/azureml-examples/tree/main/arm-templates/online-endpoint.json) and [online-endpoint-deployment.json](https://github.com/Azure/azureml-examples/tree/main/arm-templates/online-endpoint-deployment.json) are used by the steps in this article.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-online-endpoints-1031.md"
                }
            },
            {
                "page_content": "\n# [Python](#tab/python)\n\n### Create online endpoint\n\nTo create a managed online endpoint, use the `ManagedOnlineEndpoint` class. This class allows users to configure the following key aspects of the endpoint:\n\n* `name` - Name of the endpoint. Needs to be unique at the Azure region level\n* `auth_mode` - The authentication method for the endpoint. Key-based authentication and Azure ML token-based authentication are supported. Key-based authentication doesn't expire but Azure ML token-based authentication does. Possible values are `key` or `aml_token`.\n* `identity`- The managed identity configuration for accessing Azure resources for endpoint provisioning and inference.\n    * `type`- The type of managed identity. Azure Machine Learning supports `system_assigned` or `user_assigned` identity.\n    * `user_assigned_identities` - List (array) of fully qualified resource IDs of the user-assigned identities. This property is required if `identity.type` is user_assigned.\n* `description`- Description of the endpoint.\n\n1. Configure the endpoint:\n\n```python\n# Creating a unique endpoint name with current datetime to avoid conflicts\nimport random\n\nonline_endpoint_name = \"endpt-moe-\" + str(random.randint(0, 10000))\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"this is a sample online endpoint\",\n    auth_mode=\"key\",\n    tags={\"foo\": \"bar\"},\n)\n```\n\n    > [!NOTE]\n    > To create a Kubernetes online endpoint, use the `KubernetesOnlineEndpoint` class.\n\n1. Create the endpoint:\n\n```python\nml_client.online_endpoints.begin_create_or_update(endpoint).result()\n```\n\n### Create the 'blue' deployment\n\nA deployment is a set of resources required for hosting the model that does the actual inferencing. To create a deployment for your managed online endpoint, use the `ManagedOnlineDeployment` class. This class allows users to configure the following key aspects of the deployment:\n\n**Key aspects of deployment**\n* `name` - Name of the deployment.\n* `endpoint_name` - Name of the endpoint to create the deployment under.\n* `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n* `environment` - The environment to use for the deployment. This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification.\n* `code_configuration` - the configuration for the source code and scoring script\n    * `path`- Path to the source code directory for scoring the model\n    * `scoring_script` - Relative path to the scoring file in the source code directory\n* `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).\n* `instance_count` - The number of instances to use for the deployment\n\n1. Configure blue deployment:\n\n```python\n# create blue deployment\nmodel = Model(path=\"../model-1/model/sklearn_regression_model.pkl\")\nenv = Environment(\n    conda_file=\"../model-1/environment/conda.yml\",\n    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n)\n\nblue_deployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=online_endpoint_name,\n    model=model,\n    environment=env,\n    code_configuration=CodeConfiguration(\n        code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n    ),\n    instance_type=\"Standard_DS3_v2\",\n    instance_count=1,\n)\n```\n\n    > [!NOTE]\n    > To create a deployment for a Kubernetes online endpoint, use the `KubernetesOnlineDeployment` class.\n\n1. Create the deployment:\n\n```python\nml_client.online_deployments.begin_create_or_update(blue_deployment).result()\n```\n\n```python\n# blue deployment takes 100 traffic\nendpoint.traffic = {\"blue\": 100}\nml_client.online_endpoints.begin_create_or_update(endpoint).result()\n```\n\n\n## Confirm your existing deployment\n\n# [Azure CLI](#tab/azure-cli)\n\nYou can view the status of your existing endpoint and deployment by running:\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-safely-rollout-online-endpoints-401.md"
                }
            }
        ]
    },
    {
        "query": "Generate a python code sample to enable debugging of a ManagedOnlineEndpoint duing deployments using AzureML SDK V2?",
        "result": "To enable debugging of a ManagedOnlineEndpoint during deployments using AzureML SDK V2, you can use the `--vscode-debug` flag when creating or updating an Azure Machine Learning online deployment. You can also set the `vscode-debug` and `local` flags when creating or updating an Azure Machine Learning online deployment in Python. This will use Docker and VS Code development containers to build and configure a local debugging environment. Once the image successfully builds, your dev container opens in a VS Code window. You can then use the VS Code extensions to debug your deployments in the dev container.",
        "context": [
            {
                "page_content": "\n1. Create a YAML configuration file for your endpoint. The following example configures the name and authentication mode of the endpoint. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-endpoint.yml` in the azureml-examples repo you cloned earlier:\n\n    __create-managed-endpoint.yaml__\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\nname: my-endpoint\nauth_mode: aml_token\n```\n\n1. Create a YAML configuration file for the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-deployment.yml` in the azureml-examples repo you cloned earlier:\n\n    > [!IMPORTANT]\n    > For Triton no-code-deployment (NCD) to work, setting **`type`** to **`triton_model\u200b`** is required, `type: triton_model\u200b`. For more information, see [CLI (v2) model YAML schema](reference-yaml-model.md).\n    >\n    > This deployment uses a Standard_NC6s_v3 VM. You may need to request a quota increase for your subscription before you can use this VM. For more information, see [NCv3-series](../virtual-machines/ncv3-series.md).\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: my-endpoint\nmodel:\n  name: sample-densenet-onnx-model\n  version: 1\n  path: ./models\n  type: triton_model\ninstance_count: 1\ninstance_type: Standard_NC6s_v3\n```\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nThis section shows how you can define a Triton deployment to deploy to a managed online endpoint using the Azure Machine Learning Python SDK (v2).\n\n> [!IMPORTANT]\n> For Triton no-code-deployment, **[testing via local endpoints](how-to-deploy-online-endpoints.md#deploy-and-debug-locally-by-using-local-endpoints)** is currently not supported.\n\n\n1. To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. \n\n    ```python \n    subscription_id = \"<SUBSCRIPTION_ID>\"\n    resource_group = \"<RESOURCE_GROUP>\"\n    workspace_name = \"<AML_WORKSPACE_NAME>\"\n    ```\n\n1. Use the following command to set the name of the endpoint that will be created. In this example, a random name is created for the endpoint:\n\n    ```python\n    import random\n\n    endpoint_name = f\"endpoint-{random.randint(0, 10000)}\"\n    ```\n\n1. We use these details above in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. Check the [configuration notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n\n    ```python \n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n\n    ml_client = MLClient(\n        DefaultAzureCredential(),\n        subscription_id,\n        resource_group,\n        workspace_name,\n    )\n    ```\n\n1. Create a `ManagedOnlineEndpoint` object to configure the endpoint. The following example configures the name and authentication mode of the endpoint. \n\n    ```python \n    from azure.ai.ml.entities import ManagedOnlineEndpoint\n\n    endpoint = ManagedOnlineEndpoint(name=endpoint_name, auth_mode=\"key\")\n    ```\n\n1. Create a `ManagedOnlineDeployment` object to configure the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step and defines a local model inline.\n\n    ```python\n    from azure.ai.ml.entities import ManagedOnlineDeployment, Model\n    \n    model_name = \"densenet-onnx-model\"\n    model_version = 1\n    \n    deployment = ManagedOnlineDeployment(\n        name=\"blue\",\n        endpoint_name=endpoint_name,\n        model=Model(\n            name=model_name, \n            version=model_version,\n            path=\"./models\",\n            type=\"triton_model\"\n        ),\n        instance_type=\"Standard_NC6s_v3\",\n        instance_count=1,\n    )\n    ``` \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-with-triton-174.md"
                }
            },
            {
                "page_content": "The examples in this article can be found in the [Debug online endpoints locally in Visual Studio Code](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/online/managed/debug-online-endpoints-locally-in-visual-studio-code.ipynb) notebook within the[azureml-examples](https://github.com/azure/azureml-examples) repository. To run the code locally, clone the repo and then change directories to the notebook's parent directory `sdk/endpoints/online/managed`. \n\n```azurecli\ngit clone https://github.com/Azure/azureml-examples --depth 1\ncd azureml-examples\ncd sdk/python/endpoints/online/managed\n```\n\nImport the required modules: \n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import (\n    ManagedOnlineEndpoint,\n    ManagedOnlineDeployment,\n    Model,\n    CodeConfiguration,\n    Environment,\n)\nfrom azure.identity import DefaultAzureCredential\n``` \n\nSet up variables for the workspace and endpoint: \n\n```python \nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace_name = \"<AML_WORKSPACE_NAME>\"\n\nendpoint_name = \"<ENDPOINT_NAME>\"\n``` \n\n\n## Launch development container\n\n# [Azure CLI](#tab/cli)\n\nAzure Machine Learning local endpoints use Docker and VS Code development containers (dev container) to build and configure a local debugging environment. With dev containers, you can take advantage of VS Code features from inside a Docker container. For more information on dev containers, see [Create a development container](https://code.visualstudio.com/docs/remote/create-dev-container).\n\nTo debug online endpoints locally in VS Code, use the `--vscode-debug` flag when creating or updating and Azure Machine Learning online deployment. The following command uses a deployment example from the examples repo:\n\n```azurecli\naz ml online-deployment create --file endpoints/online/managed/sample/blue-deployment.yml --local --vscode-debug\n```\n\n> [!IMPORTANT]\n> On Windows Subsystem for Linux (WSL), you'll need to update your PATH environment variable to include the path to the VS Code executable or use WSL interop. For more information, see [Windows interoperability with Linux](/windows/wsl/interop).\n\nA Docker image is built locally. Any environment configuration or model file errors are surfaced at this stage of the process.\n\n> [!NOTE]\n> The first time you launch a new or updated dev container it can take several minutes.\n\nOnce the image successfully builds, your dev container opens in a VS Code window.\n\nYou'll use a few VS Code extensions to debug your deployments in the dev container. Azure Machine Learning automatically installs these extensions in your dev container.\n\n- Inference Debug\n- [Pylance](https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance)\n- [Jupyter](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter)\n- [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python)\n\n> [!IMPORTANT]\n> Before starting your debug session, make sure that the VS Code extensions have finished installing in your dev container.  \n\n\n# [Python](#tab/python)\n\nAzure Machine Learning local endpoints use Docker and VS Code development containers (dev container) to build and configure a local debugging environment. With dev containers, you can take advantage of VS Code features from inside a Docker container. For more information on dev containers, see [Create a development container](https://code.visualstudio.com/docs/remote/create-dev-container).\n\nGet a handle to the workspace: \n\n```python \ncredential = DefaultAzureCredential()\nml_client = MLClient(\n    credential,\n    subscription_id=subscription_id,\n    resource_group_name=resource_group,\n    workspace_name=workspace_name,\n)\n``` \n\nTo debug online endpoints locally in VS Code, set the `vscode-debug` and `local` flags when creating or updating an Azure Machine Learning online deployment. The following code mirrors a deployment example from the examples repo:\n\n```python\ndeployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=endpoint_name,\n    model=Model(path=\"../model-1/model/sklearn_regression_model.pkl\"),\n    code_configuration=CodeConfiguration(\n        code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n    ),\n    environment=Environment(\n        conda_file=\"../model-1/environment/conda.yml\",\n        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n    ),\n    instance_type=\"Standard_DS3_v2\",\n    instance_count=1,\n)\n\ndeployment = ml_client.online_deployments.begin_create_or_update(\n    deployment, local=True, vscode_debug=True\n).result()\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-debug-managed-online-endpoints-visual-studio-code-77.md"
                }
            },
            {
                "page_content": "\n# Upgrade deployment endpoints to SDK v2\n\nWe newly introduced [online endpoints](concept-endpoints.md) and batch endpoints as v2 concepts. There are several deployment funnels such as managed online endpoints, [kubernetes online endpoints](how-to-attach-kubernetes-anywhere.md) (including Azure Kubernetes Services and Arc-enabled Kubernetes) in v2, and Azure Container Instances (ACI) and Kubernetes Services (AKS) webservices in v1. In this article, we'll focus on the comparison of deploying to ACI webservices (v1) and managed online endpoints (v2).\n\nExamples in this article show how to:\n\n* Deploy your model to Azure\n* Score using the endpoint\n* Delete the webservice/endpoint\n\n## Create inference resources\n\n* SDK v1\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        # configure a model. example for registering a model \n        from azureml.core.model import Model\n        model = Model.register(ws, model_name=\"bidaf_onnx\", model_path=\"./model.onnx\")\n        \n        # configure an environment\n        from azureml.core import Environment\n        env = Environment(name='myenv')\n        python_packages = ['nltk', 'numpy', 'onnxruntime']\n        for package in python_packages:\n            env.python.conda_dependencies.add_pip_package(package)\n        \n        # configure an inference configuration with a scoring script\n        from azureml.core.model import InferenceConfig\n        inference_config = InferenceConfig(\n            environment=env,\n            source_directory=\"./source_dir\",\n            entry_script=\"./score.py\",\n        )\n        ```\n\n    1. Configure and deploy an **ACI webservice**:\n        ```python\n        from azureml.core.webservice import AciWebservice\n        \n        # defince compute resources for ACI\n        deployment_config = AciWebservice.deploy_configuration(\n            cpu_cores=0.5, memory_gb=1, auth_enabled=True\n        )\n        \n        # define an ACI webservice\n        service = Model.deploy(\n            ws,\n            \"myservice\",\n            [model],\n            inference_config,\n            deployment_config,\n            overwrite=True,\n        )\n        \n        # create the service \n        service.wait_for_deployment(show_output=True)\n        ```\n\nFor more information on registering models, see [Register a model from a local file](v1/how-to-deploy-and-where.md?tabs=python#register-a-model-from-a-local-file).\n\n* SDK v2\n\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        from azure.ai.ml.entities import Model\n        # configure a model\n        model = Model(path=\"../model-1/model/sklearn_regression_model.pkl\")\n        \n        # configure an environment\n        from azure.ai.ml.entities import Environment\n        env = Environment(\n            conda_file=\"../model-1/environment/conda.yml\",\n            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n        )\n        \n        # configure an inference configuration with a scoring script\n        from azure.ai.ml.entities import CodeConfiguration\n        code_config = CodeConfiguration(\n                code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n            )\n        ```\n\n    1. Configure and create an **online endpoint**:\n        ```python\n        import datetime\n        from azure.ai.ml.entities import ManagedOnlineEndpoint\n        \n        # create a unique endpoint name with current datetime to avoid conflicts\n        online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n        \n        # define an online endpoint\n        endpoint = ManagedOnlineEndpoint(\n            name=online_endpoint_name,\n            description=\"this is a sample online endpoint\",\n            auth_mode=\"key\",\n            tags={\"foo\": \"bar\"},\n        )\n        \n        # create the endpoint:\n        ml_client.begin_create_or_update(endpoint)\n        ```\n    \n    1. Configure and create an **online deployment**:\n        ```python\n        from azure.ai.ml.entities import ManagedOnlineDeployment\n        \n        # define a deployment\n        blue_deployment = ManagedOnlineDeployment(\n            name=\"blue\",\n            endpoint_name=online_endpoint_name,\n            model=model,\n            environment=env,\n            code_configuration=code_config,\n            instance_type=\"Standard_F2s_v2\",\n            instance_count=1,\n        )\n        \n        # create the deployment:\n        ml_client.begin_create_or_update(blue_deployment)\n        \n        # blue deployment takes 100 traffic\n        endpoint.traffic = {\"blue\": 100}\n        ml_client.begin_create_or_update(endpoint)\n        ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-deploy-endpoints-0.md"
                }
            }
        ]
    },
    {
        "query": "How can i troubleshoot a azureml managed endpoint failures ?",
        "result": "To troubleshoot AzureML managed endpoint failures, you can start by checking the logs for the endpoint. You can access the logs through the Azure portal or by using the Azure Machine Learning SDK. The logs can provide information on the cause of the failure, such as errors in the scoring script or issues with the model. You can also check the status of the endpoint and the associated compute resources to ensure they are running correctly. Additionally, you can try re-deploying the endpoint or updating the model to see if that resolves the issue.",
        "context": [
            {
                "page_content": "* A container registry behind a virtual network is using a private endpoint in an [unsupported region](https://aka.ms/azureml/environment/private-link-availability)\n* The image you're trying to reference doesn't exist in the container registry you specified \n* You haven't provided credentials for a private registry you're trying to pull the image from, or the provided credentials are incorrect\n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nIf you suspect that the path name to your container registry is incorrect\n* For a registry `my-registry.io` and image `test/image` with tag `3.2`, a valid image path would be `my-registry.io/test/image:3.2`\n* See [registry path documentation](https://aka.ms/azureml/environment/docker-registries)\n\nIf your container registry is behind a virtual network or is using a private endpoint in an [unsupported region](https://aka.ms/azureml/environment/private-link-availability)\n* Configure the container registry by using the service endpoint (public access) from the portal and retry\n* After you put the container registry behind a virtual network, run the [Azure Resource Manager template](https://aka.ms/azureml/environment/secure-resources-using-vnet) so the workspace can communicate with the container registry instance\n\nIf the image you're trying to reference doesn't exist in the container registry you specified\n* Check that the correct tag is used and that `user_managed_dependencies` is set to `True`. Setting [user_managed_dependencies](https://aka.ms/azureml/environment/environment-python-section) to `True` disables conda and uses the user's installed packages\n\nIf you haven't provided credentials for a private registry you're trying to pull from, or the provided credentials are incorrect\n* Set [workspace connections](https://aka.ms/azureml/environment/set-connection-v1) for the container registry if needed\n\n### I/O Error\n<!--issueDescription-->\nThis issue can happen when a Docker image pull fails due to a network issue.  \n\n**Potential causes:**\n* Network connection issue, which could be temporary\n* Firewall is blocking the connection\n* ACR is unreachable and there's network isolation. For more details, see [ACR unreachable](#acr-unreachable). \n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n<!--/issueDescription-->\n\n**Troubleshooting steps**  \n\nAdd the host to the firewall rules  \n* See [configure inbound and outbound network traffic](how-to-access-azureml-behind-firewall.md) to learn how to use Azure Firewall for your workspace and resources behind a VNet\n\nAssess your workspace set-up. Are you using a virtual network, or are any of the resources you're trying to access during your image build behind a virtual network?\n* Ensure that you've followed the steps in this article on [securing a workspace with virtual networks](https://aka.ms/azureml/environment/acr-private-endpoint)\n* Azure Machine Learning requires both inbound and outbound access to the public internet. If there's a problem with your virtual network setup, there might be an issue with accessing certain repositories required during your image build  \n\nIf you aren't using a virtual network, or if you've configured it correctly\n* Try rebuilding your image. If the timeout was due to a network issue, the problem might be transient, and a rebuild could fix the problem\n\n## *Conda issues during build*\n### Bad spec\n<!--issueDescription-->\nThis issue can happen when a package listed in your conda specification is invalid or when a conda command is executed incorrectly.\n\n**Potential causes:**\n* The syntax you used in your conda specification is incorrect\n* You're executing a conda command incorrectly\n\n**Affected areas (symptoms):**\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-1192.md"
                }
            },
            {
                "page_content": "* Ensure that any package needed to execute the command you're trying to perform is installed\n* If needed, add an installation step to your Dockerfile\n\n**Resources**\n* [Dockerfile reference on running commands](https://docs.docker.com/engine/reference/builder/#run)\n\n### Conda timeout\n<!--issueDescription-->\nThis issue can happen when conda package resolution takes too long to complete.\n\n**Potential causes:**\n* There's a large number of packages listed in your conda specification and unnecessary packages are included\n* You haven't pinned your dependencies (you included tensorflow instead of tensorflow=2.8)\n* You've listed packages for which there's no solution (you included package X=1.3 and Y=2.8, but X's version is incompatible with Y's version)\n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n* Remove any packages from your conda specification that are unnecessary\n* Pin your packages--environment resolution will be faster\n* If you're still having issues, review this article for an in-depth look at [understanding and improving conda's performance](https://aka.ms/azureml/environment/improve-conda-performance)\n\n### Out of memory\n<!--issueDescription-->\nThis issue can happen when conda package resolution fails due to available memory being exhausted.\n\n**Potential causes:**\n* There's a large number of packages listed in your conda specification and unnecessary packages are included\n* You haven't pinned your dependencies (you included tensorflow instead of tensorflow=2.8)\n* You've listed packages for which there's no solution (you included package X=1.3 and Y=2.8, but X's version is incompatible with Y's version)\n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n* Remove any packages from your conda specification that are unnecessary\n* Pin your packages--environment resolution will be faster\n* If you're still having issues, review this article for an in-depth look at [understanding and improving conda's performance](https://aka.ms/azureml/environment/improve-conda-performance)\n\n### Package not found\n<!--issueDescription-->\nThis issue can happen when one or more conda packages listed in your specification can't be found in a channel/repository.\n\n**Potential causes:**\n* The package's name or version was listed incorrectly in your conda specification \n* The package exists in a conda channel that you didn't list in your conda specification\n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n* Ensure that the package is spelled correctly and that the specified version exists\n* Ensure that the package exists on the channel you're targeting\n* Ensure that the channel/repository is listed in your conda specification so the package can be pulled correctly during package resolution\n\nSpecify channels in your conda specification:\n\n```yaml\nchannels:\n  - conda-forge\n  - anaconda\ndependencies:\n  - python=3.8\n  - tensorflow=2.8\nName: my_environment\n```\n\n**Resources**\n* [Managing channels](https://aka.ms/azureml/environment/managing-conda-channels)\n\n### Missing Python module\n<!--issueDescription-->\nThis issue can happen when a Python module listed in your conda specification doesn't exist or isn't valid.\n\n**Potential causes:**\n* The module was spelled incorrectly\n* The module isn't recognized\n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-1331.md"
                }
            },
            {
                "page_content": "* You've specified more than one Python version in your environment definition\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nChoose which Python version you want to use, and remove all other versions \n\n```python\nmyenv.python.conda_dependencies.remove_conda_package(\"python=3.6\")\n```\n\n*Applies to: all scenarios*\n\nIf you're using a YAML for your conda specification, include only one Python version as a dependency\n\n**Resources**\n* [CondaDependencies Class v1](https://aka.ms/azureml/environment/conda-dependencies-class)\n\n### Python version not supported\n<!--issueDescription-->\n**Potential causes:**\n* You've specified a Python version that is at or near its end-of-life and is no longer supported\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nSpecify a [python version](https://aka.ms/azureml/environment/python-versions) that hasn't reached and isn't nearing its [end-of-life](https://aka.ms/azureml/environment/python-end-of-life)\n\n### Python version not recommended\n<!--issueDescription-->\n**Potential causes:**\n* You've specified a Python version that is at or near its end-of-life\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nSpecify a [python version](https://aka.ms/azureml/environment/python-versions) that hasn't reached and isn't nearing its [end-of-life](https://aka.ms/azureml/environment/python-end-of-life)\n\n### Failed to validate Python version\n<!--issueDescription-->\n**Potential causes:**\n* The provided Python version was formatted improperly or specified with incorrect syntax\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nUse correct syntax to specify a Python version using the SDK\n\n```python\nmyenv.python.conda_dependencies.add_conda_package(\"python=3.8\")\n```\n\n*Applies to: all scenarios*\n\nUse correct syntax to specify a Python version in a conda YAML\n\n```yaml\nname: project_environment\ndependencies:\n  - python=3.8\n  - pip:\n      - azureml-defaults\nchannels:\n  - anaconda\n```\n\n**Resources**\n* See [conda package pinning](https://aka.ms/azureml/environment/how-to-pin-conda-packages)\n\n## *Conda issues*\n### Missing conda dependencies\n<!--issueDescription-->\n**Potential causes:**\n* You haven't provided a conda specification in your environment definition, and `user_managed_dependencies` is set to `False` (the default)\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nIf you don't want AzureML to create a Python environment for you based on `conda_dependencies,` set `user_managed_dependencies` to `True`\n\n```python\nenv.python.user_managed_dependencies = True\n```\n* You're responsible for ensuring that all necessary packages are available in the Python environment in which you choose to run the script\n\nIf you want AzureML to create a Python environment for you based on a conda specification, `conda_dependencies` needs to be populated in your environment definition \n\n```python\nfrom azureml.core.environment import CondaDependencies\n\nenv = Environment(name=\"env\")\nconda_dep = CondaDependencies()\nconda_dep.add_conda_package(\"python==3.8\")\nenv.python.conda_dependencies = conda_dep\n```\n\n*Applies to: Azure CLI & Python SDK v2*\n\nYou must specify a base Docker image for the environment, and the conda environment will be built on top of that image\n* Provide the relative path to the conda file\n* See how to [create an environment from a conda specification](https://aka.ms/azureml/environment/create-env-conda-spec-v2)\n\n**Resources**\n* See [how to create a conda file manually](https://aka.ms/azureml/environment/how-to-create-conda-file)\n* See [CondaDependencies class](https://aka.ms/azureml/environment/conda-dependencies-class)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-704.md"
                }
            }
        ]
    },
    {
        "query": "How can I  to create a GPU based compute instance using AzureML SDK V2",
        "result": "To create a GPU based compute instance using AzureML SDK V2, you can use the following Python code:\n\n```python\nfrom azure.ai.ml.entities import AmlCompute\ncompute_name = \"gpu-cluster\"\ncluster_basic = AmlCompute(\n    name=compute_name,\n    type=\"amlcompute\",\n    size=\"Standard_NC24s_v3\",\n    min_instances=0,\n    max_instances=4,\n    idle_time_before_scale_down=120,\n)\nml_client.begin_create_or_update(cluster_basic)\n```\n\nThis code creates a GPU compute of size `Standard_NC24s_v3` with four nodes. You can adjust the `min_instances`, `max_instances`, and `idle_time_before_scale_down` parameters as needed.",
        "context": [
            {
                "page_content": "This tutorial uses the NCsv3-series (with V100 GPUs) as this type of compute target leverages multiple GPUs to speed up training. Additionally, you can set up multiple nodes to take advantage of parallelism when tuning hyperparameters for your model.\n\nThe following code creates a GPU compute of size `Standard_NC24s_v3` with four nodes.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nCreate a .yml file with the following configuration.\n\n```yml\n$schema: https://azuremlschemas.azureedge.net/latest/amlCompute.schema.json \nname: gpu-cluster\ntype: amlcompute\nsize: Standard_NC24s_v3\nmin_instances: 0\nmax_instances: 4\nidle_time_before_scale_down: 120\n```\n\nTo create the compute, you run the following CLI v2 command with the path to your .yml file, workspace name, resource group and subscription ID.\n\n```azurecli\naz ml compute create -f [PATH_TO_YML_FILE] --workspace-name [YOUR_AZURE_WORKSPACE] --resource-group [YOUR_AZURE_RESOURCE_GROUP] --subscription [YOUR_AZURE_SUBSCRIPTION]\n```\n\nThe created compute can be provided using `compute` key in the `automl` task configuration yaml: \n\n```yaml\ncompute: azureml:gpu-cluster\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nfrom azure.ai.ml.entities import AmlCompute\ncompute_name = \"gpu-cluster\"\ncluster_basic = AmlCompute(\n    name=compute_name,\n    type=\"amlcompute\",\n    size=\"Standard_NC24s_v3\",\n    min_instances=0,\n    max_instances=4,\n    idle_time_before_scale_down=120,\n)\nml_client.begin_create_or_update(cluster_basic)\n```\nThis compute is used later while creating the task specific `automl` job.\n\n\n## Experiment setup\n\nYou can use an Experiment to track your model training jobs.\n\n# [Azure CLI](#tab/cli)\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\nExperiment name can be provided using `experiment_name` key as follows: \n\n```yaml\nexperiment_name: dpv2-cli-automl-image-object-detection-experiment\n```\n\n# [Python SDK](#tab/python)\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nExperiment name is used later while creating the task specific `automl` job.\n```python\nexp_name = \"dpv2-image-object-detection-experiment\"\n```\n\n## Visualize input data\n\nOnce you have the input image data prepared in [JSONL](https://jsonlines.org/) (JSON Lines) format, you can visualize the ground truth bounding boxes for an image. To do so, be sure you have `matplotlib` installed.\n\n```\n%pip install --upgrade matplotlib\n```\n```python\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.patches as patches\nfrom PIL import Image as pil_image\nimport numpy as np\nimport json\nimport os\n\ndef plot_ground_truth_boxes(image_file, ground_truth_boxes):\n    # Display the image\n    plt.figure()\n    img_np = mpimg.imread(image_file)\n    img = pil_image.fromarray(img_np.astype(\"uint8\"), \"RGB\")\n    img_w, img_h = img.size\n\n    fig,ax = plt.subplots(figsize=(12, 16))\n    ax.imshow(img_np)\n    ax.axis(\"off\")\n\n    label_to_color_mapping = {}\n\n    for gt in ground_truth_boxes:\n        label = gt[\"label\"]\n\n        xmin, ymin, xmax, ymax =  gt[\"topX\"], gt[\"topY\"], gt[\"bottomX\"], gt[\"bottomY\"]\n        topleft_x, topleft_y = img_w * xmin, img_h * ymin\n        width, height = img_w * (xmax - xmin), img_h * (ymax - ymin)\n\n        if label in label_to_color_mapping:\n            color = label_to_color_mapping[label]\n        else:\n            # Generate a random color. If you want to use a specific color, you can use something like \"red\".\n            color = np.random.rand(3)\n            label_to_color_mapping[label] = color\n\n        # Display bounding box\n        rect = patches.Rectangle((topleft_x, topleft_y), width, height,\n                                 linewidth=2, edgecolor=color, facecolor=\"none\")\n        ax.add_patch(rect)\n\n        # Display label\n        ax.text(topleft_x, topleft_y - 10, label, color=color, fontsize=20)\n\n    plt.show()\n\ndef plot_ground_truth_boxes_jsonl(image_file, jsonl_file):\n    image_base_name = os.path.basename(image_file)\n    ground_truth_data_found = False\n    with open(jsonl_file) as fp:\n        for line in fp.readlines():\n            line_json = json.loads(line)\n            filename = line_json[\"image_url\"]\n            if image_base_name in filename:\n                ground_truth_data_found = True\n                plot_ground_truth_boxes(image_file, line_json[\"label\"])\n                break\n    if not ground_truth_data_found:\n        print(\"Unable to find ground truth information for image: {}\".format(image_file))\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-auto-train-image-models-70.md"
                }
            },
            {
                "page_content": "\n# Set up AutoML to train a natural language processing model \n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n> [!div class=\"op_single_selector\" title1=\"Select the version of the developer platform of Azure Machine Learning  you are using:\"]\n> * [v1](./v1/how-to-auto-train-nlp-models-v1.md)\n> * [v2 (current version)](how-to-auto-train-nlp-models.md)\n \n\nIn this article, you learn how to train natural language processing (NLP) models with [automated ML](concept-automated-ml.md) in Azure Machine Learning. You can create NLP models with automated ML via the Azure Machine Learning Python SDK v2 or the Azure Machine Learning CLI v2. \n\nAutomated ML supports NLP which allows ML professionals and data scientists to bring their own text data and build custom models for tasks such as, multi-class text classification, multi-label text classification, and named entity recognition (NER).  \n\nYou can seamlessly integrate with the [Azure Machine Learning data labeling](how-to-create-text-labeling-projects.md) capability to label your text data or bring your existing labeled data. Automated ML provides the option to use distributed training on multi-GPU compute clusters for faster model training. The resulting model can be operationalized at scale by leveraging Azure ML\u2019s MLOps capabilities. \n\n## Prerequisites\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n* Azure subscription. If you don't have an Azure subscription, sign up to try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/) today.\n\n* An Azure Machine Learning workspace with a GPU training compute. To create the workspace, see [Create workspace resources](quickstart-create-resources.md). See [GPU optimized virtual machine sizes](../virtual-machines/sizes-gpu.md) for more details of GPU instances provided by Azure.\n\n    > [!WARNING]\n    > Support for multilingual models and the use of models with longer max sequence length is necessary for several NLP use cases, such as non-english datasets and longer range documents. As a result, these scenarios may require higher GPU memory for model training to succeed, such as the NC_v3 series or the ND series. \n  \n* The Azure Machine Learning CLI v2 installed. For guidance to update and install the latest version, see the [Install and set up CLI (v2)](how-to-configure-cli.md).\n\n* This article assumes some familiarity with setting up an automated machine learning experiment. Follow the [how-to](how-to-configure-auto-train.md) to see the main automated machine learning experiment design patterns.\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n* Azure subscription. If you don't have an Azure subscription, sign up to try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/) today.\n\n* An Azure Machine Learning workspace with a GPU training compute. To create the workspace, see [Create workspace resources](quickstart-create-resources.md). See [GPU optimized virtual machine sizes](../virtual-machines/sizes-gpu.md) for more details of GPU instances provided by Azure.\n\n   > [!WARNING]\n   > Support for multilingual models and the use of models with longer max sequence length is necessary for several NLP use cases, such as non-english datasets and longer range documents. As a result, these scenarios may require higher GPU memory for model training to succeed, such as the NC_v3 series or the ND series. \n  \n* The Azure Machine Learning Python SDK v2 installed. \n\n    To install the SDK you can either, \n    * Create a compute instance, which automatically installs the SDK and is pre-configured for ML workflows. See [Create and manage an Azure Machine Learning compute instance](how-to-create-manage-compute-instance.md) for more information. \n\n    * [Install the `automl` package yourself](https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/README.md#setup-using-a-local-conda-environment), which includes the [default installation](/python/api/overview/azure/ml/install#default-install) of the SDK.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-nlp-models-0.md"
                }
            },
            {
                "page_content": "\n# Train PyTorch models at scale with Azure Machine Learning\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n> [!div class=\"op_single_selector\" title1=\"Select the Azure Machine Learning SDK version you are using:\"]\n> * [v1](v1/how-to-train-pytorch.md)\n> * [v2 (current version)](how-to-train-pytorch.md)\n\nIn this article, you'll learn to train, hyperparameter tune, and deploy a [PyTorch](https://pytorch.org/) model using the Azure Machine Learning (AzureML) Python SDK v2.\n\nYou'll use the example scripts in this article to classify chicken and turkey images to build a deep learning neural network (DNN) based on [PyTorch's transfer learning tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html). Transfer learning is a technique that applies knowledge gained from solving one problem to a different but related problem. Transfer learning shortens the training  process by requiring less data, time, and compute resources than training from scratch. To learn more about transfer learning, see the [deep learning vs machine learning](./concept-deep-learning-vs-machine-learning.md#what-is-transfer-learning) article.\n\nWhether you're training a deep learning PyTorch model from the ground-up or you're bringing an existing model into the cloud, you can use AzureML to scale out open-source training jobs using elastic cloud compute resources. You can build, deploy, version, and monitor production-grade models with AzureML.\n\n## Prerequisites\n\nTo benefit from this article, you'll need to:\n\n- Access an Azure subscription. If you don't have one already, [create a free account](https://azure.microsoft.com/free/).\n- Run the code in this article using either an Azure Machine Learning compute instance or your own Jupyter notebook.\n    - Azure Machine Learning compute instance\u2014no downloads or installation necessary\n        - Complete the [Quickstart: Get started with Azure Machine Learning](quickstart-create-resources.md) to create a dedicated notebook server pre-loaded with the SDK and the sample repository.\n        - In the samples deep learning folder on the notebook server, find a completed and expanded notebook by navigating to this directory: **v2  > sdk > python > jobs > single-step > pytorch > train-hyperparameter-tune-deploy-with-pytorch**.\n    - Your Jupyter notebook server\n        - [Install the Azure Machine Learning SDK (v2)](https://aka.ms/sdk-v2-install).\n        - Download the training script file [pytorch_train.py](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/single-step/pytorch/train-hyperparameter-tune-deploy-with-pytorch/src/pytorch_train.py).\n\nYou can also find a completed [Jupyter Notebook version](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/single-step/pytorch/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb) of this guide on the GitHub samples page.\n\n[!INCLUDE [gpu quota](../../includes/machine-learning-gpu-quota-prereq.md)]\n\n## Set up the job\n\nThis section sets up the job for training by loading the required Python packages, connecting to a workspace, creating a compute resource to run a command job, and creating an environment to run the job.\n\n### Connect to the workspace\n\nFirst, you'll need to connect to your AzureML workspace. The [AzureML workspace](concept-workspace.md) is the top-level resource for the service. It provides you with a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n\nWe're using `DefaultAzureCredential` to get access to the workspace. This credential should be capable of handling most Azure SDK authentication scenarios.\n\nIf `DefaultAzureCredential` doesn't work for you, see [`azure-identity reference documentation`](/python/api/azure-identity/azure.identity) or [`Set up authentication`](how-to-setup-authentication.md?tabs=sdk) for more available credentials.\n\n```python\n# Handle to the workspace\nfrom azure.ai.ml import MLClient\n\n# Authentication package\nfrom azure.identity import DefaultAzureCredential\n\ncredential = DefaultAzureCredential()\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-train-pytorch-0.md"
                }
            }
        ]
    },
    {
        "query": "How to use ur_file data type inside a command Job in AzureML SDK V2?",
        "result": "To use the `uri_file` data type inside a command job in AzureML SDK V2, specify the `type` field as `uri_file` in the job YAML configuration. Then, use the `${{inputs.<input_name>.path}}` syntax to reference the input data in the command line. For example, if the input data is named `my_input`, the command line argument would be `--input-data ${{inputs.my_input.path}}`.",
        "context": [
            {
                "page_content": "[!Notebook-python[] (~/azureml-examples-main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb?name=conda.yml)]\n\nThe specification contains some usual packages, that you'll use in your pipeline (numpy, pip), together with some Azure ML specific packages (azureml-defaults, azureml-mlflow).\n\nThe Azure ML packages aren't mandatory to run Azure ML jobs. However, adding these packages will let you interact with Azure ML for logging metrics and registering models, all inside the Azure ML job. You'll use them in the training script later in this tutorial.\n\nUse the *yaml* file to create and register this custom environment in your workspace:\n\n[!Notebook-python[] (~/azureml-examples-main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb?name=custom_env_name)]\n\n## Build the training pipeline\n\nNow that you have all assets required to run your pipeline, it's time to build the pipeline itself, using the Azure ML Python SDK v2.\n\nAzure ML pipelines are reusable ML workflows that usually consist of several components. The typical life of a component is:\n\n* Write the yaml specification of the component, or create it programmatically using `ComponentMethod`.\n* Optionally, register the component with a name and version in your workspace, to make it reusable and shareable.\n* Load that component from the pipeline code.\n* Implement the pipeline using the component's inputs, outputs and parameters\n* Submit the pipeline.\n\n## Create component 1: data prep (using programmatic definition)\n\nLet's start by creating the first component. This component handles the preprocessing of the data. The preprocessing task is performed in the *data_prep.py* Python file.\n\nFirst create a source folder for the data_prep component:\n\n[!Notebook-python[] (~/azureml-examples-main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb?name=data_prep_src_dir)]\n\nThis script performs the simple task of splitting the data into train and test datasets. \nAzure ML mounts datasets as folders to the computes, therefore, we created an auxiliary `select_first_file` function to access the data file inside the mounted input folder.\n\n[MLFlow](https://mlflow.org/docs/latest/tracking.html) will be used to log the parameters and metrics during our pipeline run.\n\n[!Notebook-python[] (~/azureml-examples-main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb?name=def-main)]\n\nNow that you have a script that can perform the desired task, create an Azure ML Component from it. \n\nYou'll use the general purpose **CommandComponent** that can run command line actions. This command line action can directly call system commands or run a script. The inputs/outputs are specified on the command line via the `${{ ... }}` notation.\n\n[!Notebook-python[] (~/azureml-examples-main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb?name=data_prep_component)]\n\nOptionally, register the component in the workspace for future re-use.\n\n\n## Create component 2: training (using yaml definition)\n\nThe second component that you'll create will consume the training and test data, train a tree based model and return the output model. You'll use Azure ML logging capabilities to record and visualize the learning progress.\n\nYou used the `CommandComponent` class to create your first component. This time you'll use the yaml definition to define the second component. Each method has its own advantages. A yaml definition can actually be checked-in along the code, and would provide a readable history tracking. The programmatic method using `CommandComponent` can be easier with built-in class documentation and code completion.\n\n\nCreate the directory for this component:\n\n[!Notebook-python[] (~/azureml-examples-main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb?name=train_src_dir)]\n\nCreate the training script in the directory:\n\n[!Notebook-python[] (~/azureml-examples-main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb?name=train.py)]\n\nAs you can see in this training script, once the model is trained, the model file is saved and registered to the workspace. Now you can use the registered model in inferencing endpoints.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-pipeline-python-sdk-138.md"
                }
            },
            {
                "page_content": "    \n    ```python\n    from azure.ai.ml.entities import Data\n    from azure.ai.ml.constants import AssetTypes\n    \n    # my_path must point to folder containing MLTable artifact (MLTable file + data\n    # Supported paths include:\n    # local: './<path>'\n    # blob:  'https://<account_name>.blob.core.windows.net/<container_name>/<path>'\n    # ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/'\n    # Datastore: 'azureml://datastores/<data_store_name>/paths/<path>'\n    \n    my_path = '<path>'\n    \n    my_data = Data(\n        path=my_path,\n        type=AssetTypes.MLTABLE,\n        description=\"<description>\",\n        name=\"<name>\",\n        version='<version>'\n    )\n    \n    ml_client.data.create_or_update(my_data)\n    ```\n\n## Use data in an experiment/job\n\n* SDK v1\n\n    ```python\n    from azureml.core import ScriptRunConfig\n    \n    src = ScriptRunConfig(source_directory=script_folder,\n                          script='train_titanic.py',\n                          # pass dataset as an input with friendly name 'titanic'\n                          arguments=['--input-data', titanic_ds.as_named_input('titanic')],\n                          compute_target=compute_target,\n                          environment=myenv)\n                                 \n    # Submit the run configuration for your training run\n    run = experiment.submit(src)\n    run.wait_for_completion(show_output=True)\n    ```\n\n* SDK v2\n\n    ```python\n    from azure.ai.ml import command\n    from azure.ai.ml.entities import Data\n    from azure.ai.ml import Input, Output\n    from azure.ai.ml.constants import AssetTypes\n    \n    # Possible Asset Types for Data:\n    # AssetTypes.URI_FILE\n    # AssetTypes.URI_FOLDER\n    # AssetTypes.MLTABLE\n    \n    # Possible Paths for Data:\n    # Blob: https://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>\n    # Datastore: azureml://datastores/paths/<folder>/<file>\n    # Data Asset: azureml:<my_data>:<version>\n    \n    my_job_inputs = {\n        \"raw_data\": Input(type=AssetTypes.URI_FOLDER, path=\"<path>\")\n    }\n    \n    my_job_outputs = {\n        \"prep_data\": Output(type=AssetTypes.URI_FOLDER, path=\"<path>\")\n    }\n    \n    job = command(\n        code=\"./src\",  # local path where the code is stored\n        command=\"python process_data.py --raw_data ${{inputs.raw_data}} --prep_data ${{outputs.prep_data}}\",\n        inputs=my_job_inputs,\n        outputs=my_job_outputs,\n        environment=\"<environment_name>:<version>\",\n        compute=\"cpu-cluster\",\n    )\n    \n    # submit the command\n    returned_job = ml_client.create_or_update(job)\n    # get a URL for the status of the job\n    returned_job.services[\"Studio\"].endpoint\n    ```\n\n## Mapping of key functionality in SDK v1 and SDK v2\n\n|Functionality in SDK v1|Rough mapping in SDK v2|\n|-|-|\n|[Method/API in SDK v1](/python/api/azureml-core/azureml.data)|[Method/API in SDK v2](/python/api/azure-ai-ml/azure.ai.ml.entities)|\n\n## Next steps\n\nFor more information, see the documentation here:\n* [Data in Azure Machine Learning](concept-data.md?tabs=uri-file-example%2Ccli-data-create-example)\n* [Create data_assets](how-to-create-data-assets.md?tabs=CLI)\n* [Read and write data in a job](how-to-read-write-data-v2.md)\n* [V2 datastore operations](/python/api/azure-ai-ml/azure.ai.ml.operations.datastoreoperations)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-assets-data-114.md"
                }
            },
            {
                "page_content": "  - Fixed a bug in batch endpoint list-jobs output.\n- `az ml component`\n  - The command group is marked as GA.\n  - Added an option to list only archived components.\n- `az ml code`\n  - This command group is removed.\n\n## 2022-03-14\n\n### Azure Machine Learning CLI (v2) v2.2.1\n\n- `az ml job`\n  - For all job types, flattened the `code` section of the YAML schema. Instead of `code.local_path` to specify the path to the source code directory, it is now just `code`\n  - For all job types, changed the schema for defining data inputs to the job in the job YAML. Instead of specifying the data path using either the `file` or `folder` fields, use the `path` field to specify either a local path, a URI to a cloud path containing the data, or a reference to an existing registered Azure ML data asset via `path: azureml:<data_name>:<data_version>`. Also specify the `type` field to clarify whether the data source is a single file (`uri_file`) or a folder (`uri_folder`). If `type` field is omitted, it defaults to `type: uri_folder`. For more information, see the section of any of the [job YAML references](reference-yaml-job-command.md) that discuss the schema for specifying input data.\n  - In the [sweep job YAML schema](reference-yaml-job-sweep.md), changed the `sampling_algorithm` field from a string to an object in order to support additional configurations for the random sampling algorithm type\n  - Removed the component job YAML schema. With this release, if you want to run a command job inside a pipeline that uses a component, just specify the component to the `component` field of the command job YAML definition. \n  - For all job types, added support for referencing the latest version of a nested asset in the job YAML configuration. When referencing a registered environment or data asset to use as input in a job, you can alias by latest version rather than having to explicitly specify the version. For example: `environment: azureml:AzureML-Minimal@latest`\n  - For pipeline jobs, introduced the `${{ parent }}` context for binding inputs and outputs between steps in a pipeline. For more information, see [Expression syntax for binding inputs and outputs between steps in a pipeline job](reference-yaml-core-syntax.md#binding-inputs-and-outputs-between-steps-in-a-pipeline-job).\n  - Added support for downloading named outputs of job via the `--output-name` argument for the `az ml job download` command\n- `az ml data`\n  - Deprecated the `az ml dataset` subgroup, now using `az ml data` instead\n  - There are two types of data that can now be created, either from a single file source (`type: uri_file`) or a folder (`type: uri_folder`). When creating the data asset, you can either specify the data source from a local file / folder or from a URI to a cloud path location. See the [data YAML schema](reference-yaml-data.md) for the full schema\n- `az ml environment`\n  - In the [environment YAML schema](reference-yaml-environment.md), renamed the `build.local_path` field to `build.path`\n  - Removed the `build.context_uri` field, the URI of the uploaded build context location will be accessible via `build.path` when the environment is returned\n- `az ml model`\n  - In the [model YAML schema](reference-yaml-model.md), `model_uri` and `local_path` fields removed and consolidated to one `path` field that can take either a local path or a cloud path URI. `model_format` field renamed to `type`; the default type is `custom_model`, but you can specify one of the other types (`mlflow_model`, `triton_model`) to use the model in no-code deployment scenarios\n  - For `az ml model create`, `--model-uri` and `--local-path` arguments removed and consolidated to one `--path` argument that can take either a local path or a cloud path URI \n  - Added the `az ml model download` command to download a model's artifact files\n- `az ml online-deployment`\n  - In the [online deployment YAML schema](reference-yaml-deployment-managed-online.md), flattened the `code` section of the `code_configuration` field. Instead of `code_configuration.code.local_path` to specify the path to the source code directory containing the scoring files, it is now just `code_configuration.code`\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/azure-machine-learning-release-notes-cli-v2-119.md"
                }
            }
        ]
    },
    {
        "query": "Generate a python code samples to deploy a custom container using ManagedOnlineEndpoint and AzureML SDK V2?",
        "result": "Here is an example Python code to deploy a custom container using ManagedOnlineEndpoint and AzureML SDK V2:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, Model, Environment, CodeConfiguration\nfrom azure.identity import DefaultAzureCredential\n\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace_name = \"<AML_WORKSPACE_NAME>\"\n\nml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)\n\nendpoint_name = \"my-endpoint\"\nendpoint = ManagedOnlineEndpoint(name=endpoint_name, auth_mode=\"key\")\nml_client.begin_create_or_update(endpoint)\n\nmodel_name = \"my-model\"\nmodel_version = 1\nmodel = Model(name=model_name, version=model_version, path=\"./models\", type=\"triton_model\")\n\nenv = Environment.from_existing_conda_environment(name=\"my-env\", conda_environment_name=\"my-env\")\n\ncode_config = CodeConfiguration(code=\"./code\", scoring_script=\"score.py\")\n\ndeployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=endpoint_name,\n    model=model,\n    environment=env,\n    code_configuration=code_config,\n    instance_type=\"Standard_NC6s_v3\",\n    instance_count=1,\n)\nml_client.begin_create_or_update(deployment)\n```\n\nThis code creates a new `ManagedOnlineEndpoint` object with the name `my-endpoint` and authentication mode `key`. It then creates a new `Model` object with the name `my-model`, version `1`, path `./models`, and type `triton_model`. It also creates a new `Environment` object from an existing Conda environment with the name `my-env`. Finally, it creates a new `ManagedOnlineDeployment` object with the name `blue`, endpoint name `my-endpoint`, model `my-model`, environment `my-env`, code configuration `./code` and `score.py`, instance type `Standard_NC6s_v3`, and instance count `1`.",
        "context": [
            {
                "page_content": "\n\n# Deploy a model for use with Cognitive Search\n\n[!INCLUDE [sdk v1](../../includes/machine-learning-sdk-v1.md)]\n\nThis article teaches you how to use Azure Machine Learning to deploy a model for use with [Azure Cognitive Search](../search/search-what-is-azure-search.md).\n\nCognitive Search performs content processing over heterogenous content, to make it queryable by humans or applications. This process can be enhanced by using a model deployed from Azure Machine Learning.\n\nAzure Machine Learning can deploy a trained model as a web service. The web service is then embedded in a Cognitive Search _skill_, which becomes part of the processing pipeline.\n\n> [!IMPORTANT]\n> The information in this article is specific to the deployment of the model. It provides information on the supported deployment configurations that allow the model to be used by Cognitive Search.\n>\n> For information on how to configure Cognitive Search to use the deployed model, see the [Build and deploy a custom skill with Azure Machine Learning](../search/cognitive-search-tutorial-aml-custom-skill.md) tutorial.\n>\n> For the sample that the tutorial is based on, see [https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill](https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill).\n\nWhen deploying a model for use with Azure Cognitive Search, the deployment must meet the following requirements:\n\n* Use Azure Kubernetes Service to host the model for inference.\n* Enable transport layer security (TLS) for the Azure Kubernetes Service. TLS is used to secure HTTPS communications between Cognitive Search and the deployed model.\n* The entry script must use the `inference_schema` package to generate an OpenAPI (Swagger) schema for the service.\n* The entry script must also accept JSON data as input, and generate JSON as output.\n\n\n## Prerequisites\n\n* An Azure Machine Learning workspace. For more information, see [Create workspace resources](quickstart-create-resources.md).\n\n* A Python development environment with the Azure Machine Learning SDK installed. For more information, see [Azure Machine Learning SDK](/python/api/overview/azure/ml/install).  \n\n* A registered model. If you do not have a model, use the example notebook at [https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill](https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill).\n\n* A general understanding of [How and where to deploy models](v1/how-to-deploy-and-where.md).\n\n## Connect to your workspace\n\nAn Azure Machine Learning workspace provides a centralized place to work with all the artifacts you create when you use Azure Machine Learning. The workspace keeps a history of all training jobs, including logs, metrics, output, and a snapshot of your scripts.\n\nTo connect to an existing workspace, use the following code:\n\n> [!IMPORTANT]\n> This code snippet expects the workspace configuration to be saved in the current directory or its parent. For more information, see [Create and manage Azure Machine Learning workspaces](how-to-manage-workspace.md). For more information on saving the configuration to file, see [Create a workspace configuration file](v1/how-to-configure-environment-v1.md).\n\n```python\nfrom azureml.core import Workspace\n\ntry:\n    # Load the workspace configuration from local cached inffo\n    ws = Workspace.from_config()\n    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n    print('Library configuration succeeded')\nexcept:\n    print('Workspace not found')\n```\n\n## Create a Kubernetes cluster\n\n**Time estimate**: Approximately 20 minutes.\n\nA Kubernetes cluster is a set of virtual machine instances (called nodes) that are used for running containerized applications.\n\nWhen you deploy a model from Azure Machine Learning to Azure Kubernetes Service, the model and all the assets needed to host it as a web service are packaged into a Docker container. This container is then deployed onto the cluster.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-model-cognitive-search-0.md"
                }
            },
            {
                "page_content": "\n# Upgrade deployment endpoints to SDK v2\n\nWe newly introduced [online endpoints](concept-endpoints.md) and batch endpoints as v2 concepts. There are several deployment funnels such as managed online endpoints, [kubernetes online endpoints](how-to-attach-kubernetes-anywhere.md) (including Azure Kubernetes Services and Arc-enabled Kubernetes) in v2, and Azure Container Instances (ACI) and Kubernetes Services (AKS) webservices in v1. In this article, we'll focus on the comparison of deploying to ACI webservices (v1) and managed online endpoints (v2).\n\nExamples in this article show how to:\n\n* Deploy your model to Azure\n* Score using the endpoint\n* Delete the webservice/endpoint\n\n## Create inference resources\n\n* SDK v1\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        # configure a model. example for registering a model \n        from azureml.core.model import Model\n        model = Model.register(ws, model_name=\"bidaf_onnx\", model_path=\"./model.onnx\")\n        \n        # configure an environment\n        from azureml.core import Environment\n        env = Environment(name='myenv')\n        python_packages = ['nltk', 'numpy', 'onnxruntime']\n        for package in python_packages:\n            env.python.conda_dependencies.add_pip_package(package)\n        \n        # configure an inference configuration with a scoring script\n        from azureml.core.model import InferenceConfig\n        inference_config = InferenceConfig(\n            environment=env,\n            source_directory=\"./source_dir\",\n            entry_script=\"./score.py\",\n        )\n        ```\n\n    1. Configure and deploy an **ACI webservice**:\n        ```python\n        from azureml.core.webservice import AciWebservice\n        \n        # defince compute resources for ACI\n        deployment_config = AciWebservice.deploy_configuration(\n            cpu_cores=0.5, memory_gb=1, auth_enabled=True\n        )\n        \n        # define an ACI webservice\n        service = Model.deploy(\n            ws,\n            \"myservice\",\n            [model],\n            inference_config,\n            deployment_config,\n            overwrite=True,\n        )\n        \n        # create the service \n        service.wait_for_deployment(show_output=True)\n        ```\n\nFor more information on registering models, see [Register a model from a local file](v1/how-to-deploy-and-where.md?tabs=python#register-a-model-from-a-local-file).\n\n* SDK v2\n\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        from azure.ai.ml.entities import Model\n        # configure a model\n        model = Model(path=\"../model-1/model/sklearn_regression_model.pkl\")\n        \n        # configure an environment\n        from azure.ai.ml.entities import Environment\n        env = Environment(\n            conda_file=\"../model-1/environment/conda.yml\",\n            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n        )\n        \n        # configure an inference configuration with a scoring script\n        from azure.ai.ml.entities import CodeConfiguration\n        code_config = CodeConfiguration(\n                code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n            )\n        ```\n\n    1. Configure and create an **online endpoint**:\n        ```python\n        import datetime\n        from azure.ai.ml.entities import ManagedOnlineEndpoint\n        \n        # create a unique endpoint name with current datetime to avoid conflicts\n        online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n        \n        # define an online endpoint\n        endpoint = ManagedOnlineEndpoint(\n            name=online_endpoint_name,\n            description=\"this is a sample online endpoint\",\n            auth_mode=\"key\",\n            tags={\"foo\": \"bar\"},\n        )\n        \n        # create the endpoint:\n        ml_client.begin_create_or_update(endpoint)\n        ```\n    \n    1. Configure and create an **online deployment**:\n        ```python\n        from azure.ai.ml.entities import ManagedOnlineDeployment\n        \n        # define a deployment\n        blue_deployment = ManagedOnlineDeployment(\n            name=\"blue\",\n            endpoint_name=online_endpoint_name,\n            model=model,\n            environment=env,\n            code_configuration=code_config,\n            instance_type=\"Standard_F2s_v2\",\n            instance_count=1,\n        )\n        \n        # create the deployment:\n        ml_client.begin_create_or_update(blue_deployment)\n        \n        # blue deployment takes 100 traffic\n        endpoint.traffic = {\"blue\": 100}\n        ml_client.begin_create_or_update(endpoint)\n        ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-deploy-endpoints-0.md"
                }
            },
            {
                "page_content": "\n1. Create a YAML configuration file for your endpoint. The following example configures the name and authentication mode of the endpoint. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-endpoint.yml` in the azureml-examples repo you cloned earlier:\n\n    __create-managed-endpoint.yaml__\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\nname: my-endpoint\nauth_mode: aml_token\n```\n\n1. Create a YAML configuration file for the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-deployment.yml` in the azureml-examples repo you cloned earlier:\n\n    > [!IMPORTANT]\n    > For Triton no-code-deployment (NCD) to work, setting **`type`** to **`triton_model\u200b`** is required, `type: triton_model\u200b`. For more information, see [CLI (v2) model YAML schema](reference-yaml-model.md).\n    >\n    > This deployment uses a Standard_NC6s_v3 VM. You may need to request a quota increase for your subscription before you can use this VM. For more information, see [NCv3-series](../virtual-machines/ncv3-series.md).\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: my-endpoint\nmodel:\n  name: sample-densenet-onnx-model\n  version: 1\n  path: ./models\n  type: triton_model\ninstance_count: 1\ninstance_type: Standard_NC6s_v3\n```\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nThis section shows how you can define a Triton deployment to deploy to a managed online endpoint using the Azure Machine Learning Python SDK (v2).\n\n> [!IMPORTANT]\n> For Triton no-code-deployment, **[testing via local endpoints](how-to-deploy-online-endpoints.md#deploy-and-debug-locally-by-using-local-endpoints)** is currently not supported.\n\n\n1. To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. \n\n    ```python \n    subscription_id = \"<SUBSCRIPTION_ID>\"\n    resource_group = \"<RESOURCE_GROUP>\"\n    workspace_name = \"<AML_WORKSPACE_NAME>\"\n    ```\n\n1. Use the following command to set the name of the endpoint that will be created. In this example, a random name is created for the endpoint:\n\n    ```python\n    import random\n\n    endpoint_name = f\"endpoint-{random.randint(0, 10000)}\"\n    ```\n\n1. We use these details above in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. Check the [configuration notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n\n    ```python \n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n\n    ml_client = MLClient(\n        DefaultAzureCredential(),\n        subscription_id,\n        resource_group,\n        workspace_name,\n    )\n    ```\n\n1. Create a `ManagedOnlineEndpoint` object to configure the endpoint. The following example configures the name and authentication mode of the endpoint. \n\n    ```python \n    from azure.ai.ml.entities import ManagedOnlineEndpoint\n\n    endpoint = ManagedOnlineEndpoint(name=endpoint_name, auth_mode=\"key\")\n    ```\n\n1. Create a `ManagedOnlineDeployment` object to configure the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step and defines a local model inline.\n\n    ```python\n    from azure.ai.ml.entities import ManagedOnlineDeployment, Model\n    \n    model_name = \"densenet-onnx-model\"\n    model_version = 1\n    \n    deployment = ManagedOnlineDeployment(\n        name=\"blue\",\n        endpoint_name=endpoint_name,\n        model=Model(\n            name=model_name, \n            version=model_version,\n            path=\"./models\",\n            type=\"triton_model\"\n        ),\n        instance_type=\"Standard_NC6s_v3\",\n        instance_count=1,\n    )\n    ``` \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-with-triton-174.md"
                }
            }
        ]
    },
    {
        "query": "Generate Python code samples to deploy a high performing serving ManagedOnlineEndpoint using Triton and AzureML SDK V2?",
        "result": "To generate Python code samples to deploy a high performing serving ManagedOnlineEndpoint using Triton and AzureML SDK V2, you can follow the steps mentioned in the \"Python SDK\" section of the given context. You need to configure workspace details and get a handle to the workspace, create an online endpoint, and configure an online deployment. You can use the `ManagedOnlineEndpoint` and `ManagedOnlineDeployment` classes to configure the endpoint and deployment. You can also use the `Environment` and `CodeConfiguration` classes to configure the environment and source code for scoring the model.",
        "context": [
            {
                "page_content": "\n# High-performance serving with Triton Inference Server (Preview)\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\nLearn how to use [NVIDIA Triton Inference Server](https://aka.ms/nvidia-triton-docs) in Azure Machine Learning with [online endpoints](concept-endpoints.md#what-are-online-endpoints).\n\nTriton is multi-framework, open-source software that is optimized for inference. It supports popular machine learning frameworks like TensorFlow, ONNX Runtime, PyTorch, NVIDIA TensorRT, and more. It can be used for your CPU or GPU workloads. No-code deployment for Triton models is supported in both [managed online endpoints and Kubernetes online endpoints](concept-endpoints.md#managed-online-endpoints-vs-kubernetes-online-endpoints).\n\nIn this article, you will learn how to deploy Triton and a model to a [managed online endpoint](concept-endpoints.md#managed-online-endpoints). Information is provided on using the CLI (command line), Python SDK v2, and Azure Machine Learning studio. \n\n> [!NOTE]\n> * [NVIDIA Triton Inference Server](https://aka.ms/nvidia-triton-docs) is an open-source third-party software that is integrated in Azure Machine Learning.\n> * While Azure Machine Learning online endpoints are generally available, _using Triton with an online endpoint/deployment is still in preview_. \n\n## Prerequisites\n\n# [Azure CLI](#tab/azure-cli)\n\n[!INCLUDE [basic prereqs](../../includes/machine-learning-cli-prereqs.md)]\n\n* A working Python 3.8 (or higher) environment. \n\n* You must have additional Python packages installed for scoring and may install them with the code below. They include:\n    * Numpy - An array and numerical computing library \n    * [Triton Inference Server Client](https://github.com/triton-inference-server/client) - Facilitates requests to the Triton Inference Server\n    * Pillow - A library for image operations\n    * Gevent - A networking library used when connecting to the Triton Server\n\n```azurecli\npip install numpy\npip install tritonclient[http]\npip install pillow\npip install gevent\n```\n\n* Access to NCv3-series VMs for your Azure subscription.\n\n    > [!IMPORTANT]\n    > You may need to request a quota increase for your subscription before you can use this series of VMs. For more information, see [NCv3-series](../virtual-machines/ncv3-series.md).\n\nNVIDIA Triton Inference Server requires a specific model repository structure, where there is a directory for each model and subdirectories for the model version. The contents of each model version subdirectory is determined by the type of the model and the requirements of the backend that supports the model. To see all the model repository structure [https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#model-files](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#model-files)\n\nThe information in this document is based on using a model stored in ONNX format, so the directory structure of the model repository is `<model-repository>/<model-name>/1/model.onnx`. Specifically, this model performs image identification.\n\n[!INCLUDE [clone repo & set defaults](../../includes/machine-learning-cli-prepare.md)]\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n[!INCLUDE [sdk](../../includes/machine-learning-sdk-v2-prereqs.md)]\n\n* A working Python 3.8 (or higher) environment.\n\n* You must have additional Python packages installed for scoring and may install them with the code below. They include:\n    * Numpy - An array and numerical computing library \n    * [Triton Inference Server Client](https://github.com/triton-inference-server/client) - Facilitates requests to the Triton Inference Server\n    * Pillow - A library for image operations\n    * Gevent - A networking library used when connecting to the Triton Server\n\n    ```azurecli\n    pip install numpy\n    pip install tritonclient[http]\n    pip install pillow\n    pip install gevent\n    ```\n\n* Access to NCv3-series VMs for your Azure subscription.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-with-triton-0.md"
                }
            },
            {
                "page_content": "\n1. Create a YAML configuration file for your endpoint. The following example configures the name and authentication mode of the endpoint. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-endpoint.yml` in the azureml-examples repo you cloned earlier:\n\n    __create-managed-endpoint.yaml__\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\nname: my-endpoint\nauth_mode: aml_token\n```\n\n1. Create a YAML configuration file for the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-deployment.yml` in the azureml-examples repo you cloned earlier:\n\n    > [!IMPORTANT]\n    > For Triton no-code-deployment (NCD) to work, setting **`type`** to **`triton_model\u200b`** is required, `type: triton_model\u200b`. For more information, see [CLI (v2) model YAML schema](reference-yaml-model.md).\n    >\n    > This deployment uses a Standard_NC6s_v3 VM. You may need to request a quota increase for your subscription before you can use this VM. For more information, see [NCv3-series](../virtual-machines/ncv3-series.md).\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: my-endpoint\nmodel:\n  name: sample-densenet-onnx-model\n  version: 1\n  path: ./models\n  type: triton_model\ninstance_count: 1\ninstance_type: Standard_NC6s_v3\n```\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nThis section shows how you can define a Triton deployment to deploy to a managed online endpoint using the Azure Machine Learning Python SDK (v2).\n\n> [!IMPORTANT]\n> For Triton no-code-deployment, **[testing via local endpoints](how-to-deploy-online-endpoints.md#deploy-and-debug-locally-by-using-local-endpoints)** is currently not supported.\n\n\n1. To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. \n\n    ```python \n    subscription_id = \"<SUBSCRIPTION_ID>\"\n    resource_group = \"<RESOURCE_GROUP>\"\n    workspace_name = \"<AML_WORKSPACE_NAME>\"\n    ```\n\n1. Use the following command to set the name of the endpoint that will be created. In this example, a random name is created for the endpoint:\n\n    ```python\n    import random\n\n    endpoint_name = f\"endpoint-{random.randint(0, 10000)}\"\n    ```\n\n1. We use these details above in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. Check the [configuration notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n\n    ```python \n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n\n    ml_client = MLClient(\n        DefaultAzureCredential(),\n        subscription_id,\n        resource_group,\n        workspace_name,\n    )\n    ```\n\n1. Create a `ManagedOnlineEndpoint` object to configure the endpoint. The following example configures the name and authentication mode of the endpoint. \n\n    ```python \n    from azure.ai.ml.entities import ManagedOnlineEndpoint\n\n    endpoint = ManagedOnlineEndpoint(name=endpoint_name, auth_mode=\"key\")\n    ```\n\n1. Create a `ManagedOnlineDeployment` object to configure the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step and defines a local model inline.\n\n    ```python\n    from azure.ai.ml.entities import ManagedOnlineDeployment, Model\n    \n    model_name = \"densenet-onnx-model\"\n    model_version = 1\n    \n    deployment = ManagedOnlineDeployment(\n        name=\"blue\",\n        endpoint_name=endpoint_name,\n        model=Model(\n            name=model_name, \n            version=model_version,\n            path=\"./models\",\n            type=\"triton_model\"\n        ),\n        instance_type=\"Standard_NC6s_v3\",\n        instance_count=1,\n    )\n    ``` \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-with-triton-174.md"
                }
            },
            {
                "page_content": "\n# [Python SDK](#tab/python)\n\n### Connect to Azure Machine Learning workspace\nConnect to Azure Machine Learning Workspace, configure workspace details, and get a handle to the workspace as follows:\n\n1. Import the required libraries:\n\n```python\n# import required libraries\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import (\n   ManagedOnlineEndpoint,\n   ManagedOnlineDeployment,\n   Model,\n   Environment,\n   CodeConfiguration,\n)\nfrom azure.identity import DefaultAzureCredential\n```\n\n2. Configure workspace details and get a handle to the workspace:\n\n```python\n# enter details of your AzureML workspace\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace = \"<AZUREML_WORKSPACE_NAME>\"\n\n# get a handle to the workspace\nml_client = MLClient(\n   DefaultAzureCredential(), subscription_id, resource_group, workspace\n)\n```\n\nFor more information, see [Deploy machine learning models to managed online endpoint using Python SDK v2](how-to-deploy-managed-online-endpoint-sdk-v2.md).\n\n### Configure online endpoint\n\n> [!TIP]\n> * `name`: The name of the endpoint. It must be unique in the Azure region. The name for an endpoint must start with an upper- or lowercase letter and only consist of '-'s and alphanumeric characters. For more information on the naming rules, see [managed online endpoint limits](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).\n> * `auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` doesn't expire, but `aml_token` does expire. For more information on authenticating, see [Authenticate to an online endpoint](how-to-authenticate-online-endpoint.md).\n\nOptionally, you can add description, tags to your endpoint.\n\n```python\n# Creating a unique endpoint name with current datetime to avoid conflicts\nimport datetime\n\nonline_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"this is a sample online endpoint\",\n    auth_mode=\"key\",\n    tags={\"foo\": \"bar\"},\n)\n```\n\n### Configure online deployment\n\nA deployment is a set of resources required for hosting the model that does the actual inferencing. We'll create a deployment for our endpoint using the `ManagedOnlineDeployment` class.\n\n> [!TIP]\n> - `name` - Name of the deployment.\n> - `endpoint_name` - Name of the endpoint to create the deployment under.\n> - `model` - The model to use for the deployment. This value can be either a reference to an existing versioned > model in the workspace or an inline model specification.\n> - `environment` - The environment to use for the deployment. This value can be either a reference to an existing > versioned environment in the workspace or an inline environment specification.\n> - `code_configuration` - the configuration for the source code and scoring script\n>     - `path`- Path to the source code directory for scoring the model\n>     - `scoring_script` - Relative path to the scoring file in the source code directory\n> - `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).\n> - `instance_count` - The number of instances to use for the deployment\n\n```python\n# create a blue deployment\nmodel = Model(name=\"tfserving-mounted\", version=\"1\", path=\"half_plus_two\")\n\nenv = Environment(\n    image=\"docker.io/tensorflow/serving:latest\",\n    inference_config={\n        \"liveness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n        \"readiness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n        \"scoring_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two:predict\"},\n    },\n)\n\nblue_deployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=online_endpoint_name,\n    model=model,\n    environment=env,\n    environment_variables={\n        \"MODEL_BASE_PATH\": \"/var/azureml-app/azureml-models/tfserving-mounted/1\",\n        \"MODEL_NAME\": \"half_plus_two\",\n    },\n    instance_type=\"Standard_DS2_v2\",\n    instance_count=1,\n)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-custom-container-812.md"
                }
            }
        ]
    },
    {
        "query": "How to use uri_folder data type inside a command Job in AzureML SDK V2?",
        "result": "To use uri_folder data type inside a command Job in AzureML SDK V2, you need to create a job specification YAML file and specify the `type` and `path` parameters in the `inputs` section of the job. The `path` parameter can point to any of the supported paths, including a path on your local computer, a path on a public http(s) server, a path on Azure Storage, a path on a Datastore, or a path to a Data Asset. Then, you can use the `Input` class to define the `type` and `mode` of the data input.",
        "context": [
            {
                "page_content": "    \n    ```python\n    from azure.ai.ml.entities import Data\n    from azure.ai.ml.constants import AssetTypes\n    \n    # my_path must point to folder containing MLTable artifact (MLTable file + data\n    # Supported paths include:\n    # local: './<path>'\n    # blob:  'https://<account_name>.blob.core.windows.net/<container_name>/<path>'\n    # ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/'\n    # Datastore: 'azureml://datastores/<data_store_name>/paths/<path>'\n    \n    my_path = '<path>'\n    \n    my_data = Data(\n        path=my_path,\n        type=AssetTypes.MLTABLE,\n        description=\"<description>\",\n        name=\"<name>\",\n        version='<version>'\n    )\n    \n    ml_client.data.create_or_update(my_data)\n    ```\n\n## Use data in an experiment/job\n\n* SDK v1\n\n    ```python\n    from azureml.core import ScriptRunConfig\n    \n    src = ScriptRunConfig(source_directory=script_folder,\n                          script='train_titanic.py',\n                          # pass dataset as an input with friendly name 'titanic'\n                          arguments=['--input-data', titanic_ds.as_named_input('titanic')],\n                          compute_target=compute_target,\n                          environment=myenv)\n                                 \n    # Submit the run configuration for your training run\n    run = experiment.submit(src)\n    run.wait_for_completion(show_output=True)\n    ```\n\n* SDK v2\n\n    ```python\n    from azure.ai.ml import command\n    from azure.ai.ml.entities import Data\n    from azure.ai.ml import Input, Output\n    from azure.ai.ml.constants import AssetTypes\n    \n    # Possible Asset Types for Data:\n    # AssetTypes.URI_FILE\n    # AssetTypes.URI_FOLDER\n    # AssetTypes.MLTABLE\n    \n    # Possible Paths for Data:\n    # Blob: https://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>\n    # Datastore: azureml://datastores/paths/<folder>/<file>\n    # Data Asset: azureml:<my_data>:<version>\n    \n    my_job_inputs = {\n        \"raw_data\": Input(type=AssetTypes.URI_FOLDER, path=\"<path>\")\n    }\n    \n    my_job_outputs = {\n        \"prep_data\": Output(type=AssetTypes.URI_FOLDER, path=\"<path>\")\n    }\n    \n    job = command(\n        code=\"./src\",  # local path where the code is stored\n        command=\"python process_data.py --raw_data ${{inputs.raw_data}} --prep_data ${{outputs.prep_data}}\",\n        inputs=my_job_inputs,\n        outputs=my_job_outputs,\n        environment=\"<environment_name>:<version>\",\n        compute=\"cpu-cluster\",\n    )\n    \n    # submit the command\n    returned_job = ml_client.create_or_update(job)\n    # get a URL for the status of the job\n    returned_job.services[\"Studio\"].endpoint\n    ```\n\n## Mapping of key functionality in SDK v1 and SDK v2\n\n|Functionality in SDK v1|Rough mapping in SDK v2|\n|-|-|\n|[Method/API in SDK v1](/python/api/azureml-core/azureml.data)|[Method/API in SDK v2](/python/api/azure-ai-ml/azure.ai.ml.entities)|\n\n## Next steps\n\nFor more information, see the documentation here:\n* [Data in Azure Machine Learning](concept-data.md?tabs=uri-file-example%2Ccli-data-create-example)\n* [Create data_assets](how-to-create-data-assets.md?tabs=CLI)\n* [Read and write data in a job](how-to-read-write-data-v2.md)\n* [V2 datastore operations](/python/api/azure-ai-ml/azure.ai.ml.operations.datastoreoperations)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-assets-data-114.md"
                }
            },
            {
                "page_content": "\n# Access data in a job\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\n> [!div class=\"op_single_selector\" title1=\"Select the version of Azure Machine Learning CLI extension you use:\"]\n> * [v1](v1/how-to-train-with-datasets.md)\n> * [v2 (current version)](how-to-read-write-data-v2.md)\n\nLearn how to read and write data for your jobs with the Azure Machine Learning Python SDK v2 and the Azure Machine Learning CLI extension v2.\n\n## Prerequisites\n\n- An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n\n- The [Azure Machine Learning SDK for Python v2](https://aka.ms/sdk-v2-install).\n\n- An Azure Machine Learning workspace\n\n## Supported paths\n\nWhen you provide a data input/output to a Job, you must specify a `path` parameter that points to the data location. This table shows both the different data locations that Azure Machine Learning supports, and examples for the `path` parameter:\n\n\n|Location  | Examples  |\n|---------|---------|\n|A path on your local computer     | `./home/username/data/my_data`         |\n|A path on a public http(s) server    |  `https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv`    |\n|A path on Azure Storage     |   `https://<account_name>.blob.core.windows.net/<container_name>/<path>` <br> `abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>`    |\n|A path on a Datastore   |   `azureml://datastores/<data_store_name>/paths/<path>`      |\n|A path to a Data Asset  |  `azureml:<my_data>:<version>`  |\n\n## Supported modes\n\nWhen you run a job with data inputs/outputs, you can specify the *mode* - for example, whether the data should be read-only mounted, or downloaded to the compute target. This table shows the possible modes for different type/mode/input/output combinations:\n\nType | Input/Output | `upload` | `download` | `ro_mount` | `rw_mount` | `direct` | `eval_download` | `eval_mount` \n------ | ------ | :---: | :---: | :---: | :---: | :---: | :---: | :---:\n`uri_folder` | Input  |   | \u2713  |  \u2713  |   | \u2713  |  | \n`uri_file`   | Input |   | \u2713  |  \u2713  |   | \u2713  |  | \n`mltable`   | Input |   | \u2713  |  \u2713  |   | \u2713  | \u2713 | \u2713\n`uri_folder` | Output  | \u2713  |   |    | \u2713  |   |  | \n`uri_file`   | Output | \u2713  |   |    | \u2713  |   |  | \n`mltable`   | Output | \u2713  |   |    | \u2713  | \u2713  |  | \n\n> [!NOTE]\n> `eval_download` and `eval_mount` are unique to `mltable`. The `ro_mount` is the default mode for MLTable. In some scenarios, however, an MLTable can yield files that are not necessarily co-located with the MLTable file in storage. Alternately, an `mltable` can subset or shuffle the data located in the storage resource. That view becomes visible only if the engine actually evaluates the MLTable file. These modes provide that view of the files.\n\n\n## Read data in a job\n\n# [Azure CLI](#tab/cli)\n\nCreate a job specification YAML file (`<file-name>.yml`). In the `inputs` section of the job, specify:\n\n1. The `type`; whether the data is a specific file (`uri_file`), a folder location (`uri_folder`), or an `mltable`.\n1. The `path` of your data location; any of the paths outlined in the [Supported Paths](#supported-paths) section will work.\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n\n# Possible Paths for Data:\n# Blob: https://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>\n# Datastore: azureml://datastores/paths/<folder>/<file>\n# Data Asset: azureml:<my_data>:<version>\n\ncommand: |\n  ls ${{inputs.my_data}}\ncode: <folder where code is located>\ninputs:\n  my_data:\n    type: <type> # uri_file, uri_folder, mltable\n    path: <path>\nenvironment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\ncompute: azureml:cpu-cluster\n```\n\nNext, run in the CLI\n\n```azurecli\naz ml job create -f <file-name>.yml\n```\n\n# [Python SDK](#tab/python)\n\nUse the `Input` class to define:\n\n1. The `type`; whether the data is a specific file (`uri_file`), a folder location (`uri_folder`), or an `mltable`.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-read-write-data-v2-0.md"
                }
            },
            {
                "page_content": "| `mode` | string | Mode of how the data should be delivered to the compute target. <br><br> For read-only mount (`ro_mount`), the data will be consumed as a mount path. A folder will be mounted as a folder and a file will be mounted as a file. Azure ML will resolve the input to the mount path. <br><br> For `download` mode the data will be downloaded to the compute target. Azure ML will resolve the input to the downloaded path. <br><br> If you only want the URL of the storage location of the data artifact(s) rather than mounting or downloading the data itself, you can use the `direct` mode. This will pass in the URL of the storage location as the job input. In this case, you're fully responsible for handling credentials to access the storage. | `ro_mount`, `download`, `direct` | `ro_mount` |\n\n### Job outputs\n\n| Key | Type | Description | Allowed values | Default value |\n| --- | ---- | ----------- | -------------- | ------------- |\n| `type` | string | The type of job output. For the default `uri_folder` type, the output will correspond to a folder. | `uri_folder` | `uri_folder` |\n| `path` | string | The path to the data to use as input. This can be specified in a few ways: <br><br> - A local path to the data source file or folder, for example, `path: ./iris.csv`. The data will get uploaded during job submission. <br><br> - A URI of a cloud path to the file or folder to use as the input. Supported URI types are `azureml`, `https`, `wasbs`, `abfss`, `adl`. For more information on how to use the `azureml://` URI format, see [Core yaml syntax](reference-yaml-core-syntax.md). <br><br> - An existing registered Azure ML data asset to use as the input. To reference a registered data asset, use the `azureml:<data_name>:<data_version>` syntax or `azureml:<data_name>@latest` (to reference the latest version of that data asset), for example, `path: azureml:cifar10-data:1` or `path: azureml:cifar10-data@latest`. | | |\n| `mode` | string | Mode of how output file(s) will get delivered to the destination storage. For read-write mount mode (`rw_mount`) the output directory will be a mounted directory. For upload mode the file(s) written will get uploaded at the end of the job. | `rw_mount`, `upload` | `rw_mount` |\n\n## Remarks\n\nThe `az ml schedule` command can be used for managing Azure Machine Learning models.\n\n## Examples\n\nExamples are available in the [examples GitHub repository](https://github.com/Azure/azureml-examples/tree/main/cli/schedules). A couple are shown below.\n\n## YAML: Schedule with recurrence pattern\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/schedule.schema.json\nname: simple_recurrence_job_schedule\ndisplay_name: Simple recurrence job schedule\ndescription: a simple hourly recurrence job schedule\n\ntrigger:\n  type: recurrence\n  frequency: day #can be minute, hour, day, week, month\n  interval: 1 #every day\n  schedule:\n    hours: [4,5,10,11,12]\n    minutes: [0,30]\n  start_time: \"2022-07-10T10:00:00\" # optional - default will be schedule creation time\n  time_zone: \"Pacific Standard Time\" # optional - default will be UTC\n\ncreate_job: ./simple-pipeline-job.yml\n# create_job: azureml:simple-pipeline-job\n\n```\n\n## YAML: Schedule with cron expression\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/schedule.schema.json\nname: simple_cron_job_schedule\ndisplay_name: Simple cron job schedule\ndescription: a simple hourly cron job schedule\n\ntrigger:\n  type: cron\n  expression: \"0 * * * *\"\n  start_time: \"2022-07-10T10:00:00\" # optional - default will be schedule creation time\n  time_zone: \"Pacific Standard Time\" # optional - default will be UTC\n\n# create_job: azureml:simple-pipeline-job\ncreate_job: ./simple-pipeline-job.yml\n```\n\n## Appendix\n\n### Timezone\n\nCurrent schedule supports the following timezones. The key can be used directly in the Python SDK, while the value can be used in the YAML job. The table is organized by UTC(Coordinated Universal Time).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-yaml-schedule-87.md"
                }
            }
        ]
    },
    {
        "query": "How can I run a command job that uses train.py as entry script in AzureML SDK V2 ?",
        "result": "To run a command job that uses train.py as entry script in AzureML SDK V2, you can use the `command()` function. First, create an environment object that specifies the environment for the job. Then, create a command job object that specifies the code, command, environment, and compute target for the job. Finally, submit the job using the `create_or_update()` method of the `jobs` object. Here's an example:\n\n```python\nfrom azure.ai.ml import MLClient, command\nfrom azure.ai.ml.entities import Environment\nfrom azure.identity import DefaultAzureCredential\n\n# Connect to the workspace\nml_client = MLClient.from_config(DefaultAzureCredential())\n\n# Set up the environment\nenv = Environment(\n    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n    conda_file=\"pytorch-env.yml\",\n    name=\"pytorch-env\"\n)\n\n# Define the command job\ncommand_job = command(\n    code=\"./src\",\n    command=\"train.py\",\n    environment=env,\n    compute=\"cpu-cluster\",\n)\n\n# Submit the job\nreturned_job = ml_client.jobs.create_or_update(command_job)\n```",
        "context": [
            {
                "page_content": "\n# Upgrade parallel run step to SDK v2\n\nIn SDK v2, \"Parallel run step\" is consolidated into job concept as `parallel job`. Parallel job keeps the same target to empower users to accelerate their job execution by distributing repeated tasks on powerful multi-nodes compute clusters. On top of parallel run step, v2 parallel job provides extra benefits:\n\n- Flexible interface, which allows user to define multiple custom inputs and outputs for your parallel job. You can connect them with other steps to consume or manage their content in your entry script \n- Simplify input schema, which replaces `Dataset` as input by using v2 `data asset` concept. You can easily use your local files or blob directory URI as the inputs to parallel job.\n- More powerful features are under developed in v2 parallel job only. For example, resume the failed/canceled parallel job to continue process the failed or unprocessed mini-batches by reusing the successful result to save duplicate effort.\n\nTo upgrade your current sdk v1 parallel run step to v2, you'll need to \n\n- Use `parallel_run_function` to create parallel job by replacing `ParallelRunConfig` and `ParallelRunStep` in v1.\n- Upgrade your v1 pipeline to v2. Then invoke your v2 parallel job as a step in your v2 pipeline. See [how to upgrade pipeline from v1 to v2](migrate-to-v2-execution-pipeline.md) for the details about pipeline upgrade.\n\n> Note: User __entry script__ is compatible between v1 parallel run step and v2 parallel job. So you can keep using the same entry_script.py when you upgrade your parallel run job.\n\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2. In the following examples, we'll build a parallel job to predict input data in a pipelines job. You'll see how to build a parallel job, and how to use it in a pipeline job for both SDK v1 and SDK v2.\n\n## Prerequisites\n\n - Prepare your SDK v2 environment: [Install the Azure ML SDK v2 for Python](/python/api/overview/azure/ai-ml-readme)\n - Understand the basis of SDK v2 pipeline: [How to create Azure ML pipeline with Python SDK v2](how-to-create-component-pipeline-python.md)\n\n\n## Create parallel step\n* SDK v1\n\n    ```python\n    # Create the configuration to wrap the inference script\n    from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n    \n    parallel_run_config = ParallelRunConfig(\n        source_directory=scripts_folder,\n        entry_script=script_file,\n        mini_batch_size=PipelineParameter(name=\"batch_size_param\", default_value=\"5\"),\n        error_threshold=10,\n        output_action=\"append_row\",\n        append_row_file_name=\"mnist_outputs.txt\",\n        environment=batch_env,\n        compute_target=compute_target,\n        process_count_per_node=PipelineParameter(name=\"process_count_param\", default_value=2),\n        node_count=2\n    )\n    \n    # Create the Parallel run step\n    parallelrun_step = ParallelRunStep(\n        name=\"predict-digits-mnist\",\n        parallel_run_config=parallel_run_config,\n        inputs=[ input_mnist_ds_consumption ],\n        output=output_dir,\n        allow_reuse=False\n    )\n    ```\n\n* SDK v2\n\n    ```python\n    # parallel job to process file data\n    file_batch_inference = parallel_run_function(\n        name=\"file_batch_score\",\n        display_name=\"Batch Score with File Dataset\",\n        description=\"parallel component for batch score\",\n        inputs=dict(\n            job_data_path=Input(\n                type=AssetTypes.MLTABLE,\n                description=\"The data to be split and scored in parallel\",\n            )\n        ),\n        outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n        input_data=\"${{inputs.job_data_path}}\",\n        instance_count=2,\n        mini_batch_size=\"1\",\n        mini_batch_error_threshold=1,\n        max_concurrency_per_instance=1,\n        task=RunFunction(\n            code=\"./src\",\n            entry_script=\"file_batch_inference.py\",\n            program_arguments=\"--job_output_path ${{outputs.job_output_path}}\",\n            environment=\"azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:1\",\n        ),\n    )\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-execution-parallel-run-step-0.md"
                }
            },
            {
                "page_content": "\n# Upgrade script run to SDK v2\n\nIn SDK v2, \"experiments\" and \"runs\" are consolidated into jobs.\n\nA job has a type. Most jobs are command jobs that run a `command`, like `python main.py`. What runs in a job is agnostic to any programming language, so you can run `bash` scripts, invoke `python` interpreters, run a bunch of `curl` commands, or anything else.\n\nTo upgrade, you'll need to change your code for submitting jobs to SDK v2. What you run _within_ the job doesn't need to be upgraded to SDK v2. However, it's recommended to remove any code specific to Azure ML from your model training scripts. This separation allows for an easier transition between local and cloud and is considered best practice for mature MLOps. In practice, this means removing `azureml.*` lines of code. Model logging and tracking code should be replaced with MLflow. For more details, see [how to use MLflow in v2](how-to-use-mlflow-cli-runs.md).\n\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\n\n## Submit a script run\n\n* SDK v1\n\n    ```python\n    from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig\n    \n    # connect to the workspace\n    ws = Workspace.from_config()\n    \n    # define and configure the experiment\n    experiment = Experiment(workspace=ws, name='day1-experiment-train')\n    config = ScriptRunConfig(source_directory='./src',\n                                script='train.py',\n                                compute_target='cpu-cluster')\n    \n    # set up pytorch environment\n    env = Environment.from_conda_specification(\n        name='pytorch-env',\n        file_path='pytorch-env.yml')\n    config.run_config.environment = env\n    \n    run = experiment.submit(config)\n    \n    aml_url = run.get_portal_url()\n    print(aml_url)\n    ```\n\n* SDK v2\n\n    ```python\n    #import required libraries\n    from azure.ai.ml import MLClient, command\n    from azure.ai.ml.entities import Environment\n    from azure.identity import DefaultAzureCredential\n    \n    #connect to the workspace\n    ml_client = MLClient.from_config(DefaultAzureCredential())\n    \n    # set up pytorch environment\n    env = Environment(\n        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n        conda_file=\"pytorch-env.yml\",\n        name=\"pytorch-env\"\n    )\n    \n    # define the command\n    command_job = command(\n        code=\"./src\",\n        command=\"train.py\",\n        environment=env,\n        compute=\"cpu-cluster\",\n    )\n    \n    returned_job = ml_client.jobs.create_or_update(command_job)\n    returned_job\n    ```\n\n## Mapping of key functionality in v1 and v2\n\n|Functionality in SDK v1|Rough mapping in SDK v2|\n|-|-|\n|[experiment.submit](/python/api/azureml-core/azureml.core.experiment.experiment#azureml-core-experiment-experiment-submit)|[MLCLient.jobs.create_or_update](/python/api/azure-ai-ml/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update)|\n|[ScriptRunConfig()](/python/api/azureml-core/azureml.core.scriptrunconfig#constructor)|[command()](/python/api/azure-ai-ml/azure.ai.ml#azure-ai-ml-command)|\n\n## Next steps\n\nFor more information, see:\n\n* [V1 - Experiment](/python/api/azureml-core/azureml.core.experiment)\n* [V2 - Command Job](/python/api/azure-ai-ml/azure.ai.ml#azure-ai-ml-command)\n* [Train models with the Azure ML Python SDK v2](how-to-train-sdk.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-command-job-0.md"
                }
            },
            {
                "page_content": "\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nDeclare `job_data_path` as one of the inputs. Bind it to `input_data` attribute.\n\n```python\n# parallel task to process file data\nfile_batch_inference = parallel_run_function(\n    name=\"file_batch_score\",\n    display_name=\"Batch Score with File Dataset\",\n    description=\"parallel component for batch score\",\n    inputs=dict(\n        job_data_path=Input(\n            type=AssetTypes.MLTABLE,\n            description=\"The data to be split and scored in parallel\",\n        )\n    ),\n    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n    input_data=\"${{inputs.job_data_path}}\",\n    instance_count=2,\n    max_concurrency_per_instance=1,\n    mini_batch_size=\"1\",\n    mini_batch_error_threshold=1,\n    retry_settings=dict(max_retries=2, timeout=60),\n    logging_level=\"DEBUG\",\n    task=RunFunction(\n        code=\"./src\",\n        entry_script=\"file_batch_inference.py\",\n        program_arguments=\"--job_output_path ${{outputs.job_output_path}}\",\n        environment=\"azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n    ),\n)\n```\n\n\nOnce you have the partition setting defined, you can configure parallel setting by using two attributes below:\n\n| Attribute name | Type | Description | Default value |\n|:-|--|:-|--|\n| `instance_count` | integer | The number of nodes to use for the job. | 1 |\n| `max_concurrency_per_instance` | integer | The number of processors on each node. | For a GPU compute, the default value is 1.<br> For a CPU compute, the default value is the number of cores. |\n\nThese two attributes work together with your specified compute cluster.\n\n:::image type=\"content\" source=\"./media/how-to-use-parallel-job-in-pipeline/how-distributed-data-works-in-parallel-job.png\" alt-text=\"Diagram showing how distributed data works in parallel job.\" lightbox =\"./media/how-to-use-parallel-job-in-pipeline/how-distributed-data-works-in-parallel-job.png\":::\n\nSample code to set two attributes:\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\n\ndisplay_name: iris-batch-prediction-using-parallel\ndescription: The hello world pipeline job with inline parallel job\ntags:\n  tag: tagvalue\n  owner: sdkteam\n\nsettings:\n  default_compute: azureml:cpu-cluster\n\njobs:\n  batch_prediction:\n    type: parallel\n    compute: azureml:cpu-cluster\n    inputs:\n      input_data: \n        type: mltable\n        path: ./neural-iris-mltable\n        mode: direct\n      score_model: \n        type: uri_folder\n        path: ./iris-model\n        mode: download\n    outputs:\n      job_output_file:\n        type: uri_file\n        mode: rw_mount\n\n    input_data: ${{inputs.input_data}}\n    mini_batch_size: \"10kb\"\n    resources:\n        instance_count: 2\n    max_concurrency_per_instance: 2\n\n    logging_level: \"DEBUG\"\n    mini_batch_error_threshold: 5\n    retry_settings:\n      max_retries: 2\n      timeout: 60\n\n    task:\n      type: run_function\n      code: \"./script\"\n      entry_script: iris_prediction.py\n      environment:\n        name: \"prs-env\"\n        version: 1\n        image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n        conda_file: ./environment/environment_parallel.yml\n      program_arguments: >-\n        --model ${{inputs.score_model}}\n        --error_threshold 5\n        --allowed_failed_percent 30\n        --task_overhead_timeout 1200\n        --progress_update_timeout 600\n        --first_task_creation_timeout 600\n        --copy_logs_to_parent True\n        --resource_monitor_interva 20\n      append_row_to: ${{outputs.job_output_file}}\n\n```\n\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\n# parallel task to process file data\nfile_batch_inference = parallel_run_function(\n    name=\"file_batch_score\",\n    display_name=\"Batch Score with File Dataset\",\n    description=\"parallel component for batch score\",\n    inputs=dict(\n        job_data_path=Input(\n            type=AssetTypes.MLTABLE,\n            description=\"The data to be split and scored in parallel\",\n        )\n    ),\n    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n    input_data=\"${{inputs.job_data_path}}\",\n    instance_count=2,\n    max_concurrency_per_instance=1,\n    mini_batch_size=\"1\",\n    mini_batch_error_threshold=1,\n    retry_settings=dict(max_retries=2, timeout=60),\n    logging_level=\"DEBUG\",\n    task=RunFunction(\n        code=\"./src\",\n        entry_script=\"file_batch_inference.py\",\n        program_arguments=\"--job_output_path ${{outputs.job_output_path}}\",\n        environment=\"azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n    ),\n)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-parallel-job-in-pipeline-121.md"
                }
            }
        ]
    },
    {
        "query": "Generate a python code sample to show how to Autoscale of ManagedOnlineEndpoint using AzureML SDK V2?",
        "result": "Here is an example of how to autoscale a ManagedOnlineEndpoint using AzureML SDK V2:\n\n1. Create a `ManagedOnlineEndpoint` object to configure the endpoint.\n2. Create a `ManagedOnlineDeployment` object to configure the deployment.\n3. Use `mon_client.autoscale_settings.create_or_update()` to create an autoscale setting.\n4. Use `az monitor autoscale rule create` to add rules to the autoscale setting.\n5. Use `az monitor autoscale profile create` to create a weekend profile.\n6. Use `az monitor autoscale delete` to delete the autoscaling profile.\n7. Use `az ml online-endpoint delete` to delete the endpoint.",
        "context": [
            {
                "page_content": "\n# Upgrade deployment endpoints to SDK v2\n\nWe newly introduced [online endpoints](concept-endpoints.md) and batch endpoints as v2 concepts. There are several deployment funnels such as managed online endpoints, [kubernetes online endpoints](how-to-attach-kubernetes-anywhere.md) (including Azure Kubernetes Services and Arc-enabled Kubernetes) in v2, and Azure Container Instances (ACI) and Kubernetes Services (AKS) webservices in v1. In this article, we'll focus on the comparison of deploying to ACI webservices (v1) and managed online endpoints (v2).\n\nExamples in this article show how to:\n\n* Deploy your model to Azure\n* Score using the endpoint\n* Delete the webservice/endpoint\n\n## Create inference resources\n\n* SDK v1\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        # configure a model. example for registering a model \n        from azureml.core.model import Model\n        model = Model.register(ws, model_name=\"bidaf_onnx\", model_path=\"./model.onnx\")\n        \n        # configure an environment\n        from azureml.core import Environment\n        env = Environment(name='myenv')\n        python_packages = ['nltk', 'numpy', 'onnxruntime']\n        for package in python_packages:\n            env.python.conda_dependencies.add_pip_package(package)\n        \n        # configure an inference configuration with a scoring script\n        from azureml.core.model import InferenceConfig\n        inference_config = InferenceConfig(\n            environment=env,\n            source_directory=\"./source_dir\",\n            entry_script=\"./score.py\",\n        )\n        ```\n\n    1. Configure and deploy an **ACI webservice**:\n        ```python\n        from azureml.core.webservice import AciWebservice\n        \n        # defince compute resources for ACI\n        deployment_config = AciWebservice.deploy_configuration(\n            cpu_cores=0.5, memory_gb=1, auth_enabled=True\n        )\n        \n        # define an ACI webservice\n        service = Model.deploy(\n            ws,\n            \"myservice\",\n            [model],\n            inference_config,\n            deployment_config,\n            overwrite=True,\n        )\n        \n        # create the service \n        service.wait_for_deployment(show_output=True)\n        ```\n\nFor more information on registering models, see [Register a model from a local file](v1/how-to-deploy-and-where.md?tabs=python#register-a-model-from-a-local-file).\n\n* SDK v2\n\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        from azure.ai.ml.entities import Model\n        # configure a model\n        model = Model(path=\"../model-1/model/sklearn_regression_model.pkl\")\n        \n        # configure an environment\n        from azure.ai.ml.entities import Environment\n        env = Environment(\n            conda_file=\"../model-1/environment/conda.yml\",\n            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n        )\n        \n        # configure an inference configuration with a scoring script\n        from azure.ai.ml.entities import CodeConfiguration\n        code_config = CodeConfiguration(\n                code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n            )\n        ```\n\n    1. Configure and create an **online endpoint**:\n        ```python\n        import datetime\n        from azure.ai.ml.entities import ManagedOnlineEndpoint\n        \n        # create a unique endpoint name with current datetime to avoid conflicts\n        online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n        \n        # define an online endpoint\n        endpoint = ManagedOnlineEndpoint(\n            name=online_endpoint_name,\n            description=\"this is a sample online endpoint\",\n            auth_mode=\"key\",\n            tags={\"foo\": \"bar\"},\n        )\n        \n        # create the endpoint:\n        ml_client.begin_create_or_update(endpoint)\n        ```\n    \n    1. Configure and create an **online deployment**:\n        ```python\n        from azure.ai.ml.entities import ManagedOnlineDeployment\n        \n        # define a deployment\n        blue_deployment = ManagedOnlineDeployment(\n            name=\"blue\",\n            endpoint_name=online_endpoint_name,\n            model=model,\n            environment=env,\n            code_configuration=code_config,\n            instance_type=\"Standard_F2s_v2\",\n            instance_count=1,\n        )\n        \n        # create the deployment:\n        ml_client.begin_create_or_update(blue_deployment)\n        \n        # blue deployment takes 100 traffic\n        endpoint.traffic = {\"blue\": 100}\n        ml_client.begin_create_or_update(endpoint)\n        ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-deploy-endpoints-0.md"
                }
            },
            {
                "page_content": "\n1. Create a YAML configuration file for your endpoint. The following example configures the name and authentication mode of the endpoint. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-endpoint.yml` in the azureml-examples repo you cloned earlier:\n\n    __create-managed-endpoint.yaml__\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\nname: my-endpoint\nauth_mode: aml_token\n```\n\n1. Create a YAML configuration file for the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-deployment.yml` in the azureml-examples repo you cloned earlier:\n\n    > [!IMPORTANT]\n    > For Triton no-code-deployment (NCD) to work, setting **`type`** to **`triton_model\u200b`** is required, `type: triton_model\u200b`. For more information, see [CLI (v2) model YAML schema](reference-yaml-model.md).\n    >\n    > This deployment uses a Standard_NC6s_v3 VM. You may need to request a quota increase for your subscription before you can use this VM. For more information, see [NCv3-series](../virtual-machines/ncv3-series.md).\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: my-endpoint\nmodel:\n  name: sample-densenet-onnx-model\n  version: 1\n  path: ./models\n  type: triton_model\ninstance_count: 1\ninstance_type: Standard_NC6s_v3\n```\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nThis section shows how you can define a Triton deployment to deploy to a managed online endpoint using the Azure Machine Learning Python SDK (v2).\n\n> [!IMPORTANT]\n> For Triton no-code-deployment, **[testing via local endpoints](how-to-deploy-online-endpoints.md#deploy-and-debug-locally-by-using-local-endpoints)** is currently not supported.\n\n\n1. To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. \n\n    ```python \n    subscription_id = \"<SUBSCRIPTION_ID>\"\n    resource_group = \"<RESOURCE_GROUP>\"\n    workspace_name = \"<AML_WORKSPACE_NAME>\"\n    ```\n\n1. Use the following command to set the name of the endpoint that will be created. In this example, a random name is created for the endpoint:\n\n    ```python\n    import random\n\n    endpoint_name = f\"endpoint-{random.randint(0, 10000)}\"\n    ```\n\n1. We use these details above in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. Check the [configuration notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n\n    ```python \n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n\n    ml_client = MLClient(\n        DefaultAzureCredential(),\n        subscription_id,\n        resource_group,\n        workspace_name,\n    )\n    ```\n\n1. Create a `ManagedOnlineEndpoint` object to configure the endpoint. The following example configures the name and authentication mode of the endpoint. \n\n    ```python \n    from azure.ai.ml.entities import ManagedOnlineEndpoint\n\n    endpoint = ManagedOnlineEndpoint(name=endpoint_name, auth_mode=\"key\")\n    ```\n\n1. Create a `ManagedOnlineDeployment` object to configure the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step and defines a local model inline.\n\n    ```python\n    from azure.ai.ml.entities import ManagedOnlineDeployment, Model\n    \n    model_name = \"densenet-onnx-model\"\n    model_version = 1\n    \n    deployment = ManagedOnlineDeployment(\n        name=\"blue\",\n        endpoint_name=endpoint_name,\n        model=Model(\n            name=model_name, \n            version=model_version,\n            path=\"./models\",\n            type=\"triton_model\"\n        ),\n        instance_type=\"Standard_NC6s_v3\",\n        instance_count=1,\n    )\n    ``` \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-with-triton-174.md"
                }
            },
            {
                "page_content": "\n# [Python](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python \nmon_client.autoscale_settings.create_or_update(\n    resource_group, \n    autoscale_settings_name, \n    parameters = {\n        \"location\" : endpoint.location,\n        \"target_resource_uri\" : deployment.id,\n        \"profiles\" : [\n            AutoscaleProfile(\n                name=\"Default\",\n                capacity={\n                    \"minimum\" : 2, \n                    \"maximum\" : 2,\n                    \"default\" : 2\n                },\n                recurrence = Recurrence(\n                    frequency = \"Week\", \n                    schedule = RecurrentSchedule(\n                        time_zone = \"Pacific Standard Time\", \n                        days = [\"Saturday\", \"Sunday\"], \n                        hours = [], \n                        minutes = []\n                    )\n                )\n            )\n        ]\n    }\n)\n``` \n\n# [Studio](#tab/azure-studio)\n\nFrom the bottom of the page, select __+ Add a scale condition__. On the new scale condition, use the following information to populate the fields:\n \n* Select __Scale to a specific instance count__.\n* Set the __Instance count__ to __2__.\n* Set the __Schedule__ to __Repeat specific days__.\n* Set the schedule to __Repeat every__ __Saturday__ and __Sunday__.\n\n:::image type=\"content\" source=\"media/how-to-autoscale-endpoints/schedule-rules.png\" lightbox=\"media/how-to-autoscale-endpoints/schedule-rules.png\" alt-text=\"Screenshot showing schedule-based rules.\":::\n\n\n## Delete resources\n\nIf you are not going to use your deployments, delete them:\n\n# [Azure CLI](#tab/azure-cli)\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```azurecli\n# Note: this is based on the deploy-managed-online-endpoint.sh: it just adds autoscale settings in the end\nset -e\n\n#set the endpoint name from the how-to-deploy excercise\n# <set_endpoint_deployment_name>\n# set your existing endpoint name\nENDPOINT_NAME=your-endpoint-name\nDEPLOYMENT_NAME=blue\n# </set_endpoint_deployment_name>\n\nexport ENDPOINT_NAME=autoscale-endpt-`echo $RANDOM`\n\n# create endpoint and deployment\naz ml online-endpoint create --name $ENDPOINT_NAME -f endpoints/online/managed/sample/endpoint.yml\naz ml online-deployment create --name blue --endpoint $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment.yml --all-traffic\naz ml online-endpoint show -n $ENDPOINT_NAME\n\n# <set_other_env_variables>\n# ARM id of the deployment\nDEPLOYMENT_RESOURCE_ID=$(az ml online-deployment show -e $ENDPOINT_NAME -n $DEPLOYMENT_NAME -o tsv --query \"id\")\n# ARM id of the deployment. todo: change to --query \"id\"\nENDPOINT_RESOURCE_ID=$(az ml online-endpoint show -n $ENDPOINT_NAME -o tsv --query \"properties.\\\"azureml.onlineendpointid\\\"\")\n# set a unique name for autoscale settings for this deployment. The below will append a random number to make the name unique.\nAUTOSCALE_SETTINGS_NAME=autoscale-$ENDPOINT_NAME-$DEPLOYMENT_NAME-`echo $RANDOM`\n# </set_other_env_variables>\n\n# create autoscale settings. Note if you followed the how-to-deploy doc example, the instance count would have been 1. Now after applying this poilcy, it will scale up 2 (since min count and count are 2).\n# <create_autoscale_profile>\naz monitor autoscale create \\\n  --name $AUTOSCALE_SETTINGS_NAME \\\n  --resource $DEPLOYMENT_RESOURCE_ID \\\n  --min-count 2 --max-count 5 --count 2\n# </create_autoscale_profile>\n\n# Add rule to default profile: scale up if cpu util > 70 %\n# <scale_out_on_cpu_util>\naz monitor autoscale rule create \\\n  --autoscale-name $AUTOSCALE_SETTINGS_NAME \\\n  --condition \"CpuUtilizationPercentage > 70 avg 5m\" \\\n  --scale out 2\n# </scale_out_on_cpu_util>\n\n# Add rule to default profile: scale down if cpu util < 25 %\n# <scale_in_on_cpu_util>\naz monitor autoscale rule create \\\n  --autoscale-name $AUTOSCALE_SETTINGS_NAME \\\n  --condition \"CpuUtilizationPercentage < 25 avg 5m\" \\\n  --scale in 1\n# </scale_in_on_cpu_util>\n\n# add rule to default profile: scale up based on avg. request latency (endpoint metric)\n# <scale_up_on_request_latency>\naz monitor autoscale rule create \\\n --autoscale-name $AUTOSCALE_SETTINGS_NAME \\\n --condition \"RequestLatency > 70 avg 5m\" \\\n --scale out 1 \\\n --resource $ENDPOINT_RESOURCE_ID\n# </scale_up_on_request_latency>\n\n#create weekend profile: scale to 2 nodes in weekend\n# <weekend_profile>\naz monitor autoscale profile create \\\n  --name weekend-profile \\\n  --autoscale-name $AUTOSCALE_SETTINGS_NAME \\\n  --min-count 2 --count 2 --max-count 2 \\\n  --recurrence week sat sun --timezone \"Pacific Standard Time\" \n# </weekend_profile>\n\n# <delete_endpoint>\n# delete the autoscaling profile\naz monitor autoscale delete -n \"$AUTOSCALE_SETTINGS_NAME\"\n\n# delete the endpoint\naz ml online-endpoint delete --name $ENDPOINT_NAME --yes --no-wait\n# </delete_endpoint>\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-autoscale-endpoints-924.md"
                }
            }
        ]
    },
    {
        "query": "Code Samples to create a GPU cluster type AmlCompute using AzureML SDK V2?",
        "result": "Here is an example code to create a GPU cluster type AmlCompute using AzureML SDK V2:\n\n```python\nfrom azure.ai.ml.entities import AmlCompute\n\ngpu_compute_target = \"gpu-cluster\"\n\ntry:\n    # let's see if the compute target already exists\n    gpu_cluster = ml_client.compute.get(gpu_compute_target)\n    print(\n        f\"You already have a cluster named {gpu_compute_target}, we'll reuse it as is.\"\n    )\n\nexcept Exception:\n    print(\"Creating a new gpu compute target...\")\n\n    # Let's create the Azure ML compute object with the intended parameters\n    gpu_cluster = AmlCompute(\n        # Name assigned to the compute cluster\n        name=\"gpu-cluster\",\n        # Azure ML Compute is the on-demand VM service\n        type=\"amlcompute\",\n        # VM Family\n        size=\"STANDARD_NC6\",\n        # Minimum running nodes when there is no job running\n        min_instances=0,\n        # Nodes in cluster\n        max_instances=4,\n        # How many seconds will the node running after the job termination\n        idle_time_before_scale_down=180,\n        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n        tier=\"Dedicated\",\n    )\n\n    # Now, we pass the object to MLClient's create_or_update method\n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()\n\nprint(\n    f\"AMLCompute with name {gpu_cluster.name} is created, the compute size is {gpu_cluster.size}\"\n)\n```",
        "context": [
            {
                "page_content": "\nIf you prefer to use a browser to sign in and authenticate, you should uncomment the following code and use it instead.\n\n```python\n# Handle to the workspace\n# from azure.ai.ml import MLClient\n\n# Authentication package\n# from azure.identity import InteractiveBrowserCredential\n# credential = InteractiveBrowserCredential()\n```\n\nNext, get a handle to the workspace by providing your Subscription ID, Resource Group name, and workspace name. To find these parameters:\n\n1. Look for your workspace name in the upper-right corner of the Azure Machine Learning studio toolbar.\n2. Select your workspace name to show your Resource Group and Subscription ID.\n3. Copy the values for Resource Group and Subscription ID into the code.\n\n```python\n# Get a handle to the workspace\nml_client = MLClient(\n    credential=credential,\n    subscription_id=\"<SUBSCRIPTION_ID>\",\n    resource_group_name=\"<RESOURCE_GROUP>\",\n    workspace_name=\"<AML_WORKSPACE_NAME>\",\n)\n```\n\nThe result of running this script is a workspace handle that you'll use to manage other resources and jobs.\n\n> [!NOTE]\n> - Creating `MLClient` will not connect the client to the workspace. The client initialization is lazy and will wait for the first time it needs to make a call. In this article, this will happen during compute creation.\n\n### Create a compute resource to run the job\n\nAzureML needs a compute resource to run a job. This resource can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n\nIn the following example script, we provision a Linux [`compute cluster`](./how-to-create-attach-compute-cluster.md?tabs=python). You can see the [`Azure Machine Learning pricing`](https://azure.microsoft.com/pricing/details/machine-learning/) page for the full list of VM sizes and prices. Since we need a GPU cluster for this example, let's pick a *STANDARD_NC6* model and create an AzureML compute.\n\n```python\nfrom azure.ai.ml.entities import AmlCompute\n\ngpu_compute_target = \"gpu-cluster\"\n\ntry:\n    # let's see if the compute target already exists\n    gpu_cluster = ml_client.compute.get(gpu_compute_target)\n    print(\n        f\"You already have a cluster named {gpu_compute_target}, we'll reuse it as is.\"\n    )\n\nexcept Exception:\n    print(\"Creating a new gpu compute target...\")\n\n    # Let's create the Azure ML compute object with the intended parameters\n    gpu_cluster = AmlCompute(\n        # Name assigned to the compute cluster\n        name=\"gpu-cluster\",\n        # Azure ML Compute is the on-demand VM service\n        type=\"amlcompute\",\n        # VM Family\n        size=\"STANDARD_NC6\",\n        # Minimum running nodes when there is no job running\n        min_instances=0,\n        # Nodes in cluster\n        max_instances=4,\n        # How many seconds will the node running after the job termination\n        idle_time_before_scale_down=180,\n        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n        tier=\"Dedicated\",\n    )\n\n    # Now, we pass the object to MLClient's create_or_update method\n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()\n\nprint(\n    f\"AMLCompute with name {gpu_cluster.name} is created, the compute size is {gpu_cluster.size}\"\n)\n```\n\n### Create a job environment\n\nTo run an AzureML job, you'll need an environment. An AzureML [environment](concept-environments.md) encapsulates the dependencies (such as software runtime and libraries) needed to run your machine learning training script on your compute resource. This environment is similar to a Python environment on your local machine.\n\nAzureML allows you to either use a curated (or ready-made) environment\u2014useful for common training and inference scenarios\u2014or create a custom environment using a Docker image or a Conda configuration. \n\nIn this article, you'll reuse the curated AzureML environment `AzureML-tensorflow-2.7-ubuntu20.04-py38-cuda11-gpu`. You'll use the latest version of this environment using the `@latest` directive.\n\n```python\ncurated_env_name = \"AzureML-tensorflow-2.7-ubuntu20.04-py38-cuda11-gpu@latest\"\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-train-tensorflow-56.md"
                }
            },
            {
                "page_content": "If `DefaultAzureCredential` doesn't work for you, see [`azure-identity reference documentation`](/python/api/azure-identity/azure.identity) or [`Set up authentication`](how-to-setup-authentication.md?tabs=sdk) for more available credentials.\n\n```python\n# Handle to the workspace\nfrom azure.ai.ml import MLClient\n\n# Authentication package\nfrom azure.identity import DefaultAzureCredential\n\ncredential = DefaultAzureCredential()\n```\n\nIf you prefer to use a browser to sign in and authenticate, you should uncomment the following code and use it instead.\n\n```python\n# Handle to the workspace\n# from azure.ai.ml import MLClient\n\n# Authentication package\n# from azure.identity import InteractiveBrowserCredential\n# credential = InteractiveBrowserCredential()\n```\n\nNext, get a handle to the workspace by providing your Subscription ID, Resource Group name, and workspace name. To find these parameters:\n\n1. Look for your workspace name in the upper-right corner of the Azure Machine Learning studio toolbar.\n2. Select your workspace name to show your Resource Group and Subscription ID.\n3. Copy the values for Resource Group and Subscription ID into the code.\n\n```python\n# Get a handle to the workspace\nml_client = MLClient(\n    credential=credential,\n    subscription_id=\"<SUBSCRIPTION_ID>\",\n    resource_group_name=\"<RESOURCE_GROUP>\",\n    workspace_name=\"<AML_WORKSPACE_NAME>\",\n)\n```\n\nThe result of running this script is a workspace handle that you'll use to manage other resources and jobs.\n\n> [!NOTE]\n> - Creating `MLClient` will not connect the client to the workspace. The client initialization is lazy and will wait for the first time it needs to make a call. In this article, this will happen during compute creation.\n\n### Create a compute resource to run the job\n\nAzureML needs a compute resource to run a job. This resource can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n\nIn the following example script, we provision a Linux [`compute cluster`](./how-to-create-attach-compute-cluster.md?tabs=python). You can see the [`Azure Machine Learning pricing`](https://azure.microsoft.com/pricing/details/machine-learning/) page for the full list of VM sizes and prices. Since we need a GPU cluster for this example, let's pick a *STANDARD_NC6* model and create an AzureML compute.\n\n```python\nfrom azure.ai.ml.entities import AmlCompute\n\ngpu_compute_target = \"gpu-cluster\"\n\ntry:\n    # let's see if the compute target already exists\n    gpu_cluster = ml_client.compute.get(gpu_compute_target)\n    print(\n        f\"You already have a cluster named {gpu_compute_target}, we'll reuse it as is.\"\n    )\n\nexcept Exception:\n    print(\"Creating a new gpu compute target...\")\n\n    # Let's create the Azure ML compute object with the intended parameters\n    gpu_cluster = AmlCompute(\n        # Name assigned to the compute cluster\n        name=\"gpu-cluster\",\n        # Azure ML Compute is the on-demand VM service\n        type=\"amlcompute\",\n        # VM Family\n        size=\"STANDARD_NC6\",\n        # Minimum running nodes when there is no job running\n        min_instances=0,\n        # Nodes in cluster\n        max_instances=4,\n        # How many seconds will the node running after the job termination\n        idle_time_before_scale_down=180,\n        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n        tier=\"Dedicated\",\n    )\n\n    # Now, we pass the object to MLClient's create_or_update method\n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()\n\nprint(\n    f\"AMLCompute with name {gpu_cluster.name} is created, the compute size is {gpu_cluster.size}\"\n)\n```\n\n### Create a job environment\n\nTo run an AzureML job, you'll need an environment. An AzureML [environment](concept-environments.md) encapsulates the dependencies (such as software runtime and libraries) needed to run your machine learning training script on your compute resource. This environment is similar to a Python environment on your local machine.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-train-keras-47.md"
                }
            },
            {
                "page_content": "This tutorial uses the NCsv3-series (with V100 GPUs) as this type of compute target leverages multiple GPUs to speed up training. Additionally, you can set up multiple nodes to take advantage of parallelism when tuning hyperparameters for your model.\n\nThe following code creates a GPU compute of size `Standard_NC24s_v3` with four nodes.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nCreate a .yml file with the following configuration.\n\n```yml\n$schema: https://azuremlschemas.azureedge.net/latest/amlCompute.schema.json \nname: gpu-cluster\ntype: amlcompute\nsize: Standard_NC24s_v3\nmin_instances: 0\nmax_instances: 4\nidle_time_before_scale_down: 120\n```\n\nTo create the compute, you run the following CLI v2 command with the path to your .yml file, workspace name, resource group and subscription ID.\n\n```azurecli\naz ml compute create -f [PATH_TO_YML_FILE] --workspace-name [YOUR_AZURE_WORKSPACE] --resource-group [YOUR_AZURE_RESOURCE_GROUP] --subscription [YOUR_AZURE_SUBSCRIPTION]\n```\n\nThe created compute can be provided using `compute` key in the `automl` task configuration yaml: \n\n```yaml\ncompute: azureml:gpu-cluster\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nfrom azure.ai.ml.entities import AmlCompute\ncompute_name = \"gpu-cluster\"\ncluster_basic = AmlCompute(\n    name=compute_name,\n    type=\"amlcompute\",\n    size=\"Standard_NC24s_v3\",\n    min_instances=0,\n    max_instances=4,\n    idle_time_before_scale_down=120,\n)\nml_client.begin_create_or_update(cluster_basic)\n```\nThis compute is used later while creating the task specific `automl` job.\n\n\n## Experiment setup\n\nYou can use an Experiment to track your model training jobs.\n\n# [Azure CLI](#tab/cli)\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\nExperiment name can be provided using `experiment_name` key as follows: \n\n```yaml\nexperiment_name: dpv2-cli-automl-image-object-detection-experiment\n```\n\n# [Python SDK](#tab/python)\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nExperiment name is used later while creating the task specific `automl` job.\n```python\nexp_name = \"dpv2-image-object-detection-experiment\"\n```\n\n## Visualize input data\n\nOnce you have the input image data prepared in [JSONL](https://jsonlines.org/) (JSON Lines) format, you can visualize the ground truth bounding boxes for an image. To do so, be sure you have `matplotlib` installed.\n\n```\n%pip install --upgrade matplotlib\n```\n```python\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.patches as patches\nfrom PIL import Image as pil_image\nimport numpy as np\nimport json\nimport os\n\ndef plot_ground_truth_boxes(image_file, ground_truth_boxes):\n    # Display the image\n    plt.figure()\n    img_np = mpimg.imread(image_file)\n    img = pil_image.fromarray(img_np.astype(\"uint8\"), \"RGB\")\n    img_w, img_h = img.size\n\n    fig,ax = plt.subplots(figsize=(12, 16))\n    ax.imshow(img_np)\n    ax.axis(\"off\")\n\n    label_to_color_mapping = {}\n\n    for gt in ground_truth_boxes:\n        label = gt[\"label\"]\n\n        xmin, ymin, xmax, ymax =  gt[\"topX\"], gt[\"topY\"], gt[\"bottomX\"], gt[\"bottomY\"]\n        topleft_x, topleft_y = img_w * xmin, img_h * ymin\n        width, height = img_w * (xmax - xmin), img_h * (ymax - ymin)\n\n        if label in label_to_color_mapping:\n            color = label_to_color_mapping[label]\n        else:\n            # Generate a random color. If you want to use a specific color, you can use something like \"red\".\n            color = np.random.rand(3)\n            label_to_color_mapping[label] = color\n\n        # Display bounding box\n        rect = patches.Rectangle((topleft_x, topleft_y), width, height,\n                                 linewidth=2, edgecolor=color, facecolor=\"none\")\n        ax.add_patch(rect)\n\n        # Display label\n        ax.text(topleft_x, topleft_y - 10, label, color=color, fontsize=20)\n\n    plt.show()\n\ndef plot_ground_truth_boxes_jsonl(image_file, jsonl_file):\n    image_base_name = os.path.basename(image_file)\n    ground_truth_data_found = False\n    with open(jsonl_file) as fp:\n        for line in fp.readlines():\n            line_json = json.loads(line)\n            filename = line_json[\"image_url\"]\n            if image_base_name in filename:\n                ground_truth_data_found = True\n                plot_ground_truth_boxes(image_file, line_json[\"label\"])\n                break\n    if not ground_truth_data_found:\n        print(\"Unable to find ground truth information for image: {}\".format(image_file))\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-auto-train-image-models-70.md"
                }
            }
        ]
    },
    {
        "query": "How to connect to An existing AzureML workspace greg_ws_1 using AzureML SDK V2?",
        "result": "You can connect to an existing AzureML workspace greg_ws_1 using AzureML SDK V2 by using the following code:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import Workspace\nfrom azure.identity import DefaultAzureCredential\n\nws = MLClient.from_config(\n    DefaultAzureCredential()\n)\n```\n\nThis code will load the workspace configuration from the local cached info and connect to the workspace.",
        "context": [
            {
                "page_content": "1. When you're finished configuring networking, you can select __Review + Create__, or advance to the optional __Advanced__ configuration.\n\n\n### Advanced\n\nBy default, metadata for the workspace is stored in an Azure Cosmos DB instance that Microsoft maintains. This data is encrypted using Microsoft-managed keys.\n\nTo limit the data that Microsoft collects on your workspace, select __High business impact workspace__ in the portal, or set `hbi_workspace=true ` in Python. For more information on this setting, see [Encryption at rest](concept-data-encryption.md#encryption-at-rest).\n\n> [!IMPORTANT]\t\n> Selecting high business impact can only be done when creating a workspace. You cannot change this setting after workspace creation.\t\n\n#### Use your own data encryption key\n\nYou can provide your own key for data encryption. Doing so creates the Azure Cosmos DB instance that stores metadata in your Azure subscription. For more information, see [Customer-managed keys](concept-customer-managed-keys.md).\n\n\nUse the following steps to provide your own key:\n\n> [!IMPORTANT]\t\n> Before following these steps, you must first perform the following actions:\t\n>\n> Follow the steps in [Configure customer-managed keys](how-to-setup-customer-managed-keys.md) to:\n> * Register the Azure Cosmos DB provider\n> * Create and configure an Azure Key Vault\n> * Generate a key\n\n# [Python SDK](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\n\nfrom azure.ai.ml.entities import Workspace, CustomerManagedKey\n\n# specify the workspace details\nws = Workspace(\n    name=\"my_workspace\",\n    location=\"eastus\",\n    display_name=\"My workspace\",\n    description=\"This example shows how to create a workspace\",\n    customer_managed_key=CustomerManagedKey(\n        key_vault=\"/subscriptions/<SUBSCRIPTION_ID>/resourcegroups/<RESOURCE_GROUP>/providers/microsoft.keyvault/vaults/<VAULT_NAME>\"\n        key_uri=\"<KEY-IDENTIFIER>\"\n    )\n    tags=dict(purpose=\"demo\")\n)\n\nml_client.workspaces.begin_create(ws)\n```\n\n# [Portal](#tab/azure-portal)\n\n1. Select __Customer-managed keys__, and then select __Click to select key__.\n\n    :::image type=\"content\" source=\"media/how-to-manage-workspace/advanced-workspace.png\" alt-text=\"Customer-managed keys\":::\n\n1. On the __Select key from Azure Key Vault__ form, select an existing Azure Key Vault, a key that it contains, and the version of the key. This key is used to encrypt the data stored in Azure Cosmos DB. Finally, use the __Select__ button to use this key.\n\n   :::image type=\"content\" source=\"media/how-to-manage-workspace/select-key-vault.png\" alt-text=\"Select the key\":::\n\n\n### Download a configuration file\n\nIf you'll be running your code on a [compute instance](quickstart-create-resources.md), skip this step.  The compute instance will create and store copy of this file for you.\n\nIf you plan to use code on your local environment that references this workspace, download the file:\n1. Select your workspace in [Azure studio](https://ml.azure.com)\n1. At the top right, select the workspace name, then select  **Download config.json**\n\n   ![Download config.json](./media/how-to-manage-workspace/configure.png)\n\nPlace the file into  the directory structure with your Python scripts or Jupyter Notebooks. It can be in the same directory, a subdirectory named *.azureml*, or in a parent directory. When you create a compute instance, this file is added to the correct directory on the VM for you.\n\n## Connect to a workspace\n\nWhen running machine learning tasks using the SDK, you require a MLClient object that specifies the connection to your workspace. You can create an `MLClient` object from parameters, or with a configuration file.\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n* **With a configuration file:** This code will read the contents of the configuration file to find your workspace.  You'll get a prompt to sign in if you aren't already authenticated.\n\n    ```python\n    from azure.ai.ml import MLClient\n    \n    # read the config from the current directory\n    ws_from_config = MLClient.from_config()\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-workspace-231.md"
                }
            },
            {
                "page_content": "\n\n# Deploy a model for use with Cognitive Search\n\n[!INCLUDE [sdk v1](../../includes/machine-learning-sdk-v1.md)]\n\nThis article teaches you how to use Azure Machine Learning to deploy a model for use with [Azure Cognitive Search](../search/search-what-is-azure-search.md).\n\nCognitive Search performs content processing over heterogenous content, to make it queryable by humans or applications. This process can be enhanced by using a model deployed from Azure Machine Learning.\n\nAzure Machine Learning can deploy a trained model as a web service. The web service is then embedded in a Cognitive Search _skill_, which becomes part of the processing pipeline.\n\n> [!IMPORTANT]\n> The information in this article is specific to the deployment of the model. It provides information on the supported deployment configurations that allow the model to be used by Cognitive Search.\n>\n> For information on how to configure Cognitive Search to use the deployed model, see the [Build and deploy a custom skill with Azure Machine Learning](../search/cognitive-search-tutorial-aml-custom-skill.md) tutorial.\n>\n> For the sample that the tutorial is based on, see [https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill](https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill).\n\nWhen deploying a model for use with Azure Cognitive Search, the deployment must meet the following requirements:\n\n* Use Azure Kubernetes Service to host the model for inference.\n* Enable transport layer security (TLS) for the Azure Kubernetes Service. TLS is used to secure HTTPS communications between Cognitive Search and the deployed model.\n* The entry script must use the `inference_schema` package to generate an OpenAPI (Swagger) schema for the service.\n* The entry script must also accept JSON data as input, and generate JSON as output.\n\n\n## Prerequisites\n\n* An Azure Machine Learning workspace. For more information, see [Create workspace resources](quickstart-create-resources.md).\n\n* A Python development environment with the Azure Machine Learning SDK installed. For more information, see [Azure Machine Learning SDK](/python/api/overview/azure/ml/install).  \n\n* A registered model. If you do not have a model, use the example notebook at [https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill](https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill).\n\n* A general understanding of [How and where to deploy models](v1/how-to-deploy-and-where.md).\n\n## Connect to your workspace\n\nAn Azure Machine Learning workspace provides a centralized place to work with all the artifacts you create when you use Azure Machine Learning. The workspace keeps a history of all training jobs, including logs, metrics, output, and a snapshot of your scripts.\n\nTo connect to an existing workspace, use the following code:\n\n> [!IMPORTANT]\n> This code snippet expects the workspace configuration to be saved in the current directory or its parent. For more information, see [Create and manage Azure Machine Learning workspaces](how-to-manage-workspace.md). For more information on saving the configuration to file, see [Create a workspace configuration file](v1/how-to-configure-environment-v1.md).\n\n```python\nfrom azureml.core import Workspace\n\ntry:\n    # Load the workspace configuration from local cached inffo\n    ws = Workspace.from_config()\n    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n    print('Library configuration succeeded')\nexcept:\n    print('Workspace not found')\n```\n\n## Create a Kubernetes cluster\n\n**Time estimate**: Approximately 20 minutes.\n\nA Kubernetes cluster is a set of virtual machine instances (called nodes) that are used for running containerized applications.\n\nWhen you deploy a model from Azure Machine Learning to Azure Kubernetes Service, the model and all the assets needed to host it as a web service are packaged into a Docker container. This container is then deployed onto the cluster.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-model-cognitive-search-0.md"
                }
            },
            {
                "page_content": "\n## Load/connect to workspace using config file\n\n* SDK v1\n\n    ```python\n    from azureml.core import Workspace\n    \n    ws = Workspace.from_config()\n    ws.get_details()\n    ```\n\n* SDK v2\n\n    ```python\n    from azure.ai.ml import MLClient\n    from azure.ai.ml.entities import Workspace\n    from azure.identity import DefaultAzureCredential\n    \n    ws = MLClient.from_config(\n        DefaultAzureCredential()\n    )\n    ```\n\n## Mapping of key functionality in SDK v1 and SDK v2\n\n|Functionality in SDK v1|Rough mapping in SDK v2|\n|-|-|\n|[Method/API in SDK v1 (use links to ref docs)](/python/api/azureml-core/azureml.core.workspace.workspace)|[Method/API in SDK v2 (use links to ref docs)](/python/api/azure-ai-ml/azure.ai.ml.entities.workspace)|\n\n## Related documents\n\nFor more information, see:\n\n* [What is a workspace?](concept-workspace.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-resource-workspace-139.md"
                }
            }
        ]
    },
    {
        "query": "Generate a Python code samples to deploy a managedonlineendpoint using AzureML rest API?",
        "result": "To deploy a managed online endpoint using AzureML REST API, you can create a YAML configuration file for the endpoint and another for the deployment. Then, you can use the `az ml online-endpoint create` command to create the endpoint and the `az ml online-deployment create` command to create the deployment. Alternatively, you can use the Azure Machine Learning Python SDK to define a Triton deployment to deploy to a managed online endpoint. You can use the `ManagedOnlineEndpoint` and `ManagedOnlineDeployment` classes to configure the endpoint and deployment, respectively.",
        "context": [
            {
                "page_content": "In the last section, you'll deploy a model from registry to an online endpoint in a workspace. You can choose to deploy any workspace you have access to in your organization, provided the location of the workspace is one of the locations supported by the registry. This capability is helpful if you trained a model in a `dev` workspace and now need to deploy the model to `test` or `prod` workspace, while preserving the lineage information around the code, environment and data used to train the model.\n\nOnline endpoints let you deploy models and submit inference requests through the REST APIs. For more information, see [How to deploy and score a machine learning model by using an online endpoint](how-to-deploy-online-endpoints.md).\n\n# [Azure CLI](#tab/cli)\n\nCreate an online endpoint. \n\n```azurecli\naz ml online-endpoint create --name reg-ep-1234\n```\n\nUpdate the `model:` line `deploy.yml` available in the `cli/jobs/pipelines-with-components/nyc_taxi_data_regression` folder to refer the model name and version from the pervious step. Create an online deployment to the online endpoint. The `deploy.yml` is shown below for reference.\n\n```YAML\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\nname: demo\nendpoint_name: reg-ep-1234\nmodel: azureml://registries/<registry-name>/models/nyc-taxi-model/versions/1\ninstance_type: Standard_DS2_v2\ninstance_count: 1\n```\nCreate the online deployment. The deployment takes several minutes to complete. \n\n```azurecli\naz ml online-deployment create --file deploy.yml --all-traffic\n```\n\nFetch the scoring URI and submit a sample scoring request. Sample data for the scoring request is available in the `scoring-data.json` in the `cli/jobs/pipelines-with-components/nyc_taxi_data_regression` folder. \n\n```azurecli\nENDPOINT_KEY=$(az ml online-endpoint get-credentials -n reg-ep-1234 -o tsv --query primaryKey)\nSCORING_URI=$(az ml online-endpoint show -n $ep_name -o tsv --query scoring_uri)\ncurl --request POST \"$SCORING_URI\" --header \"Authorization: Bearer $ENDPOINT_KEY\" --header 'Content-Type: application/json' --data @./scoring-data.json\n```\n\n> [!TIP]\n> * `curl` command works only on Linux.\n> * If you have not configured the default workspace and resource group as explained in the prerequisites section, you will need to specify the `--workspace-name` and `--resource-group` parameters for the `az ml online-endpoint` and `az ml online-deployment` commands to work.\n\n# [Python SDK](#tab/python)\n\nCreate an online endpoint. \n\n```python\nonline_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"this is a sample online endpoint for mlflow model\",\n    auth_mode=\"key\"\n)\nml_client_workspace.begin_create_or_update(endpoint)\n```\n\nMake sure you have the `mlflow_model_from_registry` model object from the previous section or fetch the model from the registry using `ml_client_registry.models.get()` method. Pass it to the deployment configuration object and create the online deployment. The deployment takes several minutes to complete. Set all traffic to be routed to the new deployment. \n\n```python\ndemo_deployment = ManagedOnlineDeployment(\n    name=\"demo\",\n    endpoint_name=online_endpoint_name,\n    model=mlflow_model_from_registry,\n    instance_type=\"Standard_F4s_v2\",\n    instance_count=1\n)\nml_client_workspace.online_deployments.begin_create_or_update(demo_deployment)\n\nendpoint.traffic = {\"demo\": 100}\nml_client_workspace.begin_create_or_update(endpoint)\n```\n\nSubmit a sample scoring request using the sample data file `scoring-data.json`. This file is available in the `cli/jobs/pipelines-with-components/nyc_taxi_data_regression` folder.\n\n```azurecli\n# test the  deployment with some sample data\nml_client_workspace.online_endpoints.invoke(\n    endpoint_name=online_endpoint_name,\n    deployment_name=\"demo\",\n    request_file=parent_dir + \"/scoring-data.json\"\n)\n```\n\n\n## Clean up resources\n\nIf you aren't going use the deployment, you should delete it to reduce costs. The following example deletes the endpoint and all the underlying deployments:\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-share-models-pipelines-across-workspaces-with-registries-567.md"
                }
            },
            {
                "page_content": "With Azure Machine Learning, you can use automated ML to build a Python model and have it converted to the ONNX format. Once the models are in the ONNX format, they can be run on a variety of platforms and devices. Learn more about [accelerating ML models with ONNX](concept-onnx.md).\n\nSee how to convert to ONNX format [in this Jupyter notebook example](https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/tutorials/automl-with-azureml/classification-bank-marketing-all-features). Learn which [algorithms are supported in ONNX](how-to-configure-auto-train.md#supported-algorithms).\n\nThe ONNX runtime also supports C#, so you can use the model built automatically in your C# apps without any need for recoding or any of the network latencies that REST endpoints introduce. Learn more about [using an AutoML ONNX model in a .NET application with ML.NET](./how-to-use-automl-onnx-model-dotnet.md) and [inferencing ONNX models with the ONNX runtime C# API](https://onnxruntime.ai/docs/api/csharp-api.html). \n\n## Next steps\n\nThere are multiple resources to get you up and running with AutoML. \n\n### Tutorials/ how-tos\nTutorials are end-to-end introductory examples of AutoML scenarios.\n\n+ **For a code first experience**, follow the [Tutorial: Train an object detection model with AutoML and Python](tutorial-auto-train-image-models.md)\n\n+ **For a low or no-code experience**, see the [Tutorial: Train a classification model with no-code AutoML in Azure Machine Learning studio](tutorial-first-experiment-automated-ml.md).\n   \nHow-to articles provide additional detail into what functionality automated ML offers. For example, \n\n+ Configure the settings for automatic training experiments\n    + [Without code in the Azure Machine Learning studio](how-to-use-automated-ml-for-ml-models.md). \n    + [With the Python SDK](how-to-configure-auto-train.md).\n\n+  Learn how to [train computer vision models with Python](how-to-auto-train-image-models.md).\n\n+  Learn how to [view the generated code from your automated ML models](how-to-generate-automl-training-code.md).\n   \n### Jupyter notebook samples \n\nReview detailed code examples and use cases in the [GitHub notebook repository for automated machine learning samples](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/automl-standalone-jobs.\n\n\n### Python SDK reference\n\nDeepen your expertise of SDK design patterns and class specifications with the [AutoML Job class reference documentation](/python/api/azure-ai-ml/azure.ai.ml.automl). \n\n> [!Note]\n> Automated machine learning capabilities are also available in other Microsoft solutions such as, \n[ML.NET](/dotnet/machine-learning/automl-overview), \n[HDInsight](../hdinsight/spark/apache-spark-run-machine-learning-automl.md), [Power BI](/power-bi/service-machine-learning-automated) and [SQL Server](https://cloudblogs.microsoft.com/sqlserver/2019/01/09/how-to-automate-machine-learning-on-sql-server-2019-big-data-clusters/)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-automated-ml-176.md"
                }
            },
            {
                "page_content": "\n1. Create a YAML configuration file for your endpoint. The following example configures the name and authentication mode of the endpoint. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-endpoint.yml` in the azureml-examples repo you cloned earlier:\n\n    __create-managed-endpoint.yaml__\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\nname: my-endpoint\nauth_mode: aml_token\n```\n\n1. Create a YAML configuration file for the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-deployment.yml` in the azureml-examples repo you cloned earlier:\n\n    > [!IMPORTANT]\n    > For Triton no-code-deployment (NCD) to work, setting **`type`** to **`triton_model\u200b`** is required, `type: triton_model\u200b`. For more information, see [CLI (v2) model YAML schema](reference-yaml-model.md).\n    >\n    > This deployment uses a Standard_NC6s_v3 VM. You may need to request a quota increase for your subscription before you can use this VM. For more information, see [NCv3-series](../virtual-machines/ncv3-series.md).\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: my-endpoint\nmodel:\n  name: sample-densenet-onnx-model\n  version: 1\n  path: ./models\n  type: triton_model\ninstance_count: 1\ninstance_type: Standard_NC6s_v3\n```\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nThis section shows how you can define a Triton deployment to deploy to a managed online endpoint using the Azure Machine Learning Python SDK (v2).\n\n> [!IMPORTANT]\n> For Triton no-code-deployment, **[testing via local endpoints](how-to-deploy-online-endpoints.md#deploy-and-debug-locally-by-using-local-endpoints)** is currently not supported.\n\n\n1. To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. \n\n    ```python \n    subscription_id = \"<SUBSCRIPTION_ID>\"\n    resource_group = \"<RESOURCE_GROUP>\"\n    workspace_name = \"<AML_WORKSPACE_NAME>\"\n    ```\n\n1. Use the following command to set the name of the endpoint that will be created. In this example, a random name is created for the endpoint:\n\n    ```python\n    import random\n\n    endpoint_name = f\"endpoint-{random.randint(0, 10000)}\"\n    ```\n\n1. We use these details above in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. Check the [configuration notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n\n    ```python \n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n\n    ml_client = MLClient(\n        DefaultAzureCredential(),\n        subscription_id,\n        resource_group,\n        workspace_name,\n    )\n    ```\n\n1. Create a `ManagedOnlineEndpoint` object to configure the endpoint. The following example configures the name and authentication mode of the endpoint. \n\n    ```python \n    from azure.ai.ml.entities import ManagedOnlineEndpoint\n\n    endpoint = ManagedOnlineEndpoint(name=endpoint_name, auth_mode=\"key\")\n    ```\n\n1. Create a `ManagedOnlineDeployment` object to configure the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step and defines a local model inline.\n\n    ```python\n    from azure.ai.ml.entities import ManagedOnlineDeployment, Model\n    \n    model_name = \"densenet-onnx-model\"\n    model_version = 1\n    \n    deployment = ManagedOnlineDeployment(\n        name=\"blue\",\n        endpoint_name=endpoint_name,\n        model=Model(\n            name=model_name, \n            version=model_version,\n            path=\"./models\",\n            type=\"triton_model\"\n        ),\n        instance_type=\"Standard_NC6s_v3\",\n        instance_count=1,\n    )\n    ``` \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-with-triton-174.md"
                }
            }
        ]
    },
    {
        "query": "Generate Python Code Samples to create two model deployments using a registered model and existing ManagedOnlineEndpoint assigning 50% traffic to each deploying using AzureML SDK V2?",
        "result": "Here's an example Python code to create two model deployments using a registered model and existing ManagedOnlineEndpoint assigning 50% traffic to each deploying using AzureML SDK V2:\n\n```python\nfrom azure.ai.ml.entities import (\n    Model,\n    Environment,\n    CodeConfiguration,\n    ManagedOnlineDeployment,\n    ManagedOnlineEndpoint,\n)\nfrom azure.identity import DefaultAzureCredential\nfrom azure.ai.ml import MLClient\n\n# configure workspace details and get a handle to the workspace\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace = \"<AZUREML_WORKSPACE_NAME>\"\nml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)\n\n# define the model, environment, and code configuration\nmodel = Model(name=\"<MODEL_NAME>\", version=\"<MODEL_VERSION>\")\nenv = Environment(name=\"<ENVIRONMENT_NAME>\")\ncode_config = CodeConfiguration(code=\"<CODE_DIRECTORY>\", scoring_script=\"<SCORING_SCRIPT>\")\n\n# define the endpoint and create it\nendpoint_name = \"<ENDPOINT_NAME>\"\nendpoint = ManagedOnlineEndpoint(name=endpoint_name)\nml_client.online_endpoints.begin_create_or_update(endpoint).result()\n\n# define the two deployments and create them\ndeployment_1 = ManagedOnlineDeployment(\n    name=\"<DEPLOYMENT_NAME_1>\",\n    endpoint_name=endpoint_name,\n    model=model,\n    environment=env,\n    code_configuration=code_config,\n    instance_type=\"<INSTANCE_TYPE>\",\n    instance_count=<INSTANCE_COUNT>,\n)\ndeployment_2 = ManagedOnlineDeployment(\n    name=\"<DEPLOYMENT_NAME_2>\",\n    endpoint_name=endpoint_name,\n    model=model,\n    environment=env,\n    code_configuration=code_config,\n    instance_type=\"<INSTANCE_TYPE>\",\n    instance_count=<INSTANCE_COUNT>,\n)\nml_client.online_deployments.begin_create_or_update(deployment_1).result()\nml_client.online_deployments.begin_create_or_update(deployment_2).result()\n\n# assign 50% traffic to each deployment\nendpoint.traffic = {\"<DEPLOYMENT_NAME_1>\": 50, \"<DEPLOYMENT_NAME_2>\": 50}\nml_client.online_endpoints.begin_create_or_update(endpoint).result()\n```\n\nReplace the placeholders with your own values before running the code.",
        "context": [
            {
                "page_content": "\n# Upgrade deployment endpoints to SDK v2\n\nWe newly introduced [online endpoints](concept-endpoints.md) and batch endpoints as v2 concepts. There are several deployment funnels such as managed online endpoints, [kubernetes online endpoints](how-to-attach-kubernetes-anywhere.md) (including Azure Kubernetes Services and Arc-enabled Kubernetes) in v2, and Azure Container Instances (ACI) and Kubernetes Services (AKS) webservices in v1. In this article, we'll focus on the comparison of deploying to ACI webservices (v1) and managed online endpoints (v2).\n\nExamples in this article show how to:\n\n* Deploy your model to Azure\n* Score using the endpoint\n* Delete the webservice/endpoint\n\n## Create inference resources\n\n* SDK v1\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        # configure a model. example for registering a model \n        from azureml.core.model import Model\n        model = Model.register(ws, model_name=\"bidaf_onnx\", model_path=\"./model.onnx\")\n        \n        # configure an environment\n        from azureml.core import Environment\n        env = Environment(name='myenv')\n        python_packages = ['nltk', 'numpy', 'onnxruntime']\n        for package in python_packages:\n            env.python.conda_dependencies.add_pip_package(package)\n        \n        # configure an inference configuration with a scoring script\n        from azureml.core.model import InferenceConfig\n        inference_config = InferenceConfig(\n            environment=env,\n            source_directory=\"./source_dir\",\n            entry_script=\"./score.py\",\n        )\n        ```\n\n    1. Configure and deploy an **ACI webservice**:\n        ```python\n        from azureml.core.webservice import AciWebservice\n        \n        # defince compute resources for ACI\n        deployment_config = AciWebservice.deploy_configuration(\n            cpu_cores=0.5, memory_gb=1, auth_enabled=True\n        )\n        \n        # define an ACI webservice\n        service = Model.deploy(\n            ws,\n            \"myservice\",\n            [model],\n            inference_config,\n            deployment_config,\n            overwrite=True,\n        )\n        \n        # create the service \n        service.wait_for_deployment(show_output=True)\n        ```\n\nFor more information on registering models, see [Register a model from a local file](v1/how-to-deploy-and-where.md?tabs=python#register-a-model-from-a-local-file).\n\n* SDK v2\n\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        from azure.ai.ml.entities import Model\n        # configure a model\n        model = Model(path=\"../model-1/model/sklearn_regression_model.pkl\")\n        \n        # configure an environment\n        from azure.ai.ml.entities import Environment\n        env = Environment(\n            conda_file=\"../model-1/environment/conda.yml\",\n            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n        )\n        \n        # configure an inference configuration with a scoring script\n        from azure.ai.ml.entities import CodeConfiguration\n        code_config = CodeConfiguration(\n                code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n            )\n        ```\n\n    1. Configure and create an **online endpoint**:\n        ```python\n        import datetime\n        from azure.ai.ml.entities import ManagedOnlineEndpoint\n        \n        # create a unique endpoint name with current datetime to avoid conflicts\n        online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n        \n        # define an online endpoint\n        endpoint = ManagedOnlineEndpoint(\n            name=online_endpoint_name,\n            description=\"this is a sample online endpoint\",\n            auth_mode=\"key\",\n            tags={\"foo\": \"bar\"},\n        )\n        \n        # create the endpoint:\n        ml_client.begin_create_or_update(endpoint)\n        ```\n    \n    1. Configure and create an **online deployment**:\n        ```python\n        from azure.ai.ml.entities import ManagedOnlineDeployment\n        \n        # define a deployment\n        blue_deployment = ManagedOnlineDeployment(\n            name=\"blue\",\n            endpoint_name=online_endpoint_name,\n            model=model,\n            environment=env,\n            code_configuration=code_config,\n            instance_type=\"Standard_F2s_v2\",\n            instance_count=1,\n        )\n        \n        # create the deployment:\n        ml_client.begin_create_or_update(blue_deployment)\n        \n        # blue deployment takes 100 traffic\n        endpoint.traffic = {\"blue\": 100}\n        ml_client.begin_create_or_update(endpoint)\n        ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-deploy-endpoints-0.md"
                }
            },
            {
                "page_content": "\n# [Python SDK](#tab/python)\n\n### Connect to Azure Machine Learning workspace\nConnect to Azure Machine Learning Workspace, configure workspace details, and get a handle to the workspace as follows:\n\n1. Import the required libraries:\n\n```python\n# import required libraries\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import (\n   ManagedOnlineEndpoint,\n   ManagedOnlineDeployment,\n   Model,\n   Environment,\n   CodeConfiguration,\n)\nfrom azure.identity import DefaultAzureCredential\n```\n\n2. Configure workspace details and get a handle to the workspace:\n\n```python\n# enter details of your AzureML workspace\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace = \"<AZUREML_WORKSPACE_NAME>\"\n\n# get a handle to the workspace\nml_client = MLClient(\n   DefaultAzureCredential(), subscription_id, resource_group, workspace\n)\n```\n\nFor more information, see [Deploy machine learning models to managed online endpoint using Python SDK v2](how-to-deploy-managed-online-endpoint-sdk-v2.md).\n\n### Configure online endpoint\n\n> [!TIP]\n> * `name`: The name of the endpoint. It must be unique in the Azure region. The name for an endpoint must start with an upper- or lowercase letter and only consist of '-'s and alphanumeric characters. For more information on the naming rules, see [managed online endpoint limits](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).\n> * `auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` doesn't expire, but `aml_token` does expire. For more information on authenticating, see [Authenticate to an online endpoint](how-to-authenticate-online-endpoint.md).\n\nOptionally, you can add description, tags to your endpoint.\n\n```python\n# Creating a unique endpoint name with current datetime to avoid conflicts\nimport datetime\n\nonline_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"this is a sample online endpoint\",\n    auth_mode=\"key\",\n    tags={\"foo\": \"bar\"},\n)\n```\n\n### Configure online deployment\n\nA deployment is a set of resources required for hosting the model that does the actual inferencing. We'll create a deployment for our endpoint using the `ManagedOnlineDeployment` class.\n\n> [!TIP]\n> - `name` - Name of the deployment.\n> - `endpoint_name` - Name of the endpoint to create the deployment under.\n> - `model` - The model to use for the deployment. This value can be either a reference to an existing versioned > model in the workspace or an inline model specification.\n> - `environment` - The environment to use for the deployment. This value can be either a reference to an existing > versioned environment in the workspace or an inline environment specification.\n> - `code_configuration` - the configuration for the source code and scoring script\n>     - `path`- Path to the source code directory for scoring the model\n>     - `scoring_script` - Relative path to the scoring file in the source code directory\n> - `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).\n> - `instance_count` - The number of instances to use for the deployment\n\n```python\n# create a blue deployment\nmodel = Model(name=\"tfserving-mounted\", version=\"1\", path=\"half_plus_two\")\n\nenv = Environment(\n    image=\"docker.io/tensorflow/serving:latest\",\n    inference_config={\n        \"liveness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n        \"readiness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n        \"scoring_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two:predict\"},\n    },\n)\n\nblue_deployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=online_endpoint_name,\n    model=model,\n    environment=env,\n    environment_variables={\n        \"MODEL_BASE_PATH\": \"/var/azureml-app/azureml-models/tfserving-mounted/1\",\n        \"MODEL_NAME\": \"half_plus_two\",\n    },\n    instance_type=\"Standard_DS2_v2\",\n    instance_count=1,\n)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-custom-container-812.md"
                }
            },
            {
                "page_content": "\n# [Python](#tab/python)\n\n### Create online endpoint\n\nTo create a managed online endpoint, use the `ManagedOnlineEndpoint` class. This class allows users to configure the following key aspects of the endpoint:\n\n* `name` - Name of the endpoint. Needs to be unique at the Azure region level\n* `auth_mode` - The authentication method for the endpoint. Key-based authentication and Azure ML token-based authentication are supported. Key-based authentication doesn't expire but Azure ML token-based authentication does. Possible values are `key` or `aml_token`.\n* `identity`- The managed identity configuration for accessing Azure resources for endpoint provisioning and inference.\n    * `type`- The type of managed identity. Azure Machine Learning supports `system_assigned` or `user_assigned` identity.\n    * `user_assigned_identities` - List (array) of fully qualified resource IDs of the user-assigned identities. This property is required if `identity.type` is user_assigned.\n* `description`- Description of the endpoint.\n\n1. Configure the endpoint:\n\n```python\n# Creating a unique endpoint name with current datetime to avoid conflicts\nimport random\n\nonline_endpoint_name = \"endpt-moe-\" + str(random.randint(0, 10000))\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"this is a sample online endpoint\",\n    auth_mode=\"key\",\n    tags={\"foo\": \"bar\"},\n)\n```\n\n    > [!NOTE]\n    > To create a Kubernetes online endpoint, use the `KubernetesOnlineEndpoint` class.\n\n1. Create the endpoint:\n\n```python\nml_client.online_endpoints.begin_create_or_update(endpoint).result()\n```\n\n### Create the 'blue' deployment\n\nA deployment is a set of resources required for hosting the model that does the actual inferencing. To create a deployment for your managed online endpoint, use the `ManagedOnlineDeployment` class. This class allows users to configure the following key aspects of the deployment:\n\n**Key aspects of deployment**\n* `name` - Name of the deployment.\n* `endpoint_name` - Name of the endpoint to create the deployment under.\n* `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n* `environment` - The environment to use for the deployment. This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification.\n* `code_configuration` - the configuration for the source code and scoring script\n    * `path`- Path to the source code directory for scoring the model\n    * `scoring_script` - Relative path to the scoring file in the source code directory\n* `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).\n* `instance_count` - The number of instances to use for the deployment\n\n1. Configure blue deployment:\n\n```python\n# create blue deployment\nmodel = Model(path=\"../model-1/model/sklearn_regression_model.pkl\")\nenv = Environment(\n    conda_file=\"../model-1/environment/conda.yml\",\n    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n)\n\nblue_deployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=online_endpoint_name,\n    model=model,\n    environment=env,\n    code_configuration=CodeConfiguration(\n        code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n    ),\n    instance_type=\"Standard_DS3_v2\",\n    instance_count=1,\n)\n```\n\n    > [!NOTE]\n    > To create a deployment for a Kubernetes online endpoint, use the `KubernetesOnlineDeployment` class.\n\n1. Create the deployment:\n\n```python\nml_client.online_deployments.begin_create_or_update(blue_deployment).result()\n```\n\n```python\n# blue deployment takes 100 traffic\nendpoint.traffic = {\"blue\": 100}\nml_client.online_endpoints.begin_create_or_update(endpoint).result()\n```\n\n\n## Confirm your existing deployment\n\n# [Azure CLI](#tab/azure-cli)\n\nYou can view the status of your existing endpoint and deployment by running:\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-safely-rollout-online-endpoints-401.md"
                }
            }
        ]
    },
    {
        "query": "How to register an MLFlow model which is stored in Blob datastore using AzureML SDK V2?",
        "result": "You can register an MLFlow model stored in a Blob datastore using AzureML SDK V2 by creating an instance of the `Model` class and specifying the path to the model in the `path` parameter. The `path` parameter should be in the format `azureml://datastores/<datastore-name>/paths/<path_on_datastore>`. Then, call the `create_or_update` method of the `models` object of the `MLClient` instance to register the model.",
        "context": [
            {
                "page_content": "\n# Work with models in Azure Machine Learning\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\nAzure Machine Learning allows you to work with different types of models. In this article, you learn about using Azure Machine Learning to work with different model types, such as custom, MLflow, and Triton. You also learn how to register a model from different locations, and how to use the Azure Machine Learning SDK, the user interface (UI), and the Azure Machine Learning CLI to manage your models.\n\n> [!TIP]\n> If you have model assets created that use the SDK/CLI v1, you can still use those with SDK/CLI v2. Full backward compatibility is provided. All models registered with the V1 SDK are assigned the type `custom`.\n\n## Prerequisites\n\n* An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n* An Azure Machine Learning workspace.\n* The Azure Machine Learning [SDK v2 for Python](https://aka.ms/sdk-v2-install).\n* The Azure Machine Learning [CLI v2](how-to-configure-cli.md).\n\nAdditionally, you will need to:\n\n# [Azure CLI](#tab/cli)\n\n- Install the Azure CLI and the ml extension to the Azure CLI. For more information, see [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n\n# [Python SDK](#tab/python)\n\n- Install the Azure Machine Learning SDK for Python\n    \n    ```bash\n    pip install azure-ai-ml\n    ```\n\n## Supported paths\n\nWhen you provide a model you want to register, you'll need to specify a `path` parameter that points to the data or job location. Below is a table that shows the different data locations supported in Azure Machine Learning and examples for the `path` parameter:\n\n\n|Location  | Examples  |\n|---------|---------|\n|A path on your local computer     | `mlflow-model/model.pkl`         |\n|A path on an AzureML Datastore   |   `azureml://datastores/<datastore-name>/paths/<path_on_datastore>`      |\n|A path from an AzureML job   |   `azureml://jobs/<job-name>/outputs/<output-name>/paths/<path-to-model-relative-to-the-named-output-location>`      |\n|A path from an MLflow job   |   `runs:/<run-id>/<path-to-model-relative-to-the-root-of-the-artifact-location>`      |\n|A path from a Model Asset in AzureML Workspace  | `azureml:<model-name>:<version>`|\n|A path from a Model Asset in  AzureML Registry  | `azureml://registries/<registry-name>/models/<model-name>/versions/<version>`|\n\n## Supported modes\n\nWhen you run a job with model inputs/outputs, you can specify the *mode* - for example, whether you would like the model to be read-only mounted or downloaded to the compute target. The table below shows the possible modes for different type/mode/input/output combinations:\n\nType | Input/Output | `upload` | `download` | `ro_mount` | `rw_mount` | `direct` \n------ | ------ | :---: | :---: | :---: | :---: | :---: \n`custom` file  | Input  |   |  |   |  |    \n`custom` folder    | Input |   | \u2713 | \u2713  | |\u2713  \n`mlflow`    | Input |   | \u2713 |  \u2713 |   |   \n`custom` file | Output  | \u2713  |   |    | \u2713  | \u2713   \n`custom` folder    | Output | \u2713  |   |   | \u2713 | \u2713  \n`mlflow`   | Output | \u2713  |   |    | \u2713  | \u2713 \n\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in a Jupyter Notebook. In the [azureml-examples](https://github.com/azure/azureml-examples) repository, open the notebook: [model.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/assets/model/model.ipynb).\n\n## Create a model in the model registry\n\n[Model registration](concept-model-management-and-deployment.md) allows you to store and version your models in the Azure cloud, in your workspace. The model registry helps you organize and keep track of your trained models.\n\nThe code snippets in this section cover how to:\n\n* Register your model as an asset in Machine Learning by using the CLI.\n* Register your model as an asset in Machine Learning by using the SDK.\n* Register your model as an asset in Machine Learning by using the UI.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-models-0.md"
                }
            },
            {
                "page_content": "This option is an `azureml job` reference URI format, which helps you register a model from artifacts in any of the job's outputs. This format is aligned with the existing `azureml` datastore reference URI format, and also supports referencing artifacts from named outputs of the job (not just the default artifact location). You can establish a lineage between a registered model and the job it was trained from, if you didn't directly register your model within the training script by using MLflow.\n\nFormat:\n`azureml://jobs/<job-name>/outputs/<output-name>/paths/<path-to-model-relative-to-the-named-output-location>`\n\nExamples:\n- Default artifact location: `azureml://jobs/<run-id>/outputs/artifacts/paths/model/`\n  * This is equivalent to `runs:/<run-id>/model/`.\n  * *artifacts* is the reserved keyword to refer to the output that represents the default artifact location.\n- From a named output directory: `azureml://jobs/<run-id>/outputs/trained-model`\n- From a specific file or folder path within the named output directory:\n  * `azureml://jobs/<run-id>/outputs/trained-model/paths/cifar.pt`\n  * `azureml://jobs/<run-id>/outputs/checkpoints/paths/model/`\n\nSaving model from a named output:\n\n```cli\naz ml model create --name my-model --version 1 --path azureml://jobs/<run-id>/outputs/trained-model\n```\n\nFor a complete example, see the [CLI reference](/cli/azure/ml/model).\n\n\n### Register your model as an asset in Machine Learning by using the SDK\n\nUse the following tabs to select where your model is located.\n\n# [Local model](#tab/use-local)\n\n```python\nfrom azure.ai.ml.entities import Model\nfrom azure.ai.ml.constants import AssetTypes\n\nfile_model = Model(\n    path=\"mlflow-model/model.pkl\",\n    type=AssetTypes.CUSTOM_MODEL,\n    name=\"local-file-example\",\n    description=\"Model created from local file.\",\n)\nml_client.models.create_or_update(file_model)\n```\n\n# [Datastore](#tab/use-datastore)\n\nYou can create a model from a cloud path by using any one of the following supported URI formats.\n\n```python\nfrom azure.ai.ml.entities import Model\nfrom azure.ai.ml.constants import AssetTypes\n\ncloud_model = Model(\n    path=file_model.path,\n    # The above line basically provides a path in the format \"azureml://subscriptions/XXXXXXXXXXXXXXXX/resourceGroups/XXXXXXXXXXX/workspaces/XXXXXXXXXXX/datastores/workspaceblobstore/paths/model.pkl\"\n    # Users could also use,\"azureml://datastores/workspaceblobstore/paths/model.pkl\" as a shorthand to the same location\n    name=\"cloud-path-example\",\n    type=AssetTypes.CUSTOM_MODEL,\n    description=\"Model created from cloud path.\",\n)\nml_client.models.create_or_update(cloud_model)\n```\n\n\nThe examples use the shorthand `azureml` scheme for pointing to a path on the `datastore` by using the syntax `azureml://datastores/${{datastore-name}}/paths/${{path_on_datastore}}`.\n\n# [Job output](#tab/use-job-output)\n\nYou have two options here. You can use the MLflow run URI format, or you can use the `azureml job` URI format.\n\n### MLflow\n\nThis option is optimized for MLflow users, who are likely already familiar with the MLflow run URI format. This option helps you create a model from artifacts in the default artifact location (where all MLflow-logged models and artifacts are located). This establishes a lineage between a registered model and the run the model came from.\n\nFormat:\n`runs:/<run-id>/<path-to-model-relative-to-the-root-of-the-artifact-location>`\n\nExample:\n`runs:/<run-id>/model/`\n\n```python\nfrom azure.ai.ml.entities import Model\nfrom azure.ai.ml.constants import ModelType\n\nrun_model = Model(\n    path=\"runs:/<run-id>/model/\"\n    name=\"run-model-example\",\n    description=\"Model created from run.\",\n    type=ModelType.MLFLOW\n)\n\nml_client.models.create_or_update(run_model) \n```\n\n### azureml job\n\nThis option is an `azureml job` reference URI format, which helps you register a model from artifacts in any of the job's outputs. This format is aligned with the existing `azureml` datastore reference URI format, and also supports referencing artifacts from named outputs of the job (not just the default artifact location). You can establish a lineage between a registered model and the job it was trained from, if you didn't directly register your model within the training script by using MLflow.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-models-167.md"
                }
            },
            {
                "page_content": "\n# Upgrade datastore management to SDK v2\n\nAzure Machine Learning Datastores securely keep the connection information to your data storage on Azure, so you don't have to code it in your scripts. V2 Datastore concept remains mostly unchanged compared with V1. The difference is we won't support SQL-like data sources via AzureML Datastores. We'll support SQL-like data sources via AzureML data import&export functionalities.\n\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\n\n## Create a datastore from an Azure Blob container via account_key \n\n* SDK v1\n\n    ```python\n    blob_datastore_name='azblobsdk' # Name of the datastore to workspace\n    container_name=os.getenv(\"BLOB_CONTAINER\", \"<my-container-name>\") # Name of Azure blob container\n    account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"<my-account-name>\") # Storage account name\n    account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"<my-account-key>\") # Storage account access key\n    \n    blob_datastore = Datastore.register_azure_blob_container(workspace=ws, \n                                                             datastore_name=blob_datastore_name, \n                                                             container_name=container_name, \n                                                             account_name=account_name,\n                                                             account_key=account_key)\n    ```\n\n\n* SDK v2\n\n    ```python\n    from azure.ai.ml.entities import AzureBlobDatastore\n    from azure.ai.ml import MLClient\n    \n    ml_client = MLClient.from_config()\n    \n    store = AzureBlobDatastore(\n        name=\"blob-protocol-example\",\n        description=\"Datastore pointing to a blob container using wasbs protocol.\",\n        account_name=\"mytestblobstore\",\n        container_name=\"data-container\",\n        protocol=\"wasbs\",\n        credentials={\n            \"account_key\": \"XXXxxxXXXxXXXXxxXXXXXxXXXXXxXxxXxXXXxXXXxXXxxxXXxxXXXxXxXXXxxXxxXXXXxxxxxXXxxxxxxXXXxXXX\"\n        },\n    )\n    \n    ml_client.create_or_update(store)\n    ```\n\n\n## Create a datastore from an Azure Blob container via sas_token\n\n* SDK v1\n\n    ```python\n    blob_datastore_name='azblobsdk' # Name of the datastore to workspace\n    container_name=os.getenv(\"BLOB_CONTAINER\", \"<my-container-name>\") # Name of Azure blob container\n    sas_token=os.getenv(\"BLOB_SAS_TOKEN\", \"<my-sas-token>\") # Sas token\n    \n    blob_datastore = Datastore.register_azure_blob_container(workspace=ws, \n                                                             datastore_name=blob_datastore_name, \n                                                             container_name=container_name, \n                                                             sas_token=sas_token)\n    ```\n    \n* SDK v2\n\n    ```python\n    from azure.ai.ml.entities import AzureBlobDatastore\n    from azure.ai.ml import MLClient\n    \n    ml_client = MLClient.from_config()\n    \n    store = AzureBlobDatastore(\n        name=\"blob-sas-example\",\n        description=\"Datastore pointing to a blob container using SAS token.\",\n        account_name=\"mytestblobstore\",\n        container_name=\"data-container\",\n        credentials=SasTokenCredentials(\n            sas_token= \"?xx=XXXX-XX-XX&xx=xxxx&xxx=xxx&xx=xxxxxxxxxxx&xx=XXXX-XX-XXXXX:XX:XXX&xx=XXXX-XX-XXXXX:XX:XXX&xxx=xxxxx&xxx=XXxXXXxxxxxXXXXXXXxXxxxXXXXXxxXXXXXxXXXXxXXXxXXxXX\"\n        ),\n    )\n    \n    ml_client.create_or_update(store)\n    ```\n    \n## Create a datastore from an Azure Blob container via identity-based authentication\n\n* SDK v1\n\n```python\nblob_datastore = Datastore.register_azure_blob_container(workspace=ws,\n                                                      datastore_name='credentialless_blob',\n                                                      container_name='my_container_name',\n                                                      account_name='my_account_name')\n\n```\n\n* SDK v2\n\n    ```python\n    from azure.ai.ml.entities import AzureBlobDatastore\n    from azure.ai.ml import MLClient\n    \n    ml_client = MLClient.from_config()\n    \n    store = AzureBlobDatastore(\n        name=\"\",\n        description=\"\",\n        account_name=\"\",\n        container_name=\"\"\n    )\n    \n    ml_client.create_or_update(store)\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-resource-datastore-0.md"
                }
            }
        ]
    },
    {
        "query": "How to debug a failed Job using SSH and AzureML SDK V2?",
        "result": "To debug a failed job using SSH and AzureML SDK V2, you can SSH into the compute target and navigate to the logs folder. The logs folder contains the logs for each task in the job. You can use the AzureML SDK V2 to download the logs to your local machine for further analysis. To SSH into the compute target, you need to enable SSH access when creating the compute target. You can then use the `azureml.core.compute.ComputeTarget.list_nodes()` method to get the IP address of the node to SSH into.",
        "context": [
            {
                "page_content": "| overhead timeout | integer | The timeout in second for initialization of each mini-batch. For example, load mini-batch data and pass it to run() function. | (0, 259200] | 600 | N/A | --task_overhead_timeout |\n| progress update timeout | integer | The timeout in second for monitoring the progress of mini-batch execution. If no progress updates receive within this timeout setting, the parallel job will be marked as failed. | (0, 259200] | Dynamically calculated by other settings. | N/A | --progress_update_timeout |\n| first task creation timeout | integer | The timeout in second for monitoring the time between the job start to the run of first mini-batch. | (0, 259200] | 600 | N/A | --first_task_creation_timeout |\n| logging level | string | Define which level of logs will be dumped to user log files. | INFO, WARNING, or DEBUG | INFO | logging_level | N/A |\n| append row to | string | Aggregate all returns from each run of mini-batch and output it into this file. May reference to one of the outputs of parallel job by using the expression ${{outputs.<output_name>}} |  |  | task.append_row_to | N/A |\n| copy logs to parent | string | Boolean option to whether copy the job progress, overview, and logs to the parent pipeline job. | True or False | False | N/A | --copy_logs_to_parent |\n| resource monitor interval | integer | The time interval in seconds to dump node resource usage(for example, cpu, memory) to log folder under \"logs/sys/perf\" path.<br><br>Note: Frequent dump resource logs will slightly slow down the execution speed of your mini-batch. Set this value to \"0\" to stop dumping resource usage. | [0, int.max] | 600 | N/A | --resource_monitor_interval |\n\nSample code to update these settings:\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\n\ndisplay_name: iris-batch-prediction-using-parallel\ndescription: The hello world pipeline job with inline parallel job\ntags:\n  tag: tagvalue\n  owner: sdkteam\n\nsettings:\n  default_compute: azureml:cpu-cluster\n\njobs:\n  batch_prediction:\n    type: parallel\n    compute: azureml:cpu-cluster\n    inputs:\n      input_data: \n        type: mltable\n        path: ./neural-iris-mltable\n        mode: direct\n      score_model: \n        type: uri_folder\n        path: ./iris-model\n        mode: download\n    outputs:\n      job_output_file:\n        type: uri_file\n        mode: rw_mount\n\n    input_data: ${{inputs.input_data}}\n    mini_batch_size: \"10kb\"\n    resources:\n        instance_count: 2\n    max_concurrency_per_instance: 2\n\n    logging_level: \"DEBUG\"\n    mini_batch_error_threshold: 5\n    retry_settings:\n      max_retries: 2\n      timeout: 60\n\n    task:\n      type: run_function\n      code: \"./script\"\n      entry_script: iris_prediction.py\n      environment:\n        name: \"prs-env\"\n        version: 1\n        image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n        conda_file: ./environment/environment_parallel.yml\n      program_arguments: >-\n        --model ${{inputs.score_model}}\n        --error_threshold 5\n        --allowed_failed_percent 30\n        --task_overhead_timeout 1200\n        --progress_update_timeout 600\n        --first_task_creation_timeout 600\n        --copy_logs_to_parent True\n        --resource_monitor_interva 20\n      append_row_to: ${{outputs.job_output_file}}\n\n```\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\n# parallel task to process tabular data\ntabular_batch_inference = parallel_run_function(\n    name=\"batch_score_with_tabular_input\",\n    display_name=\"Batch Score with Tabular Dataset\",\n    description=\"parallel component for batch score\",\n    inputs=dict(\n        job_data_path=Input(\n            type=AssetTypes.MLTABLE,\n            description=\"The data to be split and scored in parallel\",\n        ),\n        score_model=Input(\n            type=AssetTypes.URI_FOLDER, description=\"The model for batch score.\"\n        ),\n    ),\n    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n    input_data=\"${{inputs.job_data_path}}\",\n    instance_count=2,\n    max_concurrency_per_instance=2,\n    mini_batch_size=\"100\",\n    mini_batch_error_threshold=5,\n    logging_level=\"DEBUG\",\n    retry_settings=dict(max_retries=2, timeout=60),\n    task=RunFunction(\n        code=\"./src\",\n        entry_script=\"tabular_batch_inference.py\",\n        environment=Environment(\n            image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n            conda_file=\"./src/environment_parallel.yml\",\n        ),\n        program_arguments=\"--model ${{inputs.score_model}} \"\n        \"--job_output_path ${{outputs.job_output_path}} \"\n        \"--error_threshold 5 \"\n        \"--allowed_failed_percent 30 \"\n        \"--task_overhead_timeout 1200 \"\n        \"--progress_update_timeout 600 \"\n        \"--first_task_creation_timeout 600 \"\n        \"--copy_logs_to_parent True \"\n        \"--resource_monitor_interva 20 \",\n        append_row_to=\"${{outputs.job_output_path}}\",\n    ),\n)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-parallel-job-in-pipeline-423.md"
                }
            },
            {
                "page_content": "Create a single- or multi- node compute cluster for your training, batch inferencing or reinforcement learning workloads. \n\n1. Navigate to [Azure Machine Learning studio](https://ml.azure.com).\n \n1. Under __Manage__, select __Compute__.\n1. If you have no compute resources, select  **Create** in the middle of the page.\n  \n    :::image type=\"content\" source=\"media/how-to-create-attach-studio/create-compute-target.png\" alt-text=\"Screenshot that shows creating a compute target\":::\n\n1. If you see a list of compute resources, select **+New** above the list.\n\n    :::image type=\"content\" source=\"media/how-to-create-attach-studio/select-new.png\" alt-text=\"Select new\":::\n\n1. In the tabs at the top, select __Compute cluster__\n\n1. Fill out the form as follows:\n\n    |Field  |Description  |\n    |---------|---------|\n    | Location | The Azure region where the compute cluster will be created. By default, this is the same location as the workspace. Setting the location to a different region than the workspace is in __preview__, and is only available for __compute clusters__, not compute instances.</br>When using a different region than your workspace or datastores, you may see increased network latency and data transfer costs. The latency and costs can occur when creating the cluster, and when running jobs on it. |\n    |Virtual machine type |  Choose CPU or GPU. This type can't be changed after creation     |\n    |Virtual machine priority | Choose **Dedicated** or **Low priority**.  Low priority virtual machines are cheaper but don't guarantee the compute nodes. Your job may be preempted.\n    |Virtual machine size     |  Supported virtual machine sizes might be restricted in your region. Check the [availability list](https://azure.microsoft.com/global-infrastructure/services/?products=virtual-machines)     |\n\n1. Select **Next** to proceed to **Advanced Settings** and fill out the form as follows:\n\n    |Field  |Description  |\n    |---------|---------|\n    |Compute name     | * Name is required and must be between 3 to 24 characters long.<br><br> * Valid characters are upper and lower case letters, digits, and the  **-** character.<br><br> * Name must start with a letter<br><br> * Name needs to be unique across all existing computes within an Azure region. You'll see an alert if the name you choose isn't unique<br><br> * If **-**  character is used, then it needs to be followed by at least one letter later in the name    |\n    |Minimum number of nodes | Minimum number of nodes that you want to provision. If you want a dedicated number of nodes, set that count here. Save money by setting the minimum to 0, so you won't pay for any nodes when the cluster is idle. |\n    |Maximum number of nodes | Maximum number of nodes that you want to provision. The compute will autoscale to a maximum of this node count when a job is submitted. |\n    | Idle seconds before scale down | Idle time before scaling the cluster down to the minimum node count. |\n    | Enable SSH access | Use the same instructions as [Enable SSH access](#enable-ssh-access) for a compute instance (above). |\n    |Advanced settings     |  Optional. Configure a virtual network. Specify the **Resource group**, **Virtual network**, and **Subnet** to create the compute instance inside an Azure Virtual Network (vnet). For more information, see these [network requirements](./how-to-secure-training-vnet.md) for vnet.   Also attach [managed identities](#set-up-managed-identity) to grant access to resources.\n\n1. Select __Create__.\n\n\n### Enable SSH access\n\nSSH access is disabled by default.  SSH access can't be changed after creation. Make sure to enable access if you plan to debug interactively with [VS Code Remote](how-to-set-up-vs-code-remote.md).  \n\n[!INCLUDE [enable-ssh](../../includes/machine-learning-enable-ssh.md)]\n\n### Connect with SSH access\n\n[!INCLUDE [ssh-access](../../includes/machine-learning-ssh-access.md)]\n\n\n ## Lower your compute cluster cost with low priority VMs\n\nYou may also choose to use [low-priority VMs](how-to-manage-optimize-cost.md#low-pri-vm) to run some or all of your workloads. These VMs don't have guaranteed availability and may be preempted while in use. You'll have to restart a preempted job. \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-attach-compute-cluster-128.md"
                }
            },
            {
                "page_content": "\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nDeclare `job_data_path` as one of the inputs. Bind it to `input_data` attribute.\n\n```python\n# parallel task to process file data\nfile_batch_inference = parallel_run_function(\n    name=\"file_batch_score\",\n    display_name=\"Batch Score with File Dataset\",\n    description=\"parallel component for batch score\",\n    inputs=dict(\n        job_data_path=Input(\n            type=AssetTypes.MLTABLE,\n            description=\"The data to be split and scored in parallel\",\n        )\n    ),\n    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n    input_data=\"${{inputs.job_data_path}}\",\n    instance_count=2,\n    max_concurrency_per_instance=1,\n    mini_batch_size=\"1\",\n    mini_batch_error_threshold=1,\n    retry_settings=dict(max_retries=2, timeout=60),\n    logging_level=\"DEBUG\",\n    task=RunFunction(\n        code=\"./src\",\n        entry_script=\"file_batch_inference.py\",\n        program_arguments=\"--job_output_path ${{outputs.job_output_path}}\",\n        environment=\"azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n    ),\n)\n```\n\n\nOnce you have the partition setting defined, you can configure parallel setting by using two attributes below:\n\n| Attribute name | Type | Description | Default value |\n|:-|--|:-|--|\n| `instance_count` | integer | The number of nodes to use for the job. | 1 |\n| `max_concurrency_per_instance` | integer | The number of processors on each node. | For a GPU compute, the default value is 1.<br> For a CPU compute, the default value is the number of cores. |\n\nThese two attributes work together with your specified compute cluster.\n\n:::image type=\"content\" source=\"./media/how-to-use-parallel-job-in-pipeline/how-distributed-data-works-in-parallel-job.png\" alt-text=\"Diagram showing how distributed data works in parallel job.\" lightbox =\"./media/how-to-use-parallel-job-in-pipeline/how-distributed-data-works-in-parallel-job.png\":::\n\nSample code to set two attributes:\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\n\ndisplay_name: iris-batch-prediction-using-parallel\ndescription: The hello world pipeline job with inline parallel job\ntags:\n  tag: tagvalue\n  owner: sdkteam\n\nsettings:\n  default_compute: azureml:cpu-cluster\n\njobs:\n  batch_prediction:\n    type: parallel\n    compute: azureml:cpu-cluster\n    inputs:\n      input_data: \n        type: mltable\n        path: ./neural-iris-mltable\n        mode: direct\n      score_model: \n        type: uri_folder\n        path: ./iris-model\n        mode: download\n    outputs:\n      job_output_file:\n        type: uri_file\n        mode: rw_mount\n\n    input_data: ${{inputs.input_data}}\n    mini_batch_size: \"10kb\"\n    resources:\n        instance_count: 2\n    max_concurrency_per_instance: 2\n\n    logging_level: \"DEBUG\"\n    mini_batch_error_threshold: 5\n    retry_settings:\n      max_retries: 2\n      timeout: 60\n\n    task:\n      type: run_function\n      code: \"./script\"\n      entry_script: iris_prediction.py\n      environment:\n        name: \"prs-env\"\n        version: 1\n        image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n        conda_file: ./environment/environment_parallel.yml\n      program_arguments: >-\n        --model ${{inputs.score_model}}\n        --error_threshold 5\n        --allowed_failed_percent 30\n        --task_overhead_timeout 1200\n        --progress_update_timeout 600\n        --first_task_creation_timeout 600\n        --copy_logs_to_parent True\n        --resource_monitor_interva 20\n      append_row_to: ${{outputs.job_output_file}}\n\n```\n\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\n# parallel task to process file data\nfile_batch_inference = parallel_run_function(\n    name=\"file_batch_score\",\n    display_name=\"Batch Score with File Dataset\",\n    description=\"parallel component for batch score\",\n    inputs=dict(\n        job_data_path=Input(\n            type=AssetTypes.MLTABLE,\n            description=\"The data to be split and scored in parallel\",\n        )\n    ),\n    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n    input_data=\"${{inputs.job_data_path}}\",\n    instance_count=2,\n    max_concurrency_per_instance=1,\n    mini_batch_size=\"1\",\n    mini_batch_error_threshold=1,\n    retry_settings=dict(max_retries=2, timeout=60),\n    logging_level=\"DEBUG\",\n    task=RunFunction(\n        code=\"./src\",\n        entry_script=\"file_batch_inference.py\",\n        program_arguments=\"--job_output_path ${{outputs.job_output_path}}\",\n        environment=\"azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n    ),\n)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-parallel-job-in-pipeline-121.md"
                }
            }
        ]
    },
    {
        "query": "Generate Python Code Samples to create a safe model deployment using ManagedOnlineEndpoint using AzureML SDK V2?",
        "result": "To create a safe model deployment using ManagedOnlineEndpoint using AzureML SDK V2, you can follow these steps:\n\n1. Connect to your AzureML workspace using the `MLClient` class and `DefaultAzureCredential`.\n2. Create a unique endpoint name using the `datetime` module and create an online endpoint using the `ManagedOnlineEndpoint` class.\n3. Create a deployment for the endpoint using the `ManagedOnlineDeployment` class, specifying the model, environment, code configuration, instance type, and instance count.\n4. Use the `ml_client.begin_create_or_update()` method to create the endpoint and deployment.\n5. Test the endpoint with sample data and troubleshoot any issues if necessary.\n\nFor more information, see the Azure documentation on [Deploy machine learning models to managed online endpoint using Python SDK v2](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-managed-online-endpoint-sdk-v2).",
        "context": [
            {
                "page_content": "\n## Create the endpoint and deployment\n\nNext, we'll create the managed online endpoints and deployments.\n\n1. Configure online endpoint:\n\n    > [!TIP]\n    > * `name`: The name of the endpoint. It must be unique in the Azure region. The name for an endpoint must start with an upper- or lowercase letter and only consist of '-'s and alphanumeric characters. For more information on the naming rules, see [managed online endpoint limits](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).\n    > * `auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` doesn't expire, but `aml_token` does expire. For more information on authenticating, see [Authenticate to an online endpoint](how-to-authenticate-online-endpoint.md).\n\n\n    ```python\n    # Creating a unique endpoint name with current datetime to avoid conflicts\n    import datetime\n\n    online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n\n    # create an online endpoint\n    endpoint = ManagedOnlineEndpoint(\n        name=online_endpoint_name,\n        description=\"this is a sample online endpoint\",\n        auth_mode=\"key\",\n    )\n    ```\n\n1. Create the endpoint:\n\n    Using the `MLClient` created earlier, we'll now create the Endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues.\n\n    ```python\n    ml_client.begin_create_or_update(endpoint)\n    ```\n\n1. Configure online deployment:\n\n    A deployment is a set of resources required for hosting the model that does the actual inferencing. We'll create a deployment for our endpoint using the `ManagedOnlineDeployment` class.\n\n    ```python\n    model = Model(path=\"./src/model.pkl\")\n    env = Environment(\n        conda_file=\"./src/conda_env_v_1_0_0.yml\",\n        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n    )\n\n    blue_deployment = ManagedOnlineDeployment(\n        name=\"blue\",\n        endpoint_name=online_endpoint_name,\n        model=model,\n        environment=env,\n        code_configuration=CodeConfiguration(\n            code=\"./src\", scoring_script=\"scoring_file_v_2_0_0.py\"\n        ),\n        instance_type=\"Standard_DS2_v2\",\n        instance_count=1,\n    )\n    ```\n\n    In the above example, we assume the files you downloaded from the AutoML Models page are in the `src` directory. You can modify the parameters in the code to suit your situation.\n    \n    | Parameter | Change to |\n    | --- | --- |\n    | `model:path` | The path to the `model.pkl` file you downloaded. |\n    | `code_configuration:code:path` | The directory in which you placed the scoring file. | \n    | `code_configuration:scoring_script` | The name of the Python scoring file (`scoring_file_<VERSION>.py`). |\n    | `environment:conda_file` | A file URL for the downloaded conda environment file (`conda_env_<VERSION>.yml`). |\n\n1. Create the deployment:\n\n    Using the `MLClient` created earlier, we'll now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues.\n\n    ```python\n    ml_client.begin_create_or_update(blue_deployment)\n    ```\n\nAfter you create a deployment, you can score it as described in [Test the endpoint with sample data](how-to-deploy-managed-online-endpoint-sdk-v2.md#test-the-endpoint-with-sample-data).\n\nYou can learn to deploy to managed online endpoints with SDK more in [Deploy machine learning models to managed online endpoint using Python SDK v2](how-to-deploy-managed-online-endpoint-sdk-v2.md).\n\n\n## Next steps\n\n- [Troubleshooting online endpoints deployment](how-to-troubleshoot-managed-online-endpoints.md)\n- [Safe rollout for online endpoints](how-to-safely-rollout-online-endpoints.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-automl-endpoint-409.md"
                }
            },
            {
                "page_content": "To follow along with this article, first clone the [examples repository (azureml-examples)](https://github.com/azure/azureml-examples). Then, go to the repository's `cli/` directory:\n\n```azurecli\ngit clone --depth 1 https://github.com/Azure/azureml-examples\ncd azureml-examples\ncd cli\n```\n\n> [!TIP]\n> Use `--depth 1` to clone only the latest commit to the repository. This reduces the time to complete the operation.\n\nThe commands in this tutorial are in the file `deploy-safe-rollout-online-endpoints.sh` in the `cli` directory, and the YAML configuration files are in the `endpoints/online/managed/sample/` subdirectory.\n\n> [!NOTE]\n> The YAML configuration files for Kubernetes online endpoints are in the `endpoints/online/kubernetes/` subdirectory.\n\n# [Python](#tab/python)\n\n### Clone the examples repository\n\nTo run the training examples, first clone the [examples repository (azureml-examples)](https://github.com/azure/azureml-examples). Then, go into the `azureml-examples/sdk/python/endpoints/online/managed` directory:\n\n```bash\ngit clone --depth 1 https://github.com/Azure/azureml-examples\ncd azureml-examples/sdk/python/endpoints/online/managed\n```\n\n> [!TIP]\n> Use `--depth 1` to clone only the latest commit to the repository. This reduces the time to complete the operation.\n\nThe information in this article is based on the [online-endpoints-safe-rollout.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/online/managed/online-endpoints-safe-rollout.ipynb) notebook. It contains the same content as this article, although the order of the codes is slightly different.\n\n> [!NOTE]\n> The steps for the Kubernetes online endpoint are based on the [kubernetes-online-endpoints-safe-rollout.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/online/kubernetes/kubernetes-online-endpoints-safe-rollout.ipynb) notebook.\n\n### Connect to Azure Machine Learning workspace\n\nThe [workspace](concept-workspace.md) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section, we'll connect to the workspace where you'll perform deployment tasks.\n\n1. Import the required libraries:\n\n```python\n# import required libraries\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import (\n    ManagedOnlineEndpoint,\n    ManagedOnlineDeployment,\n    Model,\n    Environment,\n    CodeConfiguration,\n)\nfrom azure.identity import DefaultAzureCredential\n```\n\n    > [!NOTE]\n    > If you're using the Kubernetes online endpoint, import the `KubernetesOnlineEndpoint` and `KubernetesOnlineDeployment` class from the `azure.ai.ml.entities` library.\n\n1. Configure workspace details and get a handle to the workspace:\n\n    To connect to a workspace, we need identifier parameters\u2014a subscription, resource group and workspace name. We'll use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. This example uses the [default Azure authentication](/python/api/azure-identity/azure.identity.defaultazurecredential).\n\n```python\n# enter details of your AML workspace\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace = \"<AML_WORKSPACE_NAME>\"\n```\n\n```python\n# get a handle to the workspace\nml_client = MLClient(\n    DefaultAzureCredential(), subscription_id, resource_group, workspace\n)\n```\n\n\n## Define the endpoint and deployment\n\nOnline endpoints are used for online (real-time) inferencing. Online endpoints contain deployments that are ready to receive data from clients and can send responses back in real time.\n\n# [Azure CLI](#tab/azure-cli)\n\n### Create online endpoint\n\nTo create an online endpoint:\n\n1. Set your endpoint name:\n\n   For Unix, run this command (replace `YOUR_ENDPOINT_NAME` with a unique name):\n\n```azurecli\nset -e\n\n# <set_endpoint_name>\nexport ENDPOINT_NAME=\"<YOUR_ENDPOINT_NAME>\"\n# </set_endpoint_name>\n\n#  endpoint name\nexport ENDPOINT_NAME=endpt-sr-`echo $RANDOM`\n\n# <create_endpoint>\naz ml online-endpoint create --name $ENDPOINT_NAME -f endpoints/online/managed/sample/endpoint.yml\n# </create_endpoint>\n\n# <create_blue>\naz ml online-deployment create --name blue --endpoint-name $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment.yml --all-traffic\n# </create_blue>\n\n# <test_blue>\naz ml online-endpoint invoke --name $ENDPOINT_NAME --request-file endpoints/online/model-1/sample-request.json\n# </test_blue>\n\n# <scale_blue>\naz ml online-deployment update --name blue --endpoint-name $ENDPOINT_NAME --set instance_count=2\n# </scale_blue>\n\n# <create_green>\naz ml online-deployment create --name green --endpoint-name $ENDPOINT_NAME -f endpoints/online/managed/sample/green-deployment.yml\n# </create_green>\n\n# <get_traffic>\naz ml online-endpoint show -n $ENDPOINT_NAME --query traffic\n# </get_traffic>\n\n# <test_green>\naz ml online-endpoint invoke --name $ENDPOINT_NAME --deployment-name green --request-file endpoints/online/model-2/sample-request.json\n# </test_green>\n\n# supress printing secret\nset +x\n\n# <test_green_using_curl_get_key>\n#get the key\nENDPOINT_KEY=$(az ml online-endpoint get-credentials -n $ENDPOINT_NAME -o tsv --query primaryKey)\n# </test_green_using_curl_get_key>\n\nset -x\n\n# <test_green_using_curl>\n# get the scoring uri\nSCORING_URI=$(az ml online-endpoint show -n $ENDPOINT_NAME -o tsv --query scoring_uri)\n# use curl to invoke the endpoint\ncurl --request POST \"$SCORING_URI\" --header \"Authorization: Bearer $ENDPOINT_KEY\" --header 'Content-Type: application/json' --header \"azureml-model-deployment: green\" --data @endpoints/online/model-2/sample-request.json\n# </test_green_using_curl>\n\n# <test_green_with_mirror_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --mirror-traffic \"green=10\"\n# </test_green_with_mirror_traffic>\n\n# <reset_mirror_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --mirror-traffic \"green=0\"\n# </reset_mirror_traffic>\n\n# <green_10pct_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --traffic \"blue=90 green=10\"\n# </green_10pct_traffic>\n\n# <green_100pct_traffic>\naz ml online-endpoint update --name $ENDPOINT_NAME --traffic \"blue=0 green=100\"\n# </green_100pct_traffic>\n\n# <delete_blue>\naz ml online-deployment delete --name blue --endpoint $ENDPOINT_NAME --yes --no-wait\n# </delete_blue>\n\n# <delete_endpoint>\naz ml online-endpoint delete --name $ENDPOINT_NAME --yes --no-wait\n# </delete_endpoint>\n\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-safely-rollout-online-endpoints-61.md"
                }
            },
            {
                "page_content": "\n# [Python SDK](#tab/python)\n\n### Connect to Azure Machine Learning workspace\nConnect to Azure Machine Learning Workspace, configure workspace details, and get a handle to the workspace as follows:\n\n1. Import the required libraries:\n\n```python\n# import required libraries\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import (\n   ManagedOnlineEndpoint,\n   ManagedOnlineDeployment,\n   Model,\n   Environment,\n   CodeConfiguration,\n)\nfrom azure.identity import DefaultAzureCredential\n```\n\n2. Configure workspace details and get a handle to the workspace:\n\n```python\n# enter details of your AzureML workspace\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace = \"<AZUREML_WORKSPACE_NAME>\"\n\n# get a handle to the workspace\nml_client = MLClient(\n   DefaultAzureCredential(), subscription_id, resource_group, workspace\n)\n```\n\nFor more information, see [Deploy machine learning models to managed online endpoint using Python SDK v2](how-to-deploy-managed-online-endpoint-sdk-v2.md).\n\n### Configure online endpoint\n\n> [!TIP]\n> * `name`: The name of the endpoint. It must be unique in the Azure region. The name for an endpoint must start with an upper- or lowercase letter and only consist of '-'s and alphanumeric characters. For more information on the naming rules, see [managed online endpoint limits](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).\n> * `auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` doesn't expire, but `aml_token` does expire. For more information on authenticating, see [Authenticate to an online endpoint](how-to-authenticate-online-endpoint.md).\n\nOptionally, you can add description, tags to your endpoint.\n\n```python\n# Creating a unique endpoint name with current datetime to avoid conflicts\nimport datetime\n\nonline_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"this is a sample online endpoint\",\n    auth_mode=\"key\",\n    tags={\"foo\": \"bar\"},\n)\n```\n\n### Configure online deployment\n\nA deployment is a set of resources required for hosting the model that does the actual inferencing. We'll create a deployment for our endpoint using the `ManagedOnlineDeployment` class.\n\n> [!TIP]\n> - `name` - Name of the deployment.\n> - `endpoint_name` - Name of the endpoint to create the deployment under.\n> - `model` - The model to use for the deployment. This value can be either a reference to an existing versioned > model in the workspace or an inline model specification.\n> - `environment` - The environment to use for the deployment. This value can be either a reference to an existing > versioned environment in the workspace or an inline environment specification.\n> - `code_configuration` - the configuration for the source code and scoring script\n>     - `path`- Path to the source code directory for scoring the model\n>     - `scoring_script` - Relative path to the scoring file in the source code directory\n> - `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).\n> - `instance_count` - The number of instances to use for the deployment\n\n```python\n# create a blue deployment\nmodel = Model(name=\"tfserving-mounted\", version=\"1\", path=\"half_plus_two\")\n\nenv = Environment(\n    image=\"docker.io/tensorflow/serving:latest\",\n    inference_config={\n        \"liveness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n        \"readiness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n        \"scoring_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two:predict\"},\n    },\n)\n\nblue_deployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=online_endpoint_name,\n    model=model,\n    environment=env,\n    environment_variables={\n        \"MODEL_BASE_PATH\": \"/var/azureml-app/azureml-models/tfserving-mounted/1\",\n        \"MODEL_NAME\": \"half_plus_two\",\n    },\n    instance_type=\"Standard_DS2_v2\",\n    instance_count=1,\n)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-custom-container-812.md"
                }
            }
        ]
    },
    {
        "query": "How to train an MLflow model using Command Job and AzureML SDK V2?",
        "result": "To train an MLflow model using Command Job and AzureML SDK V2, you need to define the command job using the `command()` function and specify the environment, compute, and other parameters. You can then submit the job using the `MLClient.jobs.create_or_update()` function. It is recommended to remove any code specific to Azure ML from your model training scripts and replace model logging and tracking code with MLflow. You can find more information on how to use MLflow in v2 in the Azure ML documentation.",
        "context": [
            {
                "page_content": "\n# Upgrade script run to SDK v2\n\nIn SDK v2, \"experiments\" and \"runs\" are consolidated into jobs.\n\nA job has a type. Most jobs are command jobs that run a `command`, like `python main.py`. What runs in a job is agnostic to any programming language, so you can run `bash` scripts, invoke `python` interpreters, run a bunch of `curl` commands, or anything else.\n\nTo upgrade, you'll need to change your code for submitting jobs to SDK v2. What you run _within_ the job doesn't need to be upgraded to SDK v2. However, it's recommended to remove any code specific to Azure ML from your model training scripts. This separation allows for an easier transition between local and cloud and is considered best practice for mature MLOps. In practice, this means removing `azureml.*` lines of code. Model logging and tracking code should be replaced with MLflow. For more details, see [how to use MLflow in v2](how-to-use-mlflow-cli-runs.md).\n\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\n\n## Submit a script run\n\n* SDK v1\n\n    ```python\n    from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig\n    \n    # connect to the workspace\n    ws = Workspace.from_config()\n    \n    # define and configure the experiment\n    experiment = Experiment(workspace=ws, name='day1-experiment-train')\n    config = ScriptRunConfig(source_directory='./src',\n                                script='train.py',\n                                compute_target='cpu-cluster')\n    \n    # set up pytorch environment\n    env = Environment.from_conda_specification(\n        name='pytorch-env',\n        file_path='pytorch-env.yml')\n    config.run_config.environment = env\n    \n    run = experiment.submit(config)\n    \n    aml_url = run.get_portal_url()\n    print(aml_url)\n    ```\n\n* SDK v2\n\n    ```python\n    #import required libraries\n    from azure.ai.ml import MLClient, command\n    from azure.ai.ml.entities import Environment\n    from azure.identity import DefaultAzureCredential\n    \n    #connect to the workspace\n    ml_client = MLClient.from_config(DefaultAzureCredential())\n    \n    # set up pytorch environment\n    env = Environment(\n        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n        conda_file=\"pytorch-env.yml\",\n        name=\"pytorch-env\"\n    )\n    \n    # define the command\n    command_job = command(\n        code=\"./src\",\n        command=\"train.py\",\n        environment=env,\n        compute=\"cpu-cluster\",\n    )\n    \n    returned_job = ml_client.jobs.create_or_update(command_job)\n    returned_job\n    ```\n\n## Mapping of key functionality in v1 and v2\n\n|Functionality in SDK v1|Rough mapping in SDK v2|\n|-|-|\n|[experiment.submit](/python/api/azureml-core/azureml.core.experiment.experiment#azureml-core-experiment-experiment-submit)|[MLCLient.jobs.create_or_update](/python/api/azure-ai-ml/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update)|\n|[ScriptRunConfig()](/python/api/azureml-core/azureml.core.scriptrunconfig#constructor)|[command()](/python/api/azure-ai-ml/azure.ai.ml#azure-ai-ml-command)|\n\n## Next steps\n\nFor more information, see:\n\n* [V1 - Experiment](/python/api/azureml-core/azureml.core.experiment)\n* [V2 - Command Job](/python/api/azure-ai-ml/azure.ai.ml#azure-ai-ml-command)\n* [Train models with the Azure ML Python SDK v2](how-to-train-sdk.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-command-job-0.md"
                }
            },
            {
                "page_content": "\n# Upgrade pipelines to SDK v2\n\nIn SDK v2, \"pipelines\" are consolidated into jobs.\n\nA job has a type. Most jobs are command jobs that run a `command`, like `python main.py`. What runs in a job is agnostic to any programming language, so you can run `bash` scripts, invoke `python` interpreters, run a bunch of `curl` commands, or anything else.\n\nA `pipeline` is another type of job, which defines child jobs that may have input/output relationships, forming a directed acyclic graph (DAG).\n\nTo upgrade, you'll need to change your code for defining and submitting the pipelines to SDK v2. What you run _within_ the child job doesn't need to be upgraded to SDK v2. However, it's recommended to remove any code specific to Azure ML from your model training scripts. This separation allows for an easier transition between local and cloud and is considered best practice for mature MLOps. In practice, this means removing `azureml.*` lines of code. Model logging and tracking code should be replaced with MLflow. For more information, see [how to use MLflow in v2](how-to-use-mlflow-cli-runs.md).\n\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2. In the following examples, we'll build three steps (train, score and evaluate) into a dummy pipeline job. This demonstrates how to build pipeline jobs using SDK v1 and SDK v2, and how to consume data and transfer data between steps.\n\n## Run a pipeline\n\n* SDK v1\n\n    ```python\n    # import required libraries\n    import os\n    import azureml.core\n    from azureml.core import (\n        Workspace,\n        Dataset,\n        Datastore,\n        ComputeTarget,\n        Experiment,\n        ScriptRunConfig,\n    )\n    from azureml.pipeline.steps import PythonScriptStep\n    from azureml.pipeline.core import Pipeline\n    \n    # check core SDK version number\n    print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n    \n    # load workspace\n    workspace = Workspace.from_config()\n    print(\n        \"Workspace name: \" + workspace.name,\n        \"Azure region: \" + workspace.location,\n        \"Subscription id: \" + workspace.subscription_id,\n        \"Resource group: \" + workspace.resource_group,\n        sep=\"\\n\",\n    )\n    \n    # create an ML experiment\n    experiment = Experiment(workspace=workspace, name=\"train_score_eval_pipeline\")\n    \n    # create a directory\n    script_folder = \"./src\"\n    \n    # create compute\n    from azureml.core.compute import ComputeTarget, AmlCompute\n    from azureml.core.compute_target import ComputeTargetException\n    \n    # Choose a name for your CPU cluster\n    amlcompute_cluster_name = \"cpu-cluster\"\n    \n    # Verify that cluster does not exist already\n    try:\n        aml_compute = ComputeTarget(workspace=workspace, name=amlcompute_cluster_name)\n        print('Found existing cluster, use it.')\n    except ComputeTargetException:\n        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n                                                               max_nodes=4)\n        aml_compute = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n    \n    aml_compute.wait_for_completion(show_output=True)\n    \n    # define data set\n    data_urls = [\"wasbs://demo@dprepdata.blob.core.windows.net/Titanic.csv\"]\n    input_ds = Dataset.File.from_files(data_urls)\n    \n    # define steps in pipeline\n    from azureml.data import OutputFileDatasetConfig\n    model_output = OutputFileDatasetConfig('model_output')\n    train_step = PythonScriptStep(\n        name=\"train step\",\n        script_name=\"train.py\",\n        arguments=['--training_data', input_ds.as_named_input('training_data').as_mount() ,'--max_epocs', 5, '--learning_rate', 0.1,'--model_output', model_output],\n        source_directory=script_folder,\n        compute_target=aml_compute,\n        allow_reuse=True,\n    )\n    \n    score_output = OutputFileDatasetConfig('score_output')\n    score_step = PythonScriptStep(\n        name=\"score step\",\n        script_name=\"score.py\",\n        arguments=['--model_input',model_output.as_input('model_input'), '--test_data', input_ds.as_named_input('test_data').as_mount(), '--score_output', score_output],\n        source_directory=script_folder,\n        compute_target=aml_compute,\n        allow_reuse=True,\n    )\n    \n    eval_output = OutputFileDatasetConfig('eval_output')\n    eval_step = PythonScriptStep(\n        name=\"eval step\",\n        script_name=\"eval.py\",\n        arguments=['--scoring_result',score_output.as_input('scoring_result'), '--eval_output', eval_output],\n        source_directory=script_folder,\n        compute_target=aml_compute,\n        allow_reuse=True,\n    )\n    \n    # built pipeline\n    from azureml.pipeline.core import Pipeline\n    \n    pipeline_steps = [train_step, score_step, eval_step]\n    \n    pipeline = Pipeline(workspace = workspace, steps=pipeline_steps)\n    print(\"Pipeline is built.\")\n    \n    pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n    \n    print(\"Pipeline submitted for execution.\")\n    \n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-execution-pipeline-0.md"
                }
            },
            {
                "page_content": "\n# Use MLflow models in batch deployments\n\n[!INCLUDE [cli v2](../../includes/machine-learning-dev-v2.md)]\n\nIn this article, learn how to deploy your [MLflow](https://www.mlflow.org) model to Azure ML for both batch inference using batch endpoints. Azure Machine Learning supports no-code deployment of models created and logged with MLflow. This means that you don't have to provide a scoring script or an environment.\n\nFor no-code-deployment, Azure Machine Learning \n\n* Provides a MLflow base image/curated environment that contains the required dependencies to run an Azure Machine Learning Batch job.\n* Creates a batch job pipeline with a scoring script for you that can be used to process data using parallelization.\n\n> [!NOTE]\n> For more information about the supported file types in batch endpoints with MLflow, view [Considerations when deploying to batch inference](#considerations-when-deploying-to-batch-inference).\n\n## About this example\n\nThis example shows how you can deploy an MLflow model to a batch endpoint to perform batch predictions. This example uses an MLflow model based on the [UCI Heart Disease Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). The database contains 76 attributes, but we are using a subset of 14 of them. The model tries to predict the presence of heart disease in a patient. It is integer valued from 0 (no presence) to 1 (presence).\n\nThe model has been trained using an `XGBBoost` classifier and all the required preprocessing has been packaged as a `scikit-learn` pipeline, making this model an end-to-end pipeline that goes from raw data to predictions.\n\nThe information in this article is based on code samples contained in the [azureml-examples](https://github.com/azure/azureml-examples) repository. To run the commands locally without having to copy/paste YAML and other files, clone the repo and then change directories to the `cli/endpoints/batch` if you are using the Azure CLI or `sdk/endpoints/batch` if you are using our SDK for Python.\n\n```azurecli\ngit clone https://github.com/Azure/azureml-examples --depth 1\ncd azureml-examples/cli/endpoints/batch\n```\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in the following notebooks. In the cloned repository, open the notebook: [mlflow-for-batch-tabular.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/batch/mlflow-for-batch-tabular.ipynb).\n\n## Prerequisites\n\n[!INCLUDE [basic cli prereqs](../../includes/machine-learning-cli-prereqs.md)]\n\n* You must have a MLflow model. If your model is not in MLflow format and you want to use this feature, you can [convert your custom ML model to MLflow format](how-to-convert-custom-model-to-mlflow.md).\n\n## Steps\n\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch inference over new data:\n\n1. First, let's connect to Azure Machine Learning workspace where we are going to work on.\n\n   # [Azure CLI](#tab/cli)\n   \n   ```azurecli\n   az account set --subscription <subscription>\n   az configure --defaults workspace=<workspace> group=<resource-group> location=<location>\n   ```\n   \n   # [Python](#tab/sdk)\n   \n   The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section, we'll connect to the workspace in which you'll perform deployment tasks.\n   \n   1. Import the required libraries:\n   \n   ```python\n   from azure.ai.ml import MLClient, Input\n   from azure.ai.ml.entities import BatchEndpoint, BatchDeployment, Model, AmlCompute, Data, BatchRetrySettings\n   from azure.ai.ml.constants import AssetTypes, BatchDeploymentOutputAction\n   from azure.identity import DefaultAzureCredential\n   ```\n   \n   2. Configure workspace details and get a handle to the workspace:\n   \n   ```python\n   subscription_id = \"<subscription>\"\n   resource_group = \"<resource-group>\"\n   workspace = \"<workspace>\"\n   \n   ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)\n   ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-mlflow-batch-0.md"
                }
            }
        ]
    },
    {
        "query": "Python Code Samples to create a ManagedOnlineEndpoint with a registered model using AzureML SDK V2?",
        "result": "Here is an example Python code to create a ManagedOnlineEndpoint with a registered model using AzureML SDK V2:\n\n```python\nfrom azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, Model, Environment\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"Classify handwritten digits using a deep neural network (DNN) using TensorFlow\",\n    auth_mode=\"key\",\n)\n\nendpoint = ml_client.begin_create_or_update(endpoint).result()\n\n# create an online deployment\ndeployment = ManagedOnlineDeployment(\n    name=deployment_name,\n    endpoint_name=endpoint.name,\n    model=model,\n    environment=env,\n    code_configuration=code_config,\n    instance_type=\"Standard_NC6s_v3\",\n    instance_count=1,\n)\n\ndeployment = ml_client.begin_create_or_update(deployment).result()\n```\n\nThis code creates an online endpoint with a registered model and deploys it using a specified environment and code configuration.",
        "context": [
            {
                "page_content": "\n# Upgrade deployment endpoints to SDK v2\n\nWe newly introduced [online endpoints](concept-endpoints.md) and batch endpoints as v2 concepts. There are several deployment funnels such as managed online endpoints, [kubernetes online endpoints](how-to-attach-kubernetes-anywhere.md) (including Azure Kubernetes Services and Arc-enabled Kubernetes) in v2, and Azure Container Instances (ACI) and Kubernetes Services (AKS) webservices in v1. In this article, we'll focus on the comparison of deploying to ACI webservices (v1) and managed online endpoints (v2).\n\nExamples in this article show how to:\n\n* Deploy your model to Azure\n* Score using the endpoint\n* Delete the webservice/endpoint\n\n## Create inference resources\n\n* SDK v1\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        # configure a model. example for registering a model \n        from azureml.core.model import Model\n        model = Model.register(ws, model_name=\"bidaf_onnx\", model_path=\"./model.onnx\")\n        \n        # configure an environment\n        from azureml.core import Environment\n        env = Environment(name='myenv')\n        python_packages = ['nltk', 'numpy', 'onnxruntime']\n        for package in python_packages:\n            env.python.conda_dependencies.add_pip_package(package)\n        \n        # configure an inference configuration with a scoring script\n        from azureml.core.model import InferenceConfig\n        inference_config = InferenceConfig(\n            environment=env,\n            source_directory=\"./source_dir\",\n            entry_script=\"./score.py\",\n        )\n        ```\n\n    1. Configure and deploy an **ACI webservice**:\n        ```python\n        from azureml.core.webservice import AciWebservice\n        \n        # defince compute resources for ACI\n        deployment_config = AciWebservice.deploy_configuration(\n            cpu_cores=0.5, memory_gb=1, auth_enabled=True\n        )\n        \n        # define an ACI webservice\n        service = Model.deploy(\n            ws,\n            \"myservice\",\n            [model],\n            inference_config,\n            deployment_config,\n            overwrite=True,\n        )\n        \n        # create the service \n        service.wait_for_deployment(show_output=True)\n        ```\n\nFor more information on registering models, see [Register a model from a local file](v1/how-to-deploy-and-where.md?tabs=python#register-a-model-from-a-local-file).\n\n* SDK v2\n\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        from azure.ai.ml.entities import Model\n        # configure a model\n        model = Model(path=\"../model-1/model/sklearn_regression_model.pkl\")\n        \n        # configure an environment\n        from azure.ai.ml.entities import Environment\n        env = Environment(\n            conda_file=\"../model-1/environment/conda.yml\",\n            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n        )\n        \n        # configure an inference configuration with a scoring script\n        from azure.ai.ml.entities import CodeConfiguration\n        code_config = CodeConfiguration(\n                code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n            )\n        ```\n\n    1. Configure and create an **online endpoint**:\n        ```python\n        import datetime\n        from azure.ai.ml.entities import ManagedOnlineEndpoint\n        \n        # create a unique endpoint name with current datetime to avoid conflicts\n        online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n        \n        # define an online endpoint\n        endpoint = ManagedOnlineEndpoint(\n            name=online_endpoint_name,\n            description=\"this is a sample online endpoint\",\n            auth_mode=\"key\",\n            tags={\"foo\": \"bar\"},\n        )\n        \n        # create the endpoint:\n        ml_client.begin_create_or_update(endpoint)\n        ```\n    \n    1. Configure and create an **online deployment**:\n        ```python\n        from azure.ai.ml.entities import ManagedOnlineDeployment\n        \n        # define a deployment\n        blue_deployment = ManagedOnlineDeployment(\n            name=\"blue\",\n            endpoint_name=online_endpoint_name,\n            model=model,\n            environment=env,\n            code_configuration=code_config,\n            instance_type=\"Standard_F2s_v2\",\n            instance_count=1,\n        )\n        \n        # create the deployment:\n        ml_client.begin_create_or_update(blue_deployment)\n        \n        # blue deployment takes 100 traffic\n        endpoint.traffic = {\"blue\": 100}\n        ml_client.begin_create_or_update(endpoint)\n        ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-deploy-endpoints-0.md"
                }
            },
            {
                "page_content": "\n## Deploy the model as an online endpoint\n\nYou can now deploy your model as an [online endpoint](concept-endpoints.md)\u2014that is, as a web service in the Azure cloud.\n\nTo deploy a machine learning service, you'll typically need:\n- The model assets that you want to deploy. These assets include the model's file and metadata that you already registered in your training job.\n- Some code to run as a service. The code executes the model on a given input request (an entry script). This entry script receives data submitted to a deployed web service and passes it to the model. After the model processes the data, the script returns the model's response to the client. The script is specific to your model and must understand the data that the model expects and returns. When you use an MLFlow model, AzureML automatically creates this script for you.\n\nFor more information about deployment, see [Deploy and score a machine learning model with managed online endpoint using Python SDK v2](how-to-deploy-managed-online-endpoint-sdk-v2.md).\n\n### Create a new online endpoint\n\nAs a first step to deploying your model, you need to create your online endpoint. The endpoint name must be unique in the entire Azure region. For this article, you'll create a unique name using a universally unique identifier (UUID).\n\n```python\nimport uuid\n\n# Creating a unique name for the endpoint\nonline_endpoint_name = \"aci-birds-endpoint-\" + str(uuid.uuid4())[:8]\n```\n\n```python\nfrom azure.ai.ml.entities import ManagedOnlineEndpoint\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"Classify turkey/chickens using transfer learning with PyTorch\",\n    auth_mode=\"key\",\n    tags={\"data\": \"birds\", \"method\": \"transfer learning\", \"framework\": \"pytorch\"},\n)\n\nendpoint = ml_client.begin_create_or_update(endpoint).result()\n\nprint(f\"Endpoint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")\n```\n\nOnce you've created the endpoint, you can retrieve it as follows:\n\n```python\nendpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n\nprint(\n    f'Endpint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n)\n```\n\n### Deploy the model to the endpoint\n\nAfter you've created the endpoint, you can deploy the model with the entry script. An endpoint can have multiple deployments. Using rules, the endpoint can then direct traffic to these deployments.\n\nIn the following code, you'll create a single deployment that handles 100% of the incoming traffic. We've specified an arbitrary color name (*aci-blue*) for the deployment. You could also use any other name such as *aci-green* or *aci-red* for the deployment.\nThe code to deploy the model to the endpoint does the following:\n\n- deploys the best version of the model that you registered earlier;\n- scores the model, using the `score.py` file; and\n- uses the curated environment (that you specified earlier) to perform inferencing.\n\n```python\nfrom azure.ai.ml.entities import (\n    ManagedOnlineDeployment,\n    Model,\n    Environment,\n    CodeConfiguration,\n)\n\nonline_deployment_name = \"aci-blue\"\n\n# create an online deployment.\nblue_deployment = ManagedOnlineDeployment(\n    name=online_deployment_name,\n    endpoint_name=online_endpoint_name,\n    model=model,\n    environment=curated_env_name,\n    code_configuration=CodeConfiguration(code=\"./score/\", scoring_script=\"score.py\"),\n    instance_type=\"Standard_NC6s_v3\",\n    instance_count=1,\n)\n\nblue_deployment = ml_client.begin_create_or_update(blue_deployment).result()\n```\n\n> [!NOTE]\n> Expect this deployment to take a bit of time to finish.\n\n### Test the deployed model\n\nNow that you've deployed the model to the endpoint, you can predict the output of the deployed model, using the `invoke` method on the endpoint.\n\nTo test the endpoint, let's use a sample image for prediction. First, let's display the image.\n\n```python\n# install pillow if PIL cannot imported\n%pip install pillow\nimport json\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.imshow(Image.open(\"test_img.jpg\"))\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-train-pytorch-295.md"
                }
            },
            {
                "page_content": "We also define an early termination policy\u2014the `BanditPolicy`. This policy operates by checking the job every two iterations. If the primary metric, `validation_acc`, falls outside the top ten percent range, AzureML will terminate the job. This saves the model from continuing to explore hyperparameters that show no promise of helping to reach the target metric.\n\n```python\nfrom azure.ai.ml.sweep import BanditPolicy\n\nsweep_job = job_for_sweep.sweep(\n    compute=gpu_compute_target,\n    sampling_algorithm=\"random\",\n    primary_metric=\"validation_acc\",\n    goal=\"Maximize\",\n    max_total_trials=8,\n    max_concurrent_trials=4,\n    early_termination_policy=BanditPolicy(slack_factor=0.1, evaluation_interval=2),\n)\n```\n\nNow, you can submit this job as before. This time, you'll be running a sweep job that sweeps over your train job.\n\n```python\nreturned_sweep_job = ml_client.create_or_update(sweep_job)\n\n# stream the output and wait until the job is finished\nml_client.jobs.stream(returned_sweep_job.name)\n\n# refresh the latest status of the job after streaming\nreturned_sweep_job = ml_client.jobs.get(name=returned_sweep_job.name)\n```\n\nYou can monitor the job by using the studio user interface link that is presented during the job run.\n\n## Find and register the best model\n\nOnce all the runs complete, you can find the run that produced the model with the highest accuracy.\n\n```python\nfrom azure.ai.ml.entities import Model\n\nif returned_sweep_job.status == \"Completed\":\n\n    # First let us get the run which gave us the best result\n    best_run = returned_sweep_job.properties[\"best_child_run_id\"]\n\n    # lets get the model from this run\n    model = Model(\n        # the script stores the model as \"model\"\n        path=\"azureml://jobs/{}/outputs/artifacts/paths/outputs/model/\".format(\n            best_run\n        ),\n        name=\"run-model-example\",\n        description=\"Model created from run.\",\n        type=\"custom_model\",\n    )\n\nelse:\n    print(\n        \"Sweep job status: {}. Please wait until it completes\".format(\n            returned_sweep_job.status\n        )\n    )\n```\n\nYou can then register this model.\n\n```python\nregistered_model = ml_client.models.create_or_update(model=model)\n```\n\n\n## Deploy the model as an online endpoint\n\nAfter you've registered your model, you can deploy it as an [online endpoint](concept-endpoints.md)\u2014that is, as a web service in the Azure cloud.\n\nTo deploy a machine learning service, you'll typically need:\n- The model assets that you want to deploy. These assets include the model's file and metadata that you already registered in your training job.\n- Some code to run as a service. The code executes the model on a given input request (an entry script). This entry script receives data submitted to a deployed web service and passes it to the model. After the model processes the data, the script returns the model's response to the client. The script is specific to your model and must understand the data that the model expects and returns. When you use an MLFlow model, AzureML automatically creates this script for you.\n\nFor more information about deployment, see [Deploy and score a machine learning model with managed online endpoint using Python SDK v2](how-to-deploy-managed-online-endpoint-sdk-v2.md).\n\n### Create a new online endpoint\n\nAs a first step to deploying your model, you need to create your online endpoint. The endpoint name must be unique in the entire Azure region. For this article, you'll create a unique name using a universally unique identifier (UUID).\n\n```python\nimport uuid\n\n# Creating a unique name for the endpoint\nonline_endpoint_name = \"tff-dnn-endpoint-\" + str(uuid.uuid4())[:8]\n```\n\n```python\nfrom azure.ai.ml.entities import (\n    ManagedOnlineEndpoint,\n    ManagedOnlineDeployment,\n    Model,\n    Environment,\n)\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"Classify handwritten digits using a deep neural network (DNN) using TensorFlow\",\n    auth_mode=\"key\",\n)\n\nendpoint = ml_client.begin_create_or_update(endpoint).result()\n\nprint(f\"Endpint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-train-tensorflow-271.md"
                }
            }
        ]
    },
    {
        "query": "How to register an Azure Data Lake (gen2) storage datastore and upload CSV data using AzureML SDK V2?",
        "result": "To register an Azure Data Lake (gen2) storage datastore and upload CSV data using AzureML SDK V2, you can use the following code:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\nfrom azure.ai.ml.entities import AzureDataLakeGen2Datastore\nfrom azure.ai.ml.entities import DatastoreUploadConfig\n\nsubscription_id = '<SUBSCRIPTION_ID>'\nresource_group = '<RESOURCE_GROUP>'\nworkspace_name = '<AZUREML_WORKSPACE_NAME>'\n\nml_client = MLClient(credential=DefaultAzureCredential(),\n                     subscription_id=subscription_id, \n                     resource_group_name=resource_group)\n\ndatastore = AzureDataLakeGen2Datastore(\n    name='my_datastore',\n    storage_account_name='my_storage_account_name',\n    file_system='my_file_system',\n    tenant_id='my_tenant_id',\n    client_id='my_client_id',\n    client_secret='my_client_secret'\n)\n\nml_client.datastores.create_or_update(datastore)\n\nupload_config = DatastoreUploadConfig(\n    datastore_name='my_datastore',\n    target_path='my_folder/my_file.csv',\n    overwrite=True\n)\n\nml_client.datastore.upload_files(\n    upload_config=upload_config,\n    files=['./my_file.csv']\n)\n```\n\nFor more information, see the [Azure Data Lake Gen2 Datastore documentation](https://docs.microsoft.com/en-us/python/api/azure.ai.ml.entities.azuredatalakegen2datastore?view=azure-ai-ml-py).",
        "context": [
            {
                "page_content": "\n# Data concepts in Azure Machine Learning\n\n> [!div class=\"op_single_selector\" title1=\"Select the version of Azure Machine Learning developer platform you use:\"]\n> * [v1](./v1/concept-data.md)\n> * [v2 (current version)](concept-data.md)\n\nWith Azure Machine Learning, you can bring data from a local machine or an existing cloud-based storage. In this article, you'll learn the main Azure Machine Learning data concepts.\n\n## URI\nA Uniform Resource Identifier (URI) represents a storage location on your local computer, Azure storage, or a publicly available http(s) location. These examples show URIs for different storage options:\n\n|Storage location  | URI examples  |\n|---------|---------|\n|Local computer     | `./home/username/data/my_data`         |\n|Public http(s) server    |  `https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv`    |\n|Blob storage    | `wasbs://<containername>@<accountname>.blob.core.windows.net/<folder>/`|\n|Azure Data Lake (gen2) | `abfss://<file_system>@<account_name>.dfs.core.windows.net/<folder>/<file>.csv`  |\n| Azure Data Lake (gen1) | `adl://<accountname>.azuredatalakestore.net/<folder1>/<folder2>` \n|Azure ML [Datastore](#datastore)  |   `azureml://datastores/<data_store_name>/paths/<folder1>/<folder2>/<folder3>/<file>.parquet`      |\n\nAn Azure ML job maps URIs to the compute target filesystem. This mapping means that in a command that consumes or produces a URI, that URI works like a file or a folder. A URI uses **identity-based authentication** to connect to storage services, with either your Azure Active Directory ID (default), or Managed Identity. Azure ML [Datastore](#datastore) URIs can apply either identity-based authentication, or **credential-based** (for example, Service Principal, SAS token, account key) without exposure of secrets.\n\nA URI can serve as either *input* or an *output* to an Azure ML job, and it can map to the compute target filesystem with one of four different *mode* options:\n\n- **Read-*only* mount (`ro_mount`)**: The URI represents a storage location that is *mounted* to the compute target filesystem. The mounted data location supports read-only output exclusively.\n- **Read-*write* mount (`rw_mount`)**: The URI represents a storage location that is *mounted* to the compute target filesystem. The mounted data location supports both read output from it *and* data writes to it.\n- **Download (`download`)**: The URI represents a storage location containing data that is *downloaded* to the compute target filesystem.\n- **Upload (`upload`)**: All data written to a compute target location is *uploaded* to the storage location represented by the URI.\n\nAdditionally, you can pass in the URI as a job input string with the **direct** mode. This table summarizes the combination of modes available for inputs and outputs:\n\nJob<br>Input or Output | `upload` | `download` | `ro_mount` | `rw_mount` | `direct` | \n------ | :---: | :---: | :---: | :---: | :---: | \nInput  |   | \u2713  |  \u2713  |   | \u2713 |  \nOutput  | \u2713  |   |    | \u2713  |  \n\nRead [Access data in a job](how-to-read-write-data-v2.md) for more information.\n\n## Data types\n\nA URI (storage location) can reference a file, a folder, or a data table. A machine learning job input and output definition requires one of the following three data types:\n\n|Type  |V2 API  |V1 API  |Canonical Scenarios | V2/V1 API Difference\n|---------|---------|---------|---------|---------|\n|**File**<br>Reference a single file     |    `uri_file`     |   `FileDataset`      |       Read/write a single file - the file can have any format.   |  A type new to V2 APIs. In V1 APIs, files always mapped to a folder on the compute target filesystem; this mapping required an `os.path.join`. In V2 APIs, the single file is mapped. This way, you can refer to that location in your code.   |\n|**Folder**<br> Reference a single folder     |     `uri_folder`    |   `FileDataset`      |  You must read/write a folder of parquet/CSV files into Pandas/Spark.<br><br>Deep-learning with images, text, audio, video files located in a folder.       | In V1 APIs, `FileDataset` had an associated engine that could take a file sample from a folder. In V2 APIs, a Folder is a simple mapping to the compute target filesystem. |\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-data-0.md"
                }
            },
            {
                "page_content": "\n## Get datastores from your workspace\n\n* SDK v1\n\n    ```python\n    # Get a named datastore from the current workspace\n    datastore = Datastore.get(ws, datastore_name='your datastore name')\n    ```\n    \n    ```python\n    # List all datastores registered in the current workspace\n    datastores = ws.datastores\n    for name, datastore in datastores.items():\n        print(name, datastore.datastore_type)\n    ```\n\n* SDK v2\n    \n    ```python\n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n    \n    #Enter details of your AzureML workspace\n    subscription_id = '<SUBSCRIPTION_ID>'\n    resource_group = '<RESOURCE_GROUP>'\n    workspace_name = '<AZUREML_WORKSPACE_NAME>'\n    \n    ml_client = MLClient(credential=DefaultAzureCredential(),\n                         subscription_id=subscription_id, \n                         resource_group_name=resource_group)\n    \n    datastore = ml_client.datastores.get(name='your datastore name')\n    ```\n\n## Mapping of key functionality in SDK v1 and SDK v2\n\n|Storage types in SDK v1|Storage types in SDK v2|\n|--------------|-------------------|\n|[azureml_blob_datastore](/python/api/azureml-core/azureml.data.azure_storage_datastore.azureblobdatastore?view=azure-ml-py&preserve-view=true)|[azureml_blob_datastore](/python/api/azure-ai-ml/azure.ai.ml.entities.azuredatalakegen1datastore)|\n|[azureml_data_lake_gen1_datastore](/python/api/azureml-core/azureml.data.azure_data_lake_datastore.azuredatalakedatastore?view=azure-ml-py&preserve-view=true)|[azureml_data_lake_gen1_datastore](/python/api/azure-ai-ml/azure.ai.ml.entities.azuredatalakegen1datastore)|\n|[azureml_data_lake_gen2_datastore](/python/api/azureml-core/azureml.data.azure_data_lake_datastore.azuredatalakegen2datastore?view=azure-ml-py&preserve-view=true)|[azureml_data_lake_gen2_datastore](/python/api/azure-ai-ml/azure.ai.ml.entities.azuredatalakegen2datastore)|\n|[azuremlml_sql_database_datastore](/python/api/azureml-core/azureml.data.azure_sql_database_datastore.azuresqldatabasedatastore?view=azure-ml-py&preserve-view=true)|Will be supported via import & export functionalities|\n|[azuremlml_my_sql_datastore](/python/api/azureml-core/azureml.data.azure_my_sql_datastore.azuremysqldatastore?view=azure-ml-py&preserve-view=true)|Will be supported via import & export functionalities|\n|[azuremlml_postgre_sql_datastore](/python/api/azureml-core/azureml.data.azure_postgre_sql_datastore.azurepostgresqldatastore?view=azure-ml-py&preserve-view=true)|Will be supported via import & export functionalities|\n\n\n## Next steps\n\nFor more information, see:\n\n* [Create datastores](how-to-datastore.md?tabs=cli-identity-based-access%2Csdk-adls-sp%2Csdk-azfiles-sas%2Csdk-adlsgen1-sp)\n* [Read and write data in a job](how-to-read-write-data-v2.md)\n* [V2 datastore operations](/python/api/azure-ai-ml/azure.ai.ml.operations.datastoreoperations)\n\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-resource-datastore-113.md"
                }
            },
            {
                "page_content": "\nThis [Jupyter notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/resources/compute/compute.ipynb) shows more ways to create compute using SDK v2.\n\n\n## Datastore\n\nAzure Machine Learning datastores securely keep the connection information to your data storage on Azure, so you don't have to code it in your scripts. You can register and create a datastore to easily connect to your storage account, and access the data in your underlying storage service. The CLI v2 and SDK v2 support the following types of cloud-based storage services:\n\n* Azure Blob Container\n* Azure File Share\n* Azure Data Lake\n* Azure Data Lake Gen2\n\n### [Azure CLI](#tab/cli)\n\nTo create a datastore using CLI v2, use the following command:\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```bash\naz ml datastore create --file my_datastore.yml\n```\nFor more information, see [datastore YAML schema](reference-yaml-overview.md#datastore).\n\n\n### [Python SDK](#tab/sdk)\n\nTo create a datastore using Python SDK v2, you can use the following code:\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nblob_datastore1 = AzureBlobDatastore(\n    name=\"blob-example\",\n    description=\"Datastore pointing to a blob container.\",\n    account_name=\"mytestblobstore\",\n    container_name=\"data-container\",\n    credentials={\n        \"account_key\": \"XXXxxxXXXxXXXXxxXXXXXxXXXXXxXxxXxXXXxXXXxXXxxxXXxxXXXxXxXXXxxXxxXXXXxxxxxXXxxxxxxXXXxXXX\"\n    },\n)\nml_client.create_or_update(blob_datastore1)\n```\n\nThis [Jupyter notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/resources/datastores/datastore.ipynb) shows more ways to create datastores using SDK v2.\n\n\n## Model\n\nAzure machine learning models consist of the binary file(s) that represent a machine learning model and any corresponding metadata. Models can be created from a local or remote file or directory. For remote locations `https`, `wasbs` and `azureml` locations are supported. The created model will be tracked in the workspace under the specified name and version. Azure ML supports three types of storage format for models:\n\n* `custom_model`\n* `mlflow_model`\n* `triton_model`\n\n### Creating a model\n\n### [Azure CLI](#tab/cli)\n\nTo create a model using CLI v2, use the following command:\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```bash\naz ml model create --file my_model.yml\n```\n\nFor more information, see [model YAML schema](reference-yaml-model.md).\n\n\n### [Python SDK](#tab/sdk)\n\nTo create a model using Python SDK v2, you can use the following code:\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nmy_model = Model(\n    path=\"model.pkl\", # the path to where my model file is located\n    type=\"custom_model\", # can be custom_model, mlflow_model or triton_model\n    name=\"my-model\",\n    description=\"Model created from local file.\",\n)\n\nml_client.models.create_or_update(my_model) # use the MLClient to connect to workspace and create/register the model\n```\n\n\n## Environment\n\nAzure Machine Learning environments are an encapsulation of the environment where your machine learning task happens. They specify the software packages, environment variables, and software settings around your training and scoring scripts. The environments are managed and versioned entities within your Machine Learning workspace. Environments enable reproducible, auditable, and portable machine learning workflows across a variety of computes.\n\n### Types of environment\n\nAzure ML supports two types of environments: curated and custom.\n\nCurated environments are provided by Azure Machine Learning and are available in your workspace by default. Intended to be used as is, they contain collections of Python packages and settings to help you get started with various machine learning frameworks. These pre-created environments also allow for faster deployment time. For a full list, see the [curated environments article](resource-curated-environments.md).\n\nIn custom environments, you're responsible for setting up your environment and installing packages or any other dependencies that your training or scoring script needs on the compute. Azure ML allows you to create your own environment using\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-azure-machine-learning-v2-98.md"
                }
            }
        ]
    },
    {
        "query": "How to register a blob storage datastore and upload CSV data using AzureML SDK V2?",
        "result": "To register a blob storage datastore and upload CSV data using AzureML SDK V2, you can use the following Python code:\n\n```python\nfrom azureml.core import Workspace, Datastore, Dataset\n\n# Get the workspace\nws = Workspace.from_config()\n\n# Register a blob storage datastore\nblob_datastore_name = 'my_blob_datastore'\ncontainer_name = 'my_container'\naccount_name = 'my_account_name'\naccount_key = 'my_account_key'\nblob_datastore = Datastore.register_azure_blob_container(workspace=ws,\n                                                         datastore_name=blob_datastore_name,\n                                                         container_name=container_name,\n                                                         account_name=account_name,\n                                                         account_key=account_key)\n\n# Upload CSV data to the datastore\ncsv_path = 'path/to/csv/file.csv'\ncsv_dataset = Dataset.Tabular.from_delimited_files(path=[(blob_datastore, csv_path)])\ncsv_dataset = csv_dataset.register(workspace=ws,\n                                   name='my_csv_dataset',\n                                   create_new_version=True)\n```\n\nReplace the placeholders with your own values for `blob_datastore_name`, `container_name`, `account_name`, `account_key`, and `csv_path`.",
        "context": [
            {
                "page_content": "If you have previously labeled data that you would like to use to train your model, you will first need to upload the images to the default Azure Blob Storage of your Azure ML Workspace and register it as a [data asset](how-to-create-data-assets.md). \n\nThe following script uploads the image data on your local machine at path \"./data/odFridgeObjects\" to datastore in Azure Blob Storage. It then creates a new data asset with the name \"fridge-items-images-object-detection\" in your Azure ML Workspace. \n\nIf there already exists a data asset with the name \"fridge-items-images-object-detection\" in your Azure ML Workspace, it will update the version number of the data asset and point it to the new location where the image data uploaded.\n\n# [Azure CLI](#tab/cli)\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nCreate a .yml file with the following configuration.\n\n```yml\n$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json\nname: fridge-items-images-object-detection\ndescription: Fridge-items images Object detection\npath: ./data/odFridgeObjects\ntype: uri_folder\n```\n\nTo upload the images as a data asset, you run the following CLI v2 command with the path to your .yml file, workspace name, resource group and subscription ID.\n\n```azurecli\naz ml data create -f [PATH_TO_YML_FILE] --workspace-name [YOUR_AZURE_WORKSPACE] --resource-group [YOUR_AZURE_RESOURCE_GROUP] --subscription [YOUR_AZURE_SUBSCRIPTION]\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=upload-data)]\n\n# [Studio](#tab/Studio)\n\n![Animation showing how to register a dataset from local files](media\\how-to-prepare-datasets-for-automl-images\\ui-dataset-local.gif)\n\n\nIf you already have your data present in an existing datastore and want to create a data asset out of it, you can do so by providing the path to the data in the datastore, instead of providing the path of your local machine. Update the code [above](how-to-prepare-datasets-for-automl-images.md#using-pre-labeled-training-data-from-local-machine) with the following snippet.\n\n# [Azure CLI](#tab/cli)\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nCreate a .yml file with the following configuration.\n\n```yml\n$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json\nname: fridge-items-images-object-detection\ndescription: Fridge-items images Object detection\npath: azureml://subscriptions/<my-subscription-id>/resourcegroups/<my-resource-group>/workspaces/<my-workspace>/datastores/<my-datastore>/paths/<path_to_image_data_folder>\ntype: uri_folder\n```\n\n# [Python SDK](#tab/python)\n\n \n```Python\nmy_data = Data(\n    path=\"azureml://subscriptions/<my-subscription-id>/resourcegroups/<my-resource-group>/workspaces/<my-workspace>/datastores/<my-datastore>/paths/<path_to_image_data_folder>\",\n    type=AssetTypes.URI_FOLDER,\n    description=\"Fridge-items images Object detection\",\n    name=\"fridge-items-images-object-detection\",\n)\n```\n\n# [Studio](#tab/Studio)\n\n![Animation showing how to register a dataset from data already present in datastore](media\\how-to-prepare-datasets-for-automl-images\\ui-dataset-datastore.gif)\n\n\nNext, you will need to get the label annotations in JSONL format. The schema of labeled data depends on the computer vision task at hand. Refer to [schemas for JSONL files for AutoML computer vision experiments](reference-automl-images-schema.md) to learn more about the required JSONL schema for each task type.\n\nIf your training data is in a different format (like, pascal VOC or COCO), [helper scripts](https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/image-object-detection/coco2jsonl.py) to convert the data to JSONL are available in [notebook examples](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/automl-standalone-jobs).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-prepare-datasets-for-automl-images-70.md"
                }
            },
            {
                "page_content": "\n# [Python](#tab/Python-SDK)\n\n```python\nfrom azure.ai.ml.entities import Data\nfrom azure.ai.ml.constants import AssetTypes\n\nmy_path = './data'\n\nmy_data = Data(\n    path=my_path,\n    type=AssetTypes.MLTABLE,\n    name=\"green-sample\",\n    version='1'\n)\n\nml_client.data.create_or_update(my_data)\n```\n\n> [!NOTE]\n> Your local data folder - containing the parquet file and MLTable - will automatically upload to cloud storage (default Azure ML datastore) on asset create.\n\n### Step 4: Create a job\n\nCreate a Python script called `read-mltable.py` in an `src` folder that contains:\n\n```python\n# ./src/read-mltable.py\nimport argparse\nimport mltable\n\n# parse arguments\nparser = argparse.ArgumentParser()\nparser.add_argument('--input', help='mltable artifact to read')\nargs = parser.parse_args()\n\n# load mltable\ntbl = mltable.load(args.input)\n\n# show table\nprint(tbl.show())\n```\n\nTo keep things simple, we only show how to read the table into Pandas, and print the first few records.\n\nYour job will need a Conda file that includes the Python package dependencies. Save that Conda file as `conda_dependencies.yml`:\n\n```yml\n# ./conda_dependencies.yml\ndependencies:\n  - python=3.10\n  - pip=21.2.4\n  - pip:\n      - mltable\n      - azureml-dataprep[pandas]\n```\n\nNext, submit the job:\n\n# [CLI](#tab/cli)\n\nCreate the following job YAML file:\n\n```yml\n# mltable-job.yml\n$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n\ncode: ./src\n\ncommand: python read-mltable.py --input ${{inputs.my_mltable}}\ninputs:\n    my_mltable:\n      type: mltable\n      path: azureml:green-sample:1\n\ncompute: cpu-cluster\n\nenvironment:\n  image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n  conda_file: conda_dependencies.yml\n```\n\nIn the CLI, create the job:\n\n```azurecli\naz ml job create -f mltable-job.yml\n```\n\n# [Python](#tab/Python-SDK)\n\n```python\nfrom azure.ai.ml import MLClient, command, Input\nfrom azure.ai.ml.entities import Environment\nfrom azure.identity import DefaultAzureCredential\n\n# Create a client\nml_client = MLClient.from_config(credential=DefaultAzureCredential())\n\n# get the data asset\ndata_asset = ml_client.data.get(name=\"green-sample\", version=\"1\")\n\njob = command(\n    command=\"python read-mltable.py --input ${{inputs.my_mltable}}\",\n    inputs={\n        \"my_mltable\": Input(type=\"mltable\",path=data_asset.id)\n    },\n    compute=\"cpu-cluster\",\n    environment=Environment(\n        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n        conda_file=\"./conda_dependencies.yml\"\n    ),\n    code=\"./src\"\n)\n\nml_client.jobs.create_or_update(job)\n```\n\n\n## `MLTable` file examples\n\n### Delimited Text (a CSV file)\n\nThis example assumes you have a CSV file stored in the following Azure Data Lake location:\n\n- `abfss://<filesystem>@<account_name>.dfs.core.windows.net/<folder>/<file-name>.csv`\n\n> [!NOTE]\n> You must update the `<>` placeholders for your Azure Data Lake filesystem and account name, along with the path on Azure Data lake to your CSV file.\n\n# [CLI](#tab/cli)\nCreate an `MLTable` file in the `abfss://<filesystem>@<account_name>.dfs.core.windows.net/<folder>/` location:\n    \n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/MLTable.schema.json\n\ntype: mltable\n\npaths:\n  - file: ./<file-name>.csv\n\ntransformations:\n  - read_delimited:\n      delimiter: ',' \n      empty_as_string: false \n      encoding: utf8 \n      header: all_files_same_headers\n      include_path_column: false \n      infer_column_types: true \n      support_multi_line: false\n```\n\nIf you don't already use [Option 1: Directly author `MLTable` in cloud storage with VSCode](#option-1-directly-author-mltable-in-cloud-storage-with-vscode), then you can upload your `MLTable` file with `azcopy`:\n\n```bash\nSOURCE=<local_path-to-mltable-file>\nDEST=https://<account_name>.blob.core.windows.net/<filesystem>/<folder>\nazcopy cp $SOURCE $DEST\n```\n\n# [Python](#tab/Python-SDK)\n\n```python\nimport mltable\nfrom mltable import MLTableHeaders, MLTableFileEncoding\nfrom azure.storage.blob import BlobClient\nfrom azure.identity import DefaultAzureCredential\n\n# update the file name\nmy_path = {\n    'file': './<file_name>.csv'\n}\n\ntbl = mltable.from_delimited_files(\n    paths=[my_path],\n    header=MLTableHeaders.all_files_same_headers,\n    delimiter=',',\n    encoding=MLTableFileEncoding.utf8,\n    empty_as_string=False,\n    include_path_column=False,\n    infer_column_types=True,\n    support_multi_line=False)\n\n# save the table to the local file system\nlocal_folder = \"local\"\ntbl.save(local_folder)\n\n# upload the MLTable file to your storage account so that you have an artifact\nstorage_account_url = \"https://<account_name>.blob.core.windows.net\"\ncontainer_name = \"<filesystem>\"\ndata_folder_on_storage = '<folder>'\n\n# get a blob client using default credential\nblob_client = BlobClient(\n    credential=DefaultAzureCredential(), \n    account_url=storage_account_url, \n    container_name=container_name,\n    blob_name=f'{data_folder_on_storage}/MLTable'\n)\n\n# upload to cloud storage\nwith open(f'{local_folder}/MLTable', \"rb\") as mltable_file:\n    blob_client.upload_blob(mltable_file)\n\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-mltable-530.md"
                }
            },
            {
                "page_content": "    ![Screenshot showing how to connect the Export Data components](media/how-to-designer-transform-data/export-data-pipeline.png).\n\n1. Select the **Export Data** component connected to the *left*-most port of the **Split Data** component.\n\n    For the **Split Data** component, the output port order matters. The first output port contains the rows where the regular expression is true. In this case, the first port contains rows for US-based income, and the second port contains rows for non-US based income.\n\n1. In the component details pane to the right of the canvas, set the following options:\n    \n    **Datastore type**: Azure Blob Storage\n\n    **Datastore**: Select an existing datastore, or select \"New datastore\" to create one now.\n\n    **Path**: `/data/us-income`\n\n    **File format**: csv\n\n    > [!NOTE]\n    > This article assumes that you have access to a datastore registered to the current Azure Machine Learning workspace. See [Connect to Azure storage services](v1/how-to-connect-data-ui.md#create-datastores) for datastore setup instructions.\n\n    You can create a datastore if you don't have one now. For example purposes, this article will save the datasets to the default blob storage account associated with the workspace. It will save the datasets into the `azureml` container, in a new folder named `data`.\n\n1.  Select the **Export Data** component connected to the *right*-most port of the **Split Data** component.\n\n1. To the right of the canvas in the component details pane, set the following options:\n    \n    **Datastore type**: Azure Blob Storage\n\n    **Datastore**: Select the same datastore as above\n\n    **Path**: `/data/non-us-income`\n\n    **File format**: csv\n\n1. Verify that the **Export Data** component connected to the left port of the **Split Data** has the **Path** `/data/us-income`.\n\n1. Verify that the **Export Data** component connected to the right port has the **Path** `/data/non-us-income`.\n\n    Your pipeline and settings should look like this:\n    \n    ![Screenshot showing how to configure the Export Data components](media/how-to-designer-transform-data/us-income-export-data.png).\n\n### Submit the job\n\nNow that you set up your pipeline to split and export the data, submit a pipeline job.\n\n1. Select **Submit** at the top of the canvas.\n\n1. Select **Create new** in the **Set up pipeline job**, to create an experiment.\n\n    Experiments logically group related pipeline jobs together. If you run this pipeline in the future, you should use the same experiment for logging and tracking purposes.\n\n1. Provide a descriptive experiment name - for example \"split-census-data\".\n\n1. Select **Submit**.\n\n## View results\n\nAfter the pipeline finishes running, you can navigate to your Azure portal blob storage to view your results. You can also view the intermediary results of the **Split Data** component to confirm that your data has been split correctly.\n\n1. Select the **Split Data** component.\n\n1. In the component details pane to the right of the canvas, select **Outputs + logs**.\n\n1. Select the visualize icon ![visualize icon](media/how-to-designer-transform-data/visualize-icon.png) next to **Results dataset1**.\n\n1. Verify that the \"native-country\" column contains only the value \"United-States\".\n\n1. Select the visualize icon ![visualize icon](media/how-to-designer-transform-data/visualize-icon.png) next to **Results dataset2**.\n\n1. Verify that the \"native-country\" column does not contain the value \"United-States\".\n\n## Clean up resources\n\nTo continue with part two of this [Retrain models with Azure Machine Learning designer](how-to-retrain-designer.md) how-to, skip this section.\n\n[!INCLUDE [aml-ui-cleanup](../../includes/aml-ui-cleanup.md)]\n\n## Next steps\n\nIn this article, you learned how to transform a dataset, and save it to a registered datastore.\n\nContinue to the next part of this how-to series with [Retrain models with Azure Machine Learning designer](how-to-retrain-designer.md), to use your transformed datasets and pipeline parameters to train machine learning models.",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-designer-transform-data-75.md"
                }
            }
        ]
    },
    {
        "query": "Code Samples to create a local endpoint using AzureML SDK V2 and ManagedOnlineEndpoint ?",
        "result": "To create a local endpoint using AzureML SDK V2 and ManagedOnlineEndpoint, you can use the `azure.ai.ml.entities.ManagedOnlineEndpoint` class to define the endpoint and the `azure.ai.ml.entities.ManagedOnlineDeployment` class to define the deployment. You can then use the `azure.ai.ml.MLClient` class to create or update the endpoint and deployment. Set the `local` and `vscode_debug` flags to `True` to launch a development container for local debugging. For more information and code examples, see the [Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-debug-online-endpoints-locally-vscode).",
        "context": [
            {
                "page_content": "\n1. Create a YAML configuration file for your endpoint. The following example configures the name and authentication mode of the endpoint. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-endpoint.yml` in the azureml-examples repo you cloned earlier:\n\n    __create-managed-endpoint.yaml__\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\nname: my-endpoint\nauth_mode: aml_token\n```\n\n1. Create a YAML configuration file for the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step. The one used in the following commands is located at `/cli/endpoints/online/triton/single-model/create-managed-deployment.yml` in the azureml-examples repo you cloned earlier:\n\n    > [!IMPORTANT]\n    > For Triton no-code-deployment (NCD) to work, setting **`type`** to **`triton_model\u200b`** is required, `type: triton_model\u200b`. For more information, see [CLI (v2) model YAML schema](reference-yaml-model.md).\n    >\n    > This deployment uses a Standard_NC6s_v3 VM. You may need to request a quota increase for your subscription before you can use this VM. For more information, see [NCv3-series](../virtual-machines/ncv3-series.md).\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: my-endpoint\nmodel:\n  name: sample-densenet-onnx-model\n  version: 1\n  path: ./models\n  type: triton_model\ninstance_count: 1\ninstance_type: Standard_NC6s_v3\n```\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nThis section shows how you can define a Triton deployment to deploy to a managed online endpoint using the Azure Machine Learning Python SDK (v2).\n\n> [!IMPORTANT]\n> For Triton no-code-deployment, **[testing via local endpoints](how-to-deploy-online-endpoints.md#deploy-and-debug-locally-by-using-local-endpoints)** is currently not supported.\n\n\n1. To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. \n\n    ```python \n    subscription_id = \"<SUBSCRIPTION_ID>\"\n    resource_group = \"<RESOURCE_GROUP>\"\n    workspace_name = \"<AML_WORKSPACE_NAME>\"\n    ```\n\n1. Use the following command to set the name of the endpoint that will be created. In this example, a random name is created for the endpoint:\n\n    ```python\n    import random\n\n    endpoint_name = f\"endpoint-{random.randint(0, 10000)}\"\n    ```\n\n1. We use these details above in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. Check the [configuration notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n\n    ```python \n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n\n    ml_client = MLClient(\n        DefaultAzureCredential(),\n        subscription_id,\n        resource_group,\n        workspace_name,\n    )\n    ```\n\n1. Create a `ManagedOnlineEndpoint` object to configure the endpoint. The following example configures the name and authentication mode of the endpoint. \n\n    ```python \n    from azure.ai.ml.entities import ManagedOnlineEndpoint\n\n    endpoint = ManagedOnlineEndpoint(name=endpoint_name, auth_mode=\"key\")\n    ```\n\n1. Create a `ManagedOnlineDeployment` object to configure the deployment. The following example configures a deployment named __blue__ to the endpoint defined in the previous step and defines a local model inline.\n\n    ```python\n    from azure.ai.ml.entities import ManagedOnlineDeployment, Model\n    \n    model_name = \"densenet-onnx-model\"\n    model_version = 1\n    \n    deployment = ManagedOnlineDeployment(\n        name=\"blue\",\n        endpoint_name=endpoint_name,\n        model=Model(\n            name=model_name, \n            version=model_version,\n            path=\"./models\",\n            type=\"triton_model\"\n        ),\n        instance_type=\"Standard_NC6s_v3\",\n        instance_count=1,\n    )\n    ``` \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-with-triton-174.md"
                }
            },
            {
                "page_content": "\n# Upgrade deployment endpoints to SDK v2\n\nWe newly introduced [online endpoints](concept-endpoints.md) and batch endpoints as v2 concepts. There are several deployment funnels such as managed online endpoints, [kubernetes online endpoints](how-to-attach-kubernetes-anywhere.md) (including Azure Kubernetes Services and Arc-enabled Kubernetes) in v2, and Azure Container Instances (ACI) and Kubernetes Services (AKS) webservices in v1. In this article, we'll focus on the comparison of deploying to ACI webservices (v1) and managed online endpoints (v2).\n\nExamples in this article show how to:\n\n* Deploy your model to Azure\n* Score using the endpoint\n* Delete the webservice/endpoint\n\n## Create inference resources\n\n* SDK v1\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        # configure a model. example for registering a model \n        from azureml.core.model import Model\n        model = Model.register(ws, model_name=\"bidaf_onnx\", model_path=\"./model.onnx\")\n        \n        # configure an environment\n        from azureml.core import Environment\n        env = Environment(name='myenv')\n        python_packages = ['nltk', 'numpy', 'onnxruntime']\n        for package in python_packages:\n            env.python.conda_dependencies.add_pip_package(package)\n        \n        # configure an inference configuration with a scoring script\n        from azureml.core.model import InferenceConfig\n        inference_config = InferenceConfig(\n            environment=env,\n            source_directory=\"./source_dir\",\n            entry_script=\"./score.py\",\n        )\n        ```\n\n    1. Configure and deploy an **ACI webservice**:\n        ```python\n        from azureml.core.webservice import AciWebservice\n        \n        # defince compute resources for ACI\n        deployment_config = AciWebservice.deploy_configuration(\n            cpu_cores=0.5, memory_gb=1, auth_enabled=True\n        )\n        \n        # define an ACI webservice\n        service = Model.deploy(\n            ws,\n            \"myservice\",\n            [model],\n            inference_config,\n            deployment_config,\n            overwrite=True,\n        )\n        \n        # create the service \n        service.wait_for_deployment(show_output=True)\n        ```\n\nFor more information on registering models, see [Register a model from a local file](v1/how-to-deploy-and-where.md?tabs=python#register-a-model-from-a-local-file).\n\n* SDK v2\n\n    1. Configure a model, an environment, and a scoring script:\n        ```python\n        from azure.ai.ml.entities import Model\n        # configure a model\n        model = Model(path=\"../model-1/model/sklearn_regression_model.pkl\")\n        \n        # configure an environment\n        from azure.ai.ml.entities import Environment\n        env = Environment(\n            conda_file=\"../model-1/environment/conda.yml\",\n            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n        )\n        \n        # configure an inference configuration with a scoring script\n        from azure.ai.ml.entities import CodeConfiguration\n        code_config = CodeConfiguration(\n                code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n            )\n        ```\n\n    1. Configure and create an **online endpoint**:\n        ```python\n        import datetime\n        from azure.ai.ml.entities import ManagedOnlineEndpoint\n        \n        # create a unique endpoint name with current datetime to avoid conflicts\n        online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n        \n        # define an online endpoint\n        endpoint = ManagedOnlineEndpoint(\n            name=online_endpoint_name,\n            description=\"this is a sample online endpoint\",\n            auth_mode=\"key\",\n            tags={\"foo\": \"bar\"},\n        )\n        \n        # create the endpoint:\n        ml_client.begin_create_or_update(endpoint)\n        ```\n    \n    1. Configure and create an **online deployment**:\n        ```python\n        from azure.ai.ml.entities import ManagedOnlineDeployment\n        \n        # define a deployment\n        blue_deployment = ManagedOnlineDeployment(\n            name=\"blue\",\n            endpoint_name=online_endpoint_name,\n            model=model,\n            environment=env,\n            code_configuration=code_config,\n            instance_type=\"Standard_F2s_v2\",\n            instance_count=1,\n        )\n        \n        # create the deployment:\n        ml_client.begin_create_or_update(blue_deployment)\n        \n        # blue deployment takes 100 traffic\n        endpoint.traffic = {\"blue\": 100}\n        ml_client.begin_create_or_update(endpoint)\n        ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-deploy-endpoints-0.md"
                }
            },
            {
                "page_content": "The examples in this article can be found in the [Debug online endpoints locally in Visual Studio Code](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/online/managed/debug-online-endpoints-locally-in-visual-studio-code.ipynb) notebook within the[azureml-examples](https://github.com/azure/azureml-examples) repository. To run the code locally, clone the repo and then change directories to the notebook's parent directory `sdk/endpoints/online/managed`. \n\n```azurecli\ngit clone https://github.com/Azure/azureml-examples --depth 1\ncd azureml-examples\ncd sdk/python/endpoints/online/managed\n```\n\nImport the required modules: \n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import (\n    ManagedOnlineEndpoint,\n    ManagedOnlineDeployment,\n    Model,\n    CodeConfiguration,\n    Environment,\n)\nfrom azure.identity import DefaultAzureCredential\n``` \n\nSet up variables for the workspace and endpoint: \n\n```python \nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace_name = \"<AML_WORKSPACE_NAME>\"\n\nendpoint_name = \"<ENDPOINT_NAME>\"\n``` \n\n\n## Launch development container\n\n# [Azure CLI](#tab/cli)\n\nAzure Machine Learning local endpoints use Docker and VS Code development containers (dev container) to build and configure a local debugging environment. With dev containers, you can take advantage of VS Code features from inside a Docker container. For more information on dev containers, see [Create a development container](https://code.visualstudio.com/docs/remote/create-dev-container).\n\nTo debug online endpoints locally in VS Code, use the `--vscode-debug` flag when creating or updating and Azure Machine Learning online deployment. The following command uses a deployment example from the examples repo:\n\n```azurecli\naz ml online-deployment create --file endpoints/online/managed/sample/blue-deployment.yml --local --vscode-debug\n```\n\n> [!IMPORTANT]\n> On Windows Subsystem for Linux (WSL), you'll need to update your PATH environment variable to include the path to the VS Code executable or use WSL interop. For more information, see [Windows interoperability with Linux](/windows/wsl/interop).\n\nA Docker image is built locally. Any environment configuration or model file errors are surfaced at this stage of the process.\n\n> [!NOTE]\n> The first time you launch a new or updated dev container it can take several minutes.\n\nOnce the image successfully builds, your dev container opens in a VS Code window.\n\nYou'll use a few VS Code extensions to debug your deployments in the dev container. Azure Machine Learning automatically installs these extensions in your dev container.\n\n- Inference Debug\n- [Pylance](https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance)\n- [Jupyter](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter)\n- [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python)\n\n> [!IMPORTANT]\n> Before starting your debug session, make sure that the VS Code extensions have finished installing in your dev container.  \n\n\n# [Python](#tab/python)\n\nAzure Machine Learning local endpoints use Docker and VS Code development containers (dev container) to build and configure a local debugging environment. With dev containers, you can take advantage of VS Code features from inside a Docker container. For more information on dev containers, see [Create a development container](https://code.visualstudio.com/docs/remote/create-dev-container).\n\nGet a handle to the workspace: \n\n```python \ncredential = DefaultAzureCredential()\nml_client = MLClient(\n    credential,\n    subscription_id=subscription_id,\n    resource_group_name=resource_group,\n    workspace_name=workspace_name,\n)\n``` \n\nTo debug online endpoints locally in VS Code, set the `vscode-debug` and `local` flags when creating or updating an Azure Machine Learning online deployment. The following code mirrors a deployment example from the examples repo:\n\n```python\ndeployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=endpoint_name,\n    model=Model(path=\"../model-1/model/sklearn_regression_model.pkl\"),\n    code_configuration=CodeConfiguration(\n        code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n    ),\n    environment=Environment(\n        conda_file=\"../model-1/environment/conda.yml\",\n        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n    ),\n    instance_type=\"Standard_DS3_v2\",\n    instance_count=1,\n)\n\ndeployment = ml_client.online_deployments.begin_create_or_update(\n    deployment, local=True, vscode_debug=True\n).result()\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-debug-managed-online-endpoints-visual-studio-code-77.md"
                }
            }
        ]
    },
    {
        "query": "Code Samples to create a GPU compute type ComputeInstance using AzureML SDK V2?",
        "result": "Here is an example code snippet to create a GPU compute type ComputeInstance using AzureML SDK V2:\n\n```python\nfrom azure.ai.ml.entities import ComputeInstance, AmlCompute\nimport datetime\n\nci_gpu_name = \"gpu-ci\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\nci_gpu = ComputeInstance(name=ci_gpu_name, size=\"STANDARD_NC6\", virtual_machine_type=\"GPU\")\nml_client.begin_create_or_update(ci_gpu).result()\n```\n\nThis code creates a ComputeInstance with the name \"gpu-ci\" and the size \"STANDARD_NC6\", which is a GPU compute type. The `virtual_machine_type` parameter is set to \"GPU\" to indicate that this is a GPU compute instance.",
        "context": [
            {
                "page_content": "Or use the following examples to create a compute instance with more options:\n\n# [Python SDK](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\n# Compute Instances need to have a unique name across the region.\n# Here we create a unique name with current datetime\nfrom azure.ai.ml.entities import ComputeInstance, AmlCompute\nimport datetime\n\nci_basic_name = \"basic-ci\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\nci_basic = ComputeInstance(name=ci_basic_name, size=\"STANDARD_DS3_v2\")\nml_client.begin_create_or_update(ci_basic).result()\n```\n\nFor more information on the classes, methods, and parameters used in this example, see the following reference documents:\n\n* [`AmlCompute` class](/python/api/azure-ai-ml/azure.ai.ml.entities.amlcompute)\n* [`ComputeInstance` class](/python/api/azure-ai-ml/azure.ai.ml.entities.computeinstance)\n\n# [Azure CLI](#tab/azure-cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```azurecli\naz ml compute create -f create-instance.yml\n```\n\nWhere the file *create-instance.yml* is:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/computeInstance.schema.json \nname: basic-example-i\ntype: computeinstance\nsize: STANDARD_DS3_v2\n\n```\n\n\n# [Studio](#tab/azure-studio)\n\n1. Navigate to [Azure Machine Learning studio](https://ml.azure.com).\n1. Under __Manage__, select __Compute__.\n1. Select **Compute instance** at the top.\n1. If you have no compute instances, select  **Create** in the middle of the page.\n  \n    :::image type=\"content\" source=\"media/how-to-create-attach-studio/create-compute-target.png\" alt-text=\"Create compute target\":::\n\n1. If you see a list of compute resources, select **+New** above the list.\n\n    :::image type=\"content\" source=\"media/how-to-create-attach-studio/select-new.png\" alt-text=\"Select new\":::\n1. Fill out the form:\n\n    |Field  |Description  |\n    |---------|---------|\n    |Compute name     |  <ul><li>Name is required and must be between 3 to 24 characters long.</li><li>Valid characters are upper and lower case letters, digits, and the  **-** character.</li><li>Name must start with a letter</li><li>Name needs to be unique across all existing computes within an Azure region. You'll see an alert if the name you choose isn't unique</li><li>If **-**  character is used, then it needs to be followed by at least one letter later in the name</li></ul>     |\n    |Virtual machine type |  Choose CPU or GPU. This type can't be changed after creation     |\n    |Virtual machine size     |  Supported virtual machine sizes might be restricted in your region. Check the [availability list](https://azure.microsoft.com/global-infrastructure/services/?products=virtual-machines)     |\n\n1. Select **Create** unless you want to configure advanced settings for the compute instance.\n1. <a name=\"advanced-settings\"></a> Select **Next: Advanced Settings** if you want to:\n\n    * Enable idle shutdown (preview). Configure a compute instance to automatically shut down if it's inactive. For more information, see [enable idle shutdown](#enable-idle-shutdown-preview).\n    * Add schedule. Schedule times for the compute instance to automatically start and/or shut down. See [schedule details](#schedule-automatic-start-and-stop) below.\n    * Enable SSH access.  Follow the [detailed SSH access instructions](#enable-ssh-access) below.\n    * Enable virtual network. Specify the **Resource group**, **Virtual network**, and **Subnet** to create the compute instance inside an Azure Virtual Network (vnet). You can also select __No public IP__ to prevent the creation of a public IP address, which requires a private link workspace. You must also satisfy these [network requirements](./how-to-secure-training-vnet.md) for virtual network setup. \n    * Assign the computer to another user. For more about assigning to other users, see [Create on behalf of](#create-on-behalf-of-preview)\n    * Provision with a setup script (preview) - for more information about how to create and use a setup script, see [Customize the compute instance with a script](how-to-customize-compute-instance.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-58.md"
                }
            },
            {
                "page_content": "\n# Upgrade compute management to v2\n\nThe compute management functionally remains unchanged with the v2 development platform.\n\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\n\n\n## Create compute instance\n\n* SDK v1\n\n    ```python\n    import datetime\n    import time\n    \n    from azureml.core.compute import ComputeTarget, ComputeInstance\n    from azureml.core.compute_target import ComputeTargetException\n    \n    # Compute Instances need to have a unique name across the region.\n    # Here we create a unique name with current datetime\n    ci_basic_name = \"basic-ci\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n    \n    compute_config = ComputeInstance.provisioning_configuration(\n            vm_size='STANDARD_DS3_V2'\n        )\n        instance = ComputeInstance.create(ws, ci_basic_name , compute_config)\n        instance.wait_for_completion(show_output=True)\n    ```\n\n* SDK v2\n\n    ```python\n    # Compute Instances need to have a unique name across the region.\n    # Here we create a unique name with current datetime\n    from azure.ai.ml.entities import ComputeInstance, AmlCompute\n    import datetime\n    \n    ci_basic_name = \"basic-ci\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n    ci_basic = ComputeInstance(name=ci_basic_name, size=\"STANDARD_DS3_v2\", idle_time_before_shutdown_minutes=\"30\")\n    ml_client.begin_create_or_update(ci_basic)\n    ```\n\n## Create compute cluster\n\n* SDK v1\n\n    ```python\n    from azureml.core.compute import ComputeTarget, AmlCompute\n    from azureml.core.compute_target import ComputeTargetException\n    \n    # Choose a name for your CPU cluster\n    cpu_cluster_name = \"cpucluster\"\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS3_V2',\n                                                               max_nodes=4)\n    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n    cpu_cluster.wait_for_completion(show_output=True)\n    ```\n\n* SDK v2\n\n    ```python\n    from azure.ai.ml.entities import AmlCompute\n    cpu_cluster_name = \"cpucluster\"\n    cluster_basic = AmlCompute(\n        name=cpu_cluster_name,\n        type=\"amlcompute\",\n        size=\"STANDARD_DS3_v2\",\n        max_instances=4\n    )\n    ml_client.begin_create_or_update(cluster_basic)\n    ```\n\n## Mapping of key functionality in SDK v1 and SDK v2\n\n|Functionality in SDK v1|Rough mapping in SDK v2|\n|-|-|\n|[Method/API in SDK v1 (use links to ref docs)](/python/api/azureml-core/azureml.core.compute.amlcompute(class))|[Method/API in SDK v2 (use links to ref docs)](/python/api/azure-ai-ml/azure.ai.ml.entities.amlcompute)|\n\n## Next steps\n\n* [Create and manage an Azure Machine Learning compute instance](how-to-create-manage-compute-instance.md)\n* [Create an Azure Machine Learning compute cluster](how-to-create-attach-compute-cluster.md)",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-resource-compute-0.md"
                }
            },
            {
                "page_content": "\n\n### Create a schedule with SDK\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.constants import TimeZone\nfrom azure.ai.ml.entities import ComputeInstance, AmlCompute, ComputeSchedules, ComputeStartStopSchedule, RecurrencePattern, RecurrenceTrigger\nfrom azure.identity import DefaultAzureCredential\nfrom dateutil import tz\nimport datetime\n# Enter details of your AML workspace\nsubscription_id = \"<guid>\"\nresource_group = \"sample-rg\"\nworkspace = \"sample-ws\"\n# get a handle to the workspace\nml_client = MLClient(\n    DefaultAzureCredential(), subscription_id, resource_group, workspace\n)\nci_minimal_name = \"sampleCI\"\nmytz = tz.gettz(\"Asia/Kolkata\")\nnow = datetime.datetime.now(tz = mytz)\nstarttime = now + datetime.timedelta(minutes=25)\ntriggers = RecurrenceTrigger(frequency=\"day\", interval=1, schedule=RecurrencePattern(hours=17, minutes=30))\nmyschedule = ComputeStartStopSchedule(start_time=starttime, time_zone=TimeZone.INDIA_STANDARD_TIME, trigger=triggers, action=\"Stop\")\ncom_sch = ComputeSchedules(compute_start_stop=[myschedule])\nci_minimal = ComputeInstance(name=ci_minimal_name, schedules=com_sch)\nml_client.begin_create_or_update(ci_minimal)\n```\n\n### Create a schedule with a Resource Manager template\n\nYou can schedule the automatic start and stop of a compute instance by using a Resource Manager [template](https://github.com/Azure/azure-quickstart-templates/tree/master/quickstarts/microsoft.machinelearningservices/machine-learning-compute-create-computeinstance).\n\nIn the Resource Manager template, add:\n\n```\n\"schedules\": \"[parameters('schedules')]\"\n```\n\nThen use either cron or LogicApps expressions to define the schedule that starts or stops the instance in your parameter file:\n \n```json\n        \"schedules\": {\n        \"value\": {\n        \"computeStartStop\": [\n          {\n            \"triggerType\": \"Cron\",\n            \"cron\": {              \n              \"timeZone\": \"UTC\",\n              \"expression\": \"0 18 * * *\"\n            },\n            \"action\": \"Stop\",\n            \"status\": \"Enabled\"\n          },\n          {\n            \"triggerType\": \"Cron\",\n            \"cron\": {              \n              \"timeZone\": \"UTC\",\n              \"expression\": \"0 8 * * *\"\n            },\n            \"action\": \"Start\",\n            \"status\": \"Enabled\"\n          },\n          { \n            \"triggerType\":\u202f\"Recurrence\", \n            \"recurrence\":\u202f{ \n              \"frequency\":\u202f\"Day\", \n              \"interval\":\u202f1, \n              \"timeZone\":\u202f\"UTC\", \n            \u202f\u202f\"schedule\":\u202f{ \n                \"hours\":\u202f[17], \n            \u202f\u202f\u202f\u202f\"minutes\":\u202f[0]\n              } \n            }, \n            \"action\":\u202f\"Stop\", \n            \"status\":\u202f\"Enabled\" \n          } \n        ]\n      }\n    }\n```\n\n* Action can have value of \u201cStart\u201d or \u201cStop\u201d.\n* For trigger type of `Recurrence` use the same syntax as logic app, with this [recurrence schema](../logic-apps/logic-apps-workflow-actions-triggers.md#recurrence-trigger).\n* For trigger type of `cron`, use standard cron syntax:  \n\n    ```cron\n    // Crontab expression format: \n    // \n    // * * * * * \n    // - - - - - \n    // | | | | | \n    // | | | | +----- day of week (0 - 6) (Sunday=0) \n    // | | | +------- month (1 - 12) \n    // | | +--------- day of month (1 - 31) \n    // | +----------- hour (0 - 23) \n    // +------------- min (0 - 59) \n    // \n    // Star (*) in the value field above means all legal values as in \n    // braces for that column. The value column can have a * or a list \n    // of elements separated by commas. An element is either a number in \n    // the ranges shown above or two numbers in the range separated by a \n    // hyphen (meaning an inclusive range). \n    ```\n\n### Azure Policy support to default a schedule\nUse Azure Policy to enforce a shutdown schedule exists for every compute instance in a subscription or default to a schedule if nothing exists.\nFollowing is a sample policy to default a shutdown schedule at 10 PM PST.\n```json\n{\n    \"mode\": \"All\",\n    \"policyRule\": {\n     \"if\": {\n      \"allOf\": [\n       {\n        \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/computeType\",\n        \"equals\": \"ComputeInstance\"\n       },\n       {\n        \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/schedules\",\n        \"exists\": \"false\"\n       }\n      ]\n     },\n     \"then\": {\n      \"effect\": \"append\",\n      \"details\": [\n       {\n        \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/schedules\",\n        \"value\": {\n         \"computeStartStop\": [\n          {\n           \"triggerType\": \"Cron\",\n           \"cron\": {\n            \"startTime\": \"2021-03-10T21:21:07\",\n            \"timeZone\": \"Pacific Standard Time\",\n            \"expression\": \"0 22 * * *\"\n           },\n           \"action\": \"Stop\",\n           \"status\": \"Enabled\"\n          }\n         ]\n        }\n       }\n      ]\n     }\n    }\n}    \n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-382.md"
                }
            }
        ]
    },
    {
        "query": "Generate Python Code Samples to create environments from an existing base container in MCR using AzureML SDK V2?",
        "result": "To create an environment from an existing base container in MCR using AzureML SDK V2, you can use the `Environment.from_existing_conda` method. This method creates an environment from an existing conda environment file. You can specify the name of the environment, the path to the conda environment file, and the base image to use. Here's an example:\n\n```python\nfrom azureml.core import Environment\n\nenv = Environment.from_existing_conda(\n    name=\"myenv\",\n    conda_environment_file_path=\"./myenv.yml\",\n    base_image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\"\n)\n```",
        "context": [
            {
                "page_content": "To create a new environment, you must use one of the following approaches (see [DockerSection](https://aka.ms/azureml/environment/environment-docker-section)):\n* Base image\n    * Provide base image name, repository from which to pull it, and credentials if needed\n    * Provide a conda specification\n* Base Dockerfile \n    * Provide a Dockerfile\n    * Provide a conda specification\n* Docker build context\n    * Provide the location of the build context (URL)\n    * The build context must contain at least a Dockerfile, but may contain other files as well\n\n*Applies to: Azure CLI & Python SDK v2*\n\nTo create a new environment, you must use one of the following approaches:\n* Docker image\n    * Provide the image URI of the image hosted in a registry such as Docker Hub or Azure Container Registry\n    * [Sample here](https://aka.ms/azureml/environment/create-env-docker-image-v2)\n* Docker build context\n    * Specify the directory that will serve as the build context\n    * The directory should contain a Dockerfile and any other files needed to build the image\n    * [Sample here](https://aka.ms/azureml/environment/create-env-build-context-v2)\n* Conda specification \n    * You must specify a base Docker image for the environment; the conda environment will be built on top of the Docker image provided\n    * Provide the relative path to the conda file\n    * [Sample here](https://aka.ms/azureml/environment/create-env-conda-spec-v2)\n\n### Missing Docker definition\n*Applies to: Python SDK v1*\n<!--issueDescription-->\nThis issue can happen when your environment definition is missing a `DockerSection.` This section configures settings related to the final Docker image built from your environment specification.\n \n**Potential causes:**\n* The `DockerSection` of your environment definition isn't defined (null)\n \n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n \n**Troubleshooting steps**\n\n Add a `DockerSection` to your environment definition, specifying either a base image, base dockerfile, or docker build context.\n\n```python\nfrom azureml.core import Environment\nmyenv = Environment(name=\"myenv\")\n# Specify docker steps as a string.\ndockerfile = r'''\nFROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\nRUN echo \"Hello from custom container!\"\n'''\n\nmyenv.docker.base_dockerfile = dockerfile\n```\n \n**Resources**\n* [DockerSection](https://aka.ms/azureml/environment/environment-docker-section)\n\n### Too many Docker options\n<!--issueDescription-->\n \n**Potential causes:**\n\n*Applies to: Python SDK v1*\n\nYou have more than one of these Docker options specified in your environment definition\n* `base_image`\n* `base_dockerfile`\n* `build_context`\n* See [DockerSection](https://aka.ms/azureml/environment/docker-section-class)\n\n*Applies to: Azure CLI & Python SDK v2*\n\nYou have more than one of these Docker options specified in your environment definition\n* `image`\n* `build`\n* See [azure.ai.ml.entities.Environment](https://aka.ms/azureml/environment/environment-class-v2)\n \n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n \n**Troubleshooting steps**\n\nChoose which Docker option you'd like to use to build your environment. Then set all other specified options to None.\n\n*Applies to: Python SDK v1*\n\n```python\nfrom azureml.core import Environment\nmyenv = Environment(name=\"myEnv\")\ndockerfile = r'''\nFROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\nRUN echo \"Hello from custom container!\"\n'''\nmyenv.docker.base_dockerfile = dockerfile\nmyenv.docker.base_image = \"pytorch/pytorch:latest\"\n\n# Having both base dockerfile and base image set will cause failure. Delete the one you won't use.\nmyenv.docker.base_image = None\n```\n\n### Missing Docker option\n<!--issueDescription-->\n \n**Potential causes:**\n\n*Applies to: Python SDK v1*\n\nYou didn't specify one of the following options in your environment definition\n* `base_image`\n* `base_dockerfile`\n* `build_context`\n* See [DockerSection](https://aka.ms/azureml/environment/docker-section-class)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-128.md"
                }
            },
            {
                "page_content": "\n\n# Deploy a model for use with Cognitive Search\n\n[!INCLUDE [sdk v1](../../includes/machine-learning-sdk-v1.md)]\n\nThis article teaches you how to use Azure Machine Learning to deploy a model for use with [Azure Cognitive Search](../search/search-what-is-azure-search.md).\n\nCognitive Search performs content processing over heterogenous content, to make it queryable by humans or applications. This process can be enhanced by using a model deployed from Azure Machine Learning.\n\nAzure Machine Learning can deploy a trained model as a web service. The web service is then embedded in a Cognitive Search _skill_, which becomes part of the processing pipeline.\n\n> [!IMPORTANT]\n> The information in this article is specific to the deployment of the model. It provides information on the supported deployment configurations that allow the model to be used by Cognitive Search.\n>\n> For information on how to configure Cognitive Search to use the deployed model, see the [Build and deploy a custom skill with Azure Machine Learning](../search/cognitive-search-tutorial-aml-custom-skill.md) tutorial.\n>\n> For the sample that the tutorial is based on, see [https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill](https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill).\n\nWhen deploying a model for use with Azure Cognitive Search, the deployment must meet the following requirements:\n\n* Use Azure Kubernetes Service to host the model for inference.\n* Enable transport layer security (TLS) for the Azure Kubernetes Service. TLS is used to secure HTTPS communications between Cognitive Search and the deployed model.\n* The entry script must use the `inference_schema` package to generate an OpenAPI (Swagger) schema for the service.\n* The entry script must also accept JSON data as input, and generate JSON as output.\n\n\n## Prerequisites\n\n* An Azure Machine Learning workspace. For more information, see [Create workspace resources](quickstart-create-resources.md).\n\n* A Python development environment with the Azure Machine Learning SDK installed. For more information, see [Azure Machine Learning SDK](/python/api/overview/azure/ml/install).  \n\n* A registered model. If you do not have a model, use the example notebook at [https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill](https://github.com/Azure-Samples/azure-search-python-samples/tree/master/AzureML-Custom-Skill).\n\n* A general understanding of [How and where to deploy models](v1/how-to-deploy-and-where.md).\n\n## Connect to your workspace\n\nAn Azure Machine Learning workspace provides a centralized place to work with all the artifacts you create when you use Azure Machine Learning. The workspace keeps a history of all training jobs, including logs, metrics, output, and a snapshot of your scripts.\n\nTo connect to an existing workspace, use the following code:\n\n> [!IMPORTANT]\n> This code snippet expects the workspace configuration to be saved in the current directory or its parent. For more information, see [Create and manage Azure Machine Learning workspaces](how-to-manage-workspace.md). For more information on saving the configuration to file, see [Create a workspace configuration file](v1/how-to-configure-environment-v1.md).\n\n```python\nfrom azureml.core import Workspace\n\ntry:\n    # Load the workspace configuration from local cached inffo\n    ws = Workspace.from_config()\n    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n    print('Library configuration succeeded')\nexcept:\n    print('Workspace not found')\n```\n\n## Create a Kubernetes cluster\n\n**Time estimate**: Approximately 20 minutes.\n\nA Kubernetes cluster is a set of virtual machine instances (called nodes) that are used for running containerized applications.\n\nWhen you deploy a model from Azure Machine Learning to Azure Kubernetes Service, the model and all the assets needed to host it as a web service are packaged into a Docker container. This container is then deployed onto the cluster.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-model-cognitive-search-0.md"
                }
            },
            {
                "page_content": "1. Let's create an environment where the scoring script can be executed. Since our model is MLflow, the conda requirements are also specified in the model package (for more details about MLflow models and the files included on it see The MLmodel format). We are going then to build the environment using the conda dependencies from the file. However, we need also to include the package `azureml-inference-server-http` which is required for Online Deployments in Azure Machine Learning.\n    \n    The conda definition file looks as follows:\n\n    __conda.yml__\n\n    ```yaml\n    channels:\n    - conda-forge\n    dependencies:\n    - python=3.7.11\n    - pip\n    - pip:\n      - mlflow\n      - scikit-learn==0.24.1\n      - cloudpickle==2.0.0\n      - psutil==5.8.0\n      - pandas==1.3.5\n      - azureml-inference-server-http\n    name: mlflow-env\n    ```\n\n    > [!NOTE]\n    > Note how the package `azureml-inference-server-http` has been added to the original conda dependencies file. \n\n    We will use this conda dependencies file to create the environment:\n\n    # [Azure CLI](#tab/cli)\n    \n    *The environment will be created inline in the deployment configuration.*\n    \n    # [Python (Azure ML SDK)](#tab/sdk)\n    \n    ```pythonS\n    environment = Environment(\n        conda_file=\"sklearn-diabetes/environment/conda.yml\",\n        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n    )\n    ```\n\n    # [Python (MLflow SDK)](#tab/mlflow)\n\n    *This operation is not supported in MLflow SDK*\n\n    # [Studio](#tab/studio)\n    \n    On [Azure ML studio portal](https://ml.azure.com), follow these steps:\n    \n    1. Navigate to the __Environments__ tab on the side menu.\n    1. Select the tab __Custom environments__ > __Create__.\n    1. Enter the name of the environment, in this case `sklearn-mlflow-online-py37`.\n    1. On __Select environment type__ select __Use existing docker image with conda__.\n    1. On __Container registry image path__, enter `mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04`.\n    1. On __Customize__ section copy the content of the file `sklearn-diabetes/environment/conda.yml` we introduced before. \n    1. Click on __Next__ and then on __Create__.\n    1. The environment is ready to be used.   \n\n\n1. Let's create the deployment now:\n\n    # [Azure CLI](#tab/cli)\n    \n    Create a deployment configuration file:\n    \n    ```yaml\n    $schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n    name: sklearn-diabetes-custom\n    endpoint_name: my-endpoint\n    model: azureml:sklearn-diabetes@latest\n    environment: \n      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n      conda_file: mlflow/sklearn-diabetes/environment/conda.yml\n    code_configuration:\n      source: mlflow/sklearn-diabetes/src\n      scoring_script: score.py\n    instance_type: Standard_F2s_v2\n    instance_count: 1\n    ```\n    \n    Create the deployment:\n    \n    ```azurecli\n    az ml online-deployment create -f deployment.yml\n    ```\n    \n    # [Python (Azure ML SDK)](#tab/sdk)\n    \n    ```python\n    blue_deployment = ManagedOnlineDeployment(\n        name=\"blue\",\n        endpoint_name=endpoint_name,\n        model=model,\n        environment=environment,\n        code_configuration=CodeConfiguration(\n            code=\"sklearn-diabetes/src\",\n            scoring_script=\"score.py\"\n        ),\n        instance_type=\"Standard_F4s_v2\",\n        instance_count=1,\n    )\n    ```\n\n    # [Python (MLflow SDK)](#tab/mlflow)\n\n    *This operation is not supported in MLflow SDK*\n\n    # [Studio](#tab/studio)\n    \n    > [!IMPORTANT]\n    > You can't create custom MLflow deployments in Online Endpoints using the Azure Machine Learning portal. Switch to [Azure ML CLI](?tabs=azure-cli) or the [Azure ML SDK for Python](?tabs=python).\n\n\n1. Once your deployment completes, your deployment is ready to serve request. One of the easier ways to test the deployment is by using a sample request file along with the `invoke` method.\n\n    **sample-request-sklearn.json**\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-mlflow-models-online-endpoints-841.md"
                }
            }
        ]
    },
    {
        "query": "Generate a Python code to show How to setup Environments from docker file using AzureML SDK V2?",
        "result": "To set up an environment from a Dockerfile using AzureML SDK V2, you can use the `Environment.from_dockerfile()` method. This method creates an environment from a Dockerfile in a specified directory. Here's an example code snippet:\n\n```python\nfrom azureml.core import Environment\n\n# Create an environment from a Dockerfile\nenv = Environment.from_dockerfile(name=\"myenv\", dockerfile_dir=\"./docker\")\n\n# Register the environment\nenv.register(workspace=ws)\n```\n\nThis code creates an environment named \"myenv\" from a Dockerfile in the \"./docker\" directory and registers it in the specified workspace.",
        "context": [
            {
                "page_content": "Once you have the scikit-Learn pipeline, all that is left to call is the `fit()` method to train the model:\n\n```python\ndef train_model(X, y, sample_weights):\n    \n    logger.info(\"Running train_model\")\n    model_pipeline = build_model_pipeline()\n    \n    model = model_pipeline.fit(X, y)\n    return model\n```\n\nThe return value from `train_model()` is the model fitted/trained on the input data.\n\nThe main code that runs all the previous functions is the following:\n\n```python\ndef main(training_dataset_id=None):\n    from azureml.core.run import Run\n    \n    # The following code is for when running this code as part of an AzureML script run.\n    run = Run.get_context()\n    setup_instrumentation(run)\n    \n    df = get_training_dataset(training_dataset_id)\n    X, y, sample_weights = prepare_data(df)\n    split_ratio = 0.1\n    try:\n        (X_train, y_train, sample_weights_train), (X_valid, y_valid, sample_weights_valid) = split_dataset(X, y, sample_weights, split_ratio, should_stratify=True)\n    except Exception:\n        (X_train, y_train, sample_weights_train), (X_valid, y_valid, sample_weights_valid) = split_dataset(X, y, sample_weights, split_ratio, should_stratify=False)\n\n    model = train_model(X_train, y_train, sample_weights_train)\n    \n    metrics = calculate_metrics(model, X, y, sample_weights, X_test=X_valid, y_test=y_valid)\n    \n    print(metrics)\n    for metric in metrics:\n        run.log(metric, metrics[metric])\n```\n\nOnce you have the trained model, you can use it for making predictions with the predict() method. If your experiment is for a time series model, use the forecast() method for predictions. \n\n```python\ny_pred = model.predict(X)\n```\n\nFinally, the model is serialized and saved as a `.pkl` file named \"model.pkl\":\n\n```python\n    with open('model.pkl', 'wb') as f:\n        pickle.dump(model, f)\n    run.upload_file('outputs/model.pkl', 'model.pkl')\n```\n\n## script_run_notebook.ipynb\n\nThe `script_run_notebook.ipynb` notebook serves as an easy way to execute `script.py` on an Azure ML compute.\nThis notebook is similar to the existing automated ML sample notebooks however, there are a couple of key differences as explained in the following sections.\n\n### Environment\n\nTypically, the training environment for an automated ML run is automatically set by the SDK. However, when running a custom script run like the generated code, automated ML is no longer driving the process, so the environment must be specified for the command job to succeed.\n\nCode generation reuses the environment that was used in the original automated ML experiment, if possible. Doing so guarantees that the training script run doesn't fail due to missing dependencies, and has a side benefit of not needing a Docker image rebuild, which saves time and compute resources.\n\nIf you make changes to `script.py` that require additional dependencies, or you would like to use your own environment, you need to update the environment in the `script_run_notebook.ipynb` accordingly.\n\n\n### Submit the experiment\n\nSince the generated code isn\u2019t driven by automated ML anymore, instead of creating and submitting an AutoML Job, you need to create a `Command Job` and provide the generated code (script.py) to it. \n\nThe following example contains the parameters and regular dependencies needed to run a Command Job, such as compute, environment, etc.\n```python\nfrom azure.ai.ml import command, Input\n\n# To test with new training / validation datasets, replace the default dataset id(s) taken from parent run below\ntraining_dataset_id = '<DATASET_ID>'\n\ndataset_arguments = {'training_dataset_id': training_dataset_id}\ncommand_str = 'python script.py --training_dataset_id ${{inputs.training_dataset_id}}'\n\ncommand_job = command(\n    code=project_folder,\n    command=command_str,\n    environment='AutoML-Non-Prod-DNN:25',\n    inputs=dataset_arguments,\n    compute='automl-e2e-cl2',\n    experiment_name='build_70775722_9249eda8'\n)\n \nreturned_job = ml_client.create_or_update(command_job)\nprint(returned_job.studio_url) # link to naviagate to submitted run in AzureML Studio\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-generate-automl-training-code-284.md"
                }
            },
            {
                "page_content": "\n# Manage Azure Machine Learning environments with the CLI & SDK (v2)\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\n> [!div class=\"op_single_selector\" title1=\"Select the version of Azure Machine Learning SDK or CLI extension you are using:\"]\n> * [v1](./v1/how-to-use-environments.md)\n> * [v2 (current version)](how-to-manage-environments-v2.md)\n\n\n\nAzure Machine Learning environments define the execution environments for your jobs or deployments and encapsulate the dependencies for your code. Azure ML uses the environment specification to create the Docker container that your training or scoring code runs in on the specified compute target. You can define an environment from a conda specification, Docker image, or Docker build context.\n\nIn this article, learn how to create and manage Azure ML environments using the SDK & CLI (v2).\n\n\n## Prerequisites\n\n[!INCLUDE [sdk/cliv2](../../includes/machine-learning-cli-sdk-v2-prereqs.md)]\n\n> [!TIP]\n> For a full-featured development environment, use Visual Studio Code and the [Azure Machine Learning extension](how-to-setup-vs-code.md) to [manage Azure Machine Learning resources](how-to-manage-resources-vscode.md) and [train machine learning models](tutorial-train-deploy-image-classification-model-vscode.md).\n\n### Clone examples repository\n\nTo run the training examples, first clone the examples repository. For the CLI examples, change into the `cli` directory. For the SDK examples, change into the `SDK` directory:\n\n```azurecli\ngit clone --depth 1 https://github.com/Azure/azureml-examples\n```\n\nNote that `--depth 1` clones only the latest commit to the repository, which reduces time to complete the operation.\n\n### Connect to the workspace\n\n> [!TIP]\n> Use the tabs below to select the method you want to use to work with environments. Selecting a tab will automatically switch all the tabs in this article to the same tab. You can select another tab at any time.\n\n# [Azure CLI](#tab/cli)\n\nWhen using the Azure CLI, you need identifier parameters - a subscription, resource group, and workspace name. While you can specify these parameters for each command, you can also set defaults that will be used for all the commands. Use the following commands to set default values. Replace `<subscription ID>`, `<AzureML workspace name>`, and `<resource group>` with the values for your configuration:\n\n```azurecli\naz account set --subscription <subscription ID>\naz configure --defaults workspace=<AzureML workspace name> group=<resource group>\n```\n\n# [Python SDK](#tab/python)\n\nTo connect to the workspace, you need identifier parameters - a subscription, resource group, and workspace name. You'll use these details in the `MLClient` from the `azure.ai.ml` namespace to get a handle to the required Azure Machine Learning workspace. To authenticate, you use the [default Azure authentication](/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python&preserve-view=true). Check this [example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n\n```python\n#import required libraries for workspace\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\n#import required libraries for environments examples\nfrom azure.ai.ml.entities import Environment, BuildContext\n\n#Enter details of your AzureML workspace\nsubscription_id = '<SUBSCRIPTION_ID>'\nresource_group = '<RESOURCE_GROUP>'\nworkspace = '<AZUREML_WORKSPACE_NAME>'\n\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)\n```\n\n\n## Curated environments\n\nThere are two types of environments in Azure ML: curated and custom environments. Curated environments are predefined environments containing popular ML frameworks and tooling. Custom environments are user-defined and can be created via `az ml environment create`.\n\nCurated environments are provided by Azure ML and are available in your workspace by default. Azure ML routinely updates these environments with the latest framework version releases and maintains them for bug fixes and security patches. They're backed by cached Docker images, which reduce job preparation cost and model deployment time.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-environments-v2-0.md"
                }
            },
            {
                "page_content": "In v2, \"experiments\", \"runs\", and \"pipelines\" are consolidated into jobs. A job has a type. Most jobs are `command` jobs that run a command, like `python main.py`. What runs in a job is agnostic to any programming language, so you can run `bash` scripts, invoke `python` interpreters, run a bunch of `curl` commands, or anything else. Another common type of job is `pipeline`, which defines child jobs that may have input/output relationships, forming a directed acyclic graph (DAG).\n\nFor a comparison of SDK v1 and v2 code, see \n* [Run a script](migrate-to-v2-command-job.md)\n* [Local runs](migrate-to-v2-local-runs.md)\n* [Hyperparameter tuning](migrate-to-v2-execution-hyperdrive.md)\n* [Parallel Run](migrate-to-v2-execution-parallel-run-step.md)\n* [Pipelines](migrate-to-v2-execution-pipeline.md)\n* [AutoML](migrate-to-v2-execution-automl.md)\n\n### Designer\n\nYou can use designer to build pipelines using your own v2 custom components and the new prebuilt components from registry. In this situation, you can use v1 or v2 data assets in your pipeline. \n\nYou can continue to use designer to build pipelines using classic prebuilt components and v1 dataset types (tabular, file). You cannot use existing designer classic prebuilt components with v2 data asset.\n\nYou cannot build a pipeline using both existing designer classic prebuilt components and v2 custom components.\n\n### Data (datasets in v1)\n\nDatasets are renamed to data assets. *Backwards compatibility* is provided, which means you can use V1 Datasets in V2. When you consume a V1 Dataset in a V2 job you will notice they are automatically mapped into V2 types as follows:\n\n* V1 FileDataset = V2 Folder (`uri_folder`)\n* V1 TabularDataset = V2 Table (`mltable`)\n\nIt should be noted that *forwards compatibility* is **not** provided, which means you **cannot** use V2 data assets in V1.\n\nThis article talks more about handling data in v2 - [Read and write data in a job](how-to-read-write-data-v2.md)\n\nFor a comparison of SDK v1 and v2 code, see [Data assets in SDK v1 and v2](migrate-to-v2-assets-data.md).\n\n### Model\n\nModels created from v1 can be used in v2. \n\nFor a comparison of SDK v1 and v2 code, see [Model management in SDK v1 and SDK v2](migrate-to-v2-assets-model.md)\n\n### Environment\n\nEnvironments created from v1 can be used in v2. In v2, environments have new features like creation from a local Docker context.\n\n## Managing secrets\n\nThe management of Key Vault secrets differs significantly in V2 compared to V1. The V1 set_secret and get_secret SDK methods are not available in V2. Instead, direct access using Key Vault client libraries should be used.\n\nFor details about Key Vault, see [Use authentication credential secrets in Azure Machine Learning training jobs](how-to-use-secrets-in-runs.md).\n\n## Scenarios across the machine learning lifecycle\n\nThere are a few scenarios that are common across the machine learning lifecycle using Azure ML. We'll look at a few and give general recommendations for upgrading to v2.\n\n### Azure setup\n\nAzure recommends Azure Resource Manager templates (often via Bicep for ease of use) to create resources. The same is a good approach for creating Azure ML resources as well.\n\nIf your team is only using Azure ML, you may consider provisioning the workspace and any other resources via YAML  files and CLI instead.\n\n### Prototyping models\n\nWe recommend v2 for prototyping models. You may consider using the CLI for an interactive use of Azure ML, while your model training code is Python or any other programming language. Alternatively, you may adopt a full-stack approach with Python solely using the Azure ML SDK or a mixed approach with the Azure ML Python SDK and YAML files.\n\n### Production model training\n\nWe recommend v2 for production model training. Jobs consolidate the terminology and provide a set of consistency that allows for easier transition between types (for example, `command` to `sweep`) and a GitOps-friendly process for serializing jobs into YAML files.\n\nWith v2, you should separate your machine learning code from the control plane code. This separation allows for easier iteration and allows for easier transition between local and cloud. We also recommend using MLflow for tracking and model logging. See the [MLflow concept article](concept-mlflow.md) for details.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-migrate-from-v1-111.md"
                }
            }
        ]
    },
    {
        "query": "Generate a Python Code samples that shows how to train a demand forcast using Azuomated and AzureML SDK V2?",
        "result": "Here is a sample code that shows how to train a demand forecast using Automated and AzureML SDK V2:\n\n```python\nfrom azureml.core import Workspace, Dataset\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nws = Workspace.from_config()\n\n# Define the compute target\ncompute_name = \"cpu-cluster\"\ncompute_target = ws.compute_targets[compute_name]\n\n# Define the training data\ndataset_name = \"demand_data\"\ndataset = Dataset.get_by_name(ws, dataset_name)\n\n# Define the AutoML configuration\nautoml_config = AutoMLConfig(\n    task=\"forecasting\",\n    primary_metric=\"normalized_root_mean_squared_error\",\n    experiment_timeout_minutes=15,\n    training_data=dataset,\n    label_column_name=\"demand\",\n    n_cross_validations=3,\n    enable_early_stopping=True,\n    enable_voting_ensemble=False,\n    enable_stack_ensemble=False,\n    compute_target=compute_target,\n    enable_dnn_training=True,\n    model_explainability=True,\n    time_column_name=\"timeStamp\",\n    country_or_region_for_holidays=\"US\",\n    target_lags=\"auto\",\n    max_horizon=14,\n    featurization=\"auto\",\n    debug_log=\"automl.log\",\n)\n\n# Define the run configuration\nrun_config = RunConfiguration()\nrun_config.environment.python.user_managed_dependencies = False\nrun_config.environment.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=[\"numpy\", \"pandas\", \"scikit-learn\", \"py-xgboost<=0.80\"],\n    pip_packages=[\"azureml-sdk\", \"azureml-train-automl\", \"azureml-dataprep[fuse,pandas]\", \"fbprophet\"],\n    pin_sdk_version=False,\n)\n\n# Submit the experiment\nfrom azureml.core.experiment import Experiment\n\nexperiment_name = \"demand-forecast\"\nexperiment = Experiment(ws, experiment_name)\n\nrun = experiment.submit(automl_config, run_config=run_config)\n```\n\nThis code defines the workspace, compute target, training data, AutoML configuration, and run configuration. It then submits the experiment to Azure Machine Learning for training. Note that this code uses the `forecasting` task type, sets the `time_column_name` to `timeStamp`, and sets the `country_or_region_for_holidays` to `US` to generate holiday features.",
        "context": [
            {
                "page_content": "|`wday_lbl`|String feature representing name of the day of the week. |\n|`qday`|Numeric feature representing the day within the quarter. It takes values 1 through 92.|1|\n|`yday`|Numeric feature representing the day of the year. It takes values 1 through 365, or 1 through 366 in the case of leap year.|1|\n|`week`|Numeric feature representing [ISO week](https://en.wikipedia.org/wiki/ISO_week_date) as defined in ISO 8601. ISO weeks always start on Monday and end on Sunday. It takes values 1 through 52, or 53 for years having 1st January falling on Thursday or for leap years having 1st January falling on Wednesday.|52|\n\nThe full set of standard calendar features may not be created in all cases. The generated set depends on the frequency of the time series and whether the training data contains datetime features in addition to the time index. The following table shows the features created for different column types:\n\nColumn purpose | Calendar features\n--- | ---\nTime index | The full set minus calendar features that have high correlation with other features. For example, if the time series frequency is daily, then any features with a more granular frequency than daily will be removed since they don't provide useful information.\nOther datetime column | A reduced set consisting of  `Year`,  `Month`,  `Day`,  `DayOfWeek`,  `DayOfYear`,  `QuarterOfYear`,  `WeekOfMonth`,  `Hour`,  `Minute`, and `Second`. If the column is a date with no time, `Hour`,  `Minute`, and `Second` will be 0.\n\n## Holiday features\n\nAutoML can optionally create features representing holidays from a specific country or region. These features are configured in AutoML using the `country_or_region_for_holidays` parameter which accepts an [ISO country code](https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes).\n\n> [!NOTE]\n> Holiday features can only be made for time series with daily frequency.\n\nThe following table summarizes the holiday features:\n\nFeature name | Description\n --- | ----------- |\n`Holiday`| String feature that specifies whether a date is a regional or national holiday. Days within some range of a holiday are also marked. \n`isPaidTimeOff`| Binary feature that takes value 1 if the day is a \"paid time-off holiday\" in the given country or region.\n\nAutoML uses Azure Open Datasets as a source for holiday information. For more information, see the [PublicHolidays](/python/api/azureml-opendatasets/azureml.opendatasets.publicholidays) documentation.\n\nTo better understand the holiday feature generation, consider the following example data: \n\n<img src='./media/concept-automl-forecasting-calendar-features/load_forecasting_sample_data_daily.png' alt='sample_data' width=50%></img>\n\nTo make American holiday features for this data, we set the `country_or_region_for_holiday` to 'US' in the [forecast settings](/python/api/azure-ai-ml/azure.ai.ml.automl.forecastingjob#azure-ai-ml-automl-forecastingjob-set-forecast-settings) as shown in the following code sample:\n```python\nfrom azure.ai.ml import automl\n\n# create a forcasting job\nforecasting_job = automl.forecasting(\n    compute='test_cluster',   # Name of single or multinode AML compute infrastructure created by user\n    experiment_name=exp_name, # name of experiment \n    training_data=sample_data,  \n    target_column_name='demand',\n    primary_metric='NormalizedRootMeanSquaredError',\n    n_cross_validations=3,\n    enable_model_explainability=True\n)\n\n# set custom forecast settings\nforecasting_job.set_forecast_settings(\n    time_column_name='timeStamp',\n    country_or_region_for_holidays='US'\n)\n```\nThe generated holiday features look like the following:\n\n<a name='output'><img src='./media/concept-automl-forecasting-calendar-features/sample_dataset_holiday_feature_generated.png' alt='sample_data_output' width=75%></img></a>\n\nNote that generated features have the prefix `_automl_` prepended to their column names. AutoML generally uses this prefix to distinguish input features from engineered features.\n\n## Next steps\n* Learn more about [how to set up AutoML to train a time-series forecasting model](./how-to-auto-train-forecast.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-automl-forecasting-calendar-features-31.md"
                }
            },
            {
                "page_content": " \n## Forecasting with a trained model\n\nOnce you've used AutoML to train and select a best model, the next step is to evaluate the model. If it meets your requirements, you can use it to generate forecasts into the future. This section shows how to write Python scripts for evaluation and prediction. For an example of deploying a trained model with an inference script, see our [example notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/automl-standalone-jobs/automl-forecasting-github-dau/auto-ml-forecasting-github-dau.ipynb).\n  \n### Evaluating model accuracy with a rolling forecast\n\nBefore you put a model into production, you should evaluate its accuracy on a test set held out from the training data. A best practice procedure is a rolling evaluation that rolls the trained forecaster forward in time over the test set, averaging error metrics over several prediction windows. Ideally, the test set for the evaluation is long relative to the model's forecast horizon. Estimates of forecasting error may otherwise be statistically noisy and, therefore, less reliable.\n\nFor example, suppose you train a model on daily sales to predict demand up to two weeks (14 days) into the future. If there's sufficient historic data available, you might reserve the final several months to even a year of the data for the test set. The rolling evaluation begins by generating a 14-day-ahead forecast for the first two weeks of the test set. Then, the forecaster is advanced by some number of days into the test set and you generate another 14-day-ahead forecast from the new position. The process continues until you get to the end of the test set.\n\nTo do a rolling evaluation, you call the `rolling_forecast` method of the `fitted_model`, then compute desired metrics on the result. A rolling evaluation inference script is shown in the following code sample:\n\n```python\n\"\"\"\nThis is the script that is executed on the compute instance. It relies\non the model.pkl file which is uploaded along with this script to the\ncompute instance.\n\"\"\"\n\nimport os\nimport pandas as pd\n\nfrom sklearn.externals import joblib\n\n\ndef init():\n    global target_column_name\n    global fitted_model\n\n    target_column_name = os.environ[\"TARGET_COLUMN_NAME\"]\n    # AZUREML_MODEL_DIR is an environment variable created during deployment\n    # It is the path to the model folder (./azureml-models)\n    # Please provide your model's folder name if there's one\n    model_path = os.path.join(os.environ[\"AZUREML_MODEL_DIR\"], \"model.pkl\")\n    try:\n        fitted_model = joblib.load(model_path)\n    except Exception:\n        print(\"Loading pickle failed. Trying torch.load()\")\n\n        import torch\n        model_path = os.path.join(os.environ[\"AZUREML_MODEL_DIR\"], \"model.pt\") \n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        fitted_model = torch.load(model_path, map_location=device)\n\n\ndef run(mini_batch):\n    print(f\"run method start: {__file__}, run({mini_batch})\")\n    resultList = []\n    for test in mini_batch:\n        if not test.endswith(\".csv\"):\n            continue\n        X_test = pd.read_csv(test, parse_dates=[fitted_model.time_column_name])\n        y_test = X_test.pop(target_column_name).values\n\n        # Make a rolling forecast, advancing the forecast origin by 1 period on each iteration through the test set\n        X_rf = fitted_model.rolling_forecast(\n            X_test, y_test, step=1, ignore_data_errors=True\n        )\n\n        resultList.append(X_rf)\n\n    return pd.concat(resultList, sort=False, ignore_index=True)\n```\n\nIn this sample, the step size for the rolling forecast is set to one which means that the forecaster is advanced one period, or one day in our demand prediction example, at each iteration. The total number of forecasts returned by `rolling_forecast` depends on the length of the test set and this step size. For more details and examples, see the [rolling_forecast() documentation](/python/api/azureml-training-tabular/azureml.training.tabular.models.forecasting_pipeline_wrapper_base.forecastingpipelinewrapperbase#azureml-training-tabular-models-forecasting-pipeline-wrapper-base-forecastingpipelinewrapperbase-rolling-forecast) and the [Forecasting away from training data notebook](https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/forecasting-forecast-function/auto-ml-forecasting-function.ipynb). \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-forecast-333.md"
                }
            },
            {
                "page_content": "Optional configurations are available for forecasting tasks, such as enabling deep learning and specifying a target rolling window aggregation. A complete list of parameters is available in the [forecast_settings API doc](/python/api/azure-ai-ml/azure.ai.ml.automl.forecastingjob#azure-ai-ml-automl-forecastingjob-set-forecast-settings).\n\n#### Model search settings\n\nThere are two optional settings that control the model space where AutoML searches for the best model, `allowed_training_algorithms` and `blocked_training_algorithms`. To restrict the search space to a given set of model classes, use allowed_training_algorithms as in the following sample:\n\n```python\n# Only search ExponentialSmoothing and ElasticNet models\nforecasting_job.set_training(\n    allowed_training_algorithms=[\"ExponentialSmoothing\", \"ElasticNet\"]\n)\n```\n\nIn this case, the forecasting job _only_ searches over Exponential Smoothing and Elastic Net model classes. To remove a given set of model classes from the search space, use the blocked_training_algorithms as in the following sample:\n\n```python\n# Search over all model classes except Prophet\nforecasting_job.set_training(\n    blocked_training_algorithms=[\"Prophet\"]\n)\n```\n\nNow, the job searches over all model classes _except_ Prophet. For a list of forecasting model names that are accepted in `allowed_training_algorithms` and `blocked_training_algorithms`, see [supported forecasting models](/python/api/azureml-train-automl-client/azureml.train.automl.constants.supportedmodels.forecasting) and [supported regression models](/python/api/azureml-train-automl-client/azureml.train.automl.constants.supportedmodels.regression).  \n\n#### Enable deep learning\n\nAutoML ships with a custom deep neural network (DNN) model called `ForecastTCN`. This model is a [temporal convolutional network](https://arxiv.org/abs/1803.01271), or TCN, that applies common imaging task methods to time series modeling. Namely, one-dimensional \"causal\" convolutions form the backbone of the network and enable the model to learn complex patterns over long durations in the training history.  \n\n:::image type=\"content\" source=\"media/how-to-auto-train-forecast/tcn-basic.png\" alt-text=\"Diagram showing major components of AutoML's ForecastTCN.\":::\n\nThe ForecastTCN often achieves higher accuracy than standard time series models when there are thousands or more observations in the training history. However, it also takes longer to train and sweep over ForecastTCN models due to their higher capacity.\n\nYou can enable the ForecastTCN in AutoML by setting the `enable_dnn_training` flag in the set_training() method as follows:\n\n```python\n# Include ForecastTCN models in the model search\nforecasting_job.set_training(\n    enable_dnn_training=True\n)\n```\n\nTo enable DNN for an AutoML experiment created in the Azure Machine Learning studio, see the [task type settings in the studio UI how-to](how-to-use-automated-ml-for-ml-models.md#create-and-run-experiment).\n\n> [!NOTE]\n> * When you enable DNN for experiments created with the SDK, [best model explanations](how-to-machine-learning-interpretability-automl.md) are disabled.\n> * DNN support for forecasting in Automated Machine Learning is not supported for runs initiated in Databricks.\n> * GPU compute types are recommended when DNN training is enabled \n\n#### Target rolling window aggregation\n\nRecent values of the target are often impactful features in a forecasting model. Rolling window aggregations allow you to add rolling aggregations of data values as features. Generating and using these features as extra contextual data helps with the accuracy of the train model.\n\nConsider an energy demand forecasting scenario where weather data and historical demand are available.\nThe table shows resulting feature engineering that occurs when window aggregation is applied over the most recent three hours. Columns for **minimum, maximum,** and **sum** are generated on a sliding window of three hours based on the defined settings. For instance, for the observation valid on September 8, 2017 4:00am, the maximum, minimum, and sum values are calculated using the **demand values** for September 8, 2017 1:00AM - 3:00AM. This window of three hours shifts along to populate data for the remaining rows.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-forecast-143.md"
                }
            }
        ]
    },
    {
        "query": "Generate a python code to get ManagedOnlineEndpoint deployment logs using AzureML SDK V2?",
        "result": "To get ManagedOnlineEndpoint deployment logs using AzureML SDK V2, you can use the `get_logs()` method of the `ManagedOnlineDeploymentOperations` class. Here's an example:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\n# configure workspace details and get a handle to the workspace\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace = \"<AZUREML_WORKSPACE_NAME>\"\nml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)\n\n# get the deployment logs\ndeployment_name = \"<DEPLOYMENT_NAME>\"\nendpoint_name = \"<ENDPOINT_NAME>\"\ndeployment_logs = ml_client.deployments.get_logs(endpoint_name, deployment_name)\n```",
        "context": [
            {
                "page_content": "\n# [Python SDK](#tab/python)\n\n### Connect to Azure Machine Learning workspace\nConnect to Azure Machine Learning Workspace, configure workspace details, and get a handle to the workspace as follows:\n\n1. Import the required libraries:\n\n```python\n# import required libraries\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import (\n   ManagedOnlineEndpoint,\n   ManagedOnlineDeployment,\n   Model,\n   Environment,\n   CodeConfiguration,\n)\nfrom azure.identity import DefaultAzureCredential\n```\n\n2. Configure workspace details and get a handle to the workspace:\n\n```python\n# enter details of your AzureML workspace\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace = \"<AZUREML_WORKSPACE_NAME>\"\n\n# get a handle to the workspace\nml_client = MLClient(\n   DefaultAzureCredential(), subscription_id, resource_group, workspace\n)\n```\n\nFor more information, see [Deploy machine learning models to managed online endpoint using Python SDK v2](how-to-deploy-managed-online-endpoint-sdk-v2.md).\n\n### Configure online endpoint\n\n> [!TIP]\n> * `name`: The name of the endpoint. It must be unique in the Azure region. The name for an endpoint must start with an upper- or lowercase letter and only consist of '-'s and alphanumeric characters. For more information on the naming rules, see [managed online endpoint limits](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).\n> * `auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` doesn't expire, but `aml_token` does expire. For more information on authenticating, see [Authenticate to an online endpoint](how-to-authenticate-online-endpoint.md).\n\nOptionally, you can add description, tags to your endpoint.\n\n```python\n# Creating a unique endpoint name with current datetime to avoid conflicts\nimport datetime\n\nonline_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n\n# create an online endpoint\nendpoint = ManagedOnlineEndpoint(\n    name=online_endpoint_name,\n    description=\"this is a sample online endpoint\",\n    auth_mode=\"key\",\n    tags={\"foo\": \"bar\"},\n)\n```\n\n### Configure online deployment\n\nA deployment is a set of resources required for hosting the model that does the actual inferencing. We'll create a deployment for our endpoint using the `ManagedOnlineDeployment` class.\n\n> [!TIP]\n> - `name` - Name of the deployment.\n> - `endpoint_name` - Name of the endpoint to create the deployment under.\n> - `model` - The model to use for the deployment. This value can be either a reference to an existing versioned > model in the workspace or an inline model specification.\n> - `environment` - The environment to use for the deployment. This value can be either a reference to an existing > versioned environment in the workspace or an inline environment specification.\n> - `code_configuration` - the configuration for the source code and scoring script\n>     - `path`- Path to the source code directory for scoring the model\n>     - `scoring_script` - Relative path to the scoring file in the source code directory\n> - `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).\n> - `instance_count` - The number of instances to use for the deployment\n\n```python\n# create a blue deployment\nmodel = Model(name=\"tfserving-mounted\", version=\"1\", path=\"half_plus_two\")\n\nenv = Environment(\n    image=\"docker.io/tensorflow/serving:latest\",\n    inference_config={\n        \"liveness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n        \"readiness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n        \"scoring_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two:predict\"},\n    },\n)\n\nblue_deployment = ManagedOnlineDeployment(\n    name=\"blue\",\n    endpoint_name=online_endpoint_name,\n    model=model,\n    environment=env,\n    environment_variables={\n        \"MODEL_BASE_PATH\": \"/var/azureml-app/azureml-models/tfserving-mounted/1\",\n        \"MODEL_NAME\": \"half_plus_two\",\n    },\n    instance_type=\"Standard_DS2_v2\",\n    instance_count=1,\n)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-custom-container-812.md"
                }
            },
            {
                "page_content": "> The end-to-end example in this article comes from the files in the __azureml-examples__ GitHub repository. To clone the samples repository and switch to the repository's `cli/` directory, use the following commands: \n>\n> ```azurecli\n> git clone https://github.com/Azure/azureml-examples\n> cd azureml-examples/cli\n> ```\n\n## Limitations\n\n* The `v1_legacy_mode` flag must be disabled (false) on your Azure Machine Learning workspace. If this flag is enabled, you won't be able to create a managed online endpoint. For more information, see [Network isolation with v2 API](how-to-configure-network-isolation-with-v2.md).\n\n* If your Azure Machine Learning workspace has a private endpoint that was created before May 24, 2022, you must recreate the workspace's private endpoint before configuring your online endpoints to use a private endpoint. For more information on creating a private endpoint for your workspace, see [How to configure a private endpoint for Azure Machine Learning workspace](how-to-configure-private-link.md).\n\n* Secure outbound communication creates three private endpoints per deployment. One to the Azure Blob storage, one to the Azure Container Registry, and one to your workspace.\n\n* When you use network isolation with a deployment, Azure Log Analytics is partially supported. All metrics and the `AMLOnlineEndpointTrafficLog` table are supported via Azure Log Analytics. `AMLOnlineEndpointConsoleLog` and `AMLOnlineEndpointEventLog` tables are currently not supported. As a workaround, you can use the [az ml online-deployment get_logs](/cli/azure/ml/online-deployment#az-ml-online-deployment-get-logs) CLI command, the [OnlineDeploymentOperations.get_logs()](/python/api/azure-ai-ml/azure.ai.ml.operations.onlinedeploymentoperations#azure-ai-ml-operations-onlinedeploymentoperations-get-logs) Python SDK, or the Deployment log tab in the Azure Machine Learning studio instead. For more information, see [Monitoring online endpoints](how-to-monitor-online-endpoints.md).\n\n* You can configure public access to a __managed online endpoint__ (_inbound_ and _outbound_). You can also configure [public access to an Azure Machine Learning workspace](how-to-configure-private-link.md#enable-public-access).\n\n    Outbound communication from a managed online endpoint deployment is to the _workspace API_. When the endpoint is configured to use __public outbound__, then the workspace must be able to accept that public communication (allow public access).\n\n> [!NOTE]\n> Requests to create, update, or retrieve the authentication keys are sent to the Azure Resource Manager over the public network.\n \n## Inbound (scoring)\n\nTo secure scoring requests to the online endpoint to your virtual network, set the `public_network_access` flag for the endpoint to `disabled`:\n\n# [Azure CLI](#tab/cli)\n\n```azurecli\naz ml online-endpoint create -f endpoint.yml --set public_network_access=disabled\n```\n\n# [Python](#tab/python)\n\n```python\nfrom azure.ai.ml.entities import ManagedOnlineEndpoint\n\nendpoint = ManagedOnlineEndpoint(name='my-online-endpoint',  \n                         description='this is a sample online endpoint', \n                         tags={'foo': 'bar'}, \n                         auth_mode=\"key\", \n                         public_network_access=\"disabled\" \n                         # public_network_access=\"enabled\" \n)\n```\n\n# [Studio](#tab/azure-studio)\n\n1. Go to the [Azure Machine Learning studio](https://ml.azure.com).\n1. Select the **Workspaces** page from the left navigation bar.\n1. Enter a workspace by clicking its name.\n1. Select the **Endpoints** page from the left navigation bar.\n1. Select **+ Create** to open the **Create deployment** setup wizard.\n1. Disable the **Public network access** flag at the **Create endpoint** step.\n\n    :::image type=\"content\" source=\"media/how-to-secure-online-endpoint/endpoint-disable-public-network-access.png\" alt-text=\"A screenshot of how to disable public network access for an endpoint.\" lightbox=\"media/how-to-secure-online-endpoint/endpoint-disable-public-network-access.png\":::\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-secure-online-endpoint-38.md"
                }
            },
            {
                "page_content": "\n# [Python (MLflow SDK)](#tab/mlflow)\n\n```python\nmodel_name = 'sklearn-diabetes'\nmodel_local_path = \"sklearn-diabetes/model\"\n\nregistered_model = mlflow_client.create_model_version(\n    name=model_name, source=f\"file://{model_local_path}\"\n)\nversion = registered_model.version\n```\n\n# [Studio](#tab/studio)\n\nTo create a model in Azure Machine Learning, open the Models page in Azure Machine Learning. Select **Register model** and select where your model is located. Fill out the required fields, and then select __Register__.\n\n:::image type=\"content\" source=\"./media/how-to-manage-models/register-model-as-asset.png\" alt-text=\"Screenshot of the UI to register a model.\" lightbox=\"./media/how-to-manage-models/register-model-as-asset.png\":::\n\n\nAlternatively, if your model was logged inside of a run, you can register it directly.\n\n> [!TIP]\n> To register the model, you will need to know the location where the model has been stored. If you are using `autolog` feature of MLflow, the path will depend on the type and framework of the model being used. We recommend to check the jobs output to identify which is the name of this folder. You can look for the folder that contains a file named `MLModel`. If you are logging your models manually using `log_model`, then the path is the argument you pass to such method. As an example, if you log the model using `mlflow.sklearn.log_model(my_model, \"classifier\")`, then the path where the model is stored is `classifier`.\n\n# [Azure CLI](#tab/cli)\n\nUse the Azure ML CLI v2 to create a model from a training job output. In the following example, a model named `$MODEL_NAME` is registered using the artifacts of a job with ID `$RUN_ID`. The path where the model is stored is `$MODEL_PATH`.\n\n```bash\naz ml model create --name $MODEL_NAME --path azureml://jobs/$RUN_ID/outputs/artifacts/$MODEL_PATH\n```\n\n> [!NOTE]\n> The path `$MODEL_PATH` is the location where the model has been stored in the run.\n\n# [Python (Azure ML SDK)](#tab/sdk)\n\n```python\nmodel_name = 'sklearn-diabetes'\n\nml_client.models.create_or_update(\n    Model(\n        path=f\"azureml://jobs/{RUN_ID}/outputs/artifacts/{MODEL_PATH}\"\n        name=model_name,\n        type=AssetTypes.MLFLOW_MODEL\n    )\n) \n```\n\n> [!NOTE]\n> The path `MODEL_PATH` is the location where the model has been stored in the run.\n\n# [Python (MLflow SDK)](#tab/mlflow)\n\n```python\nmodel_name = 'sklearn-diabetes'\n\nregistered_model = mlflow_client.create_model_version(\n    name=model_name, source=f\"runs://{RUN_ID}/{MODEL_PATH}\"\n)\nversion = registered_model.version\n```\n\n> [!NOTE]\n> The path `MODEL_PATH` is the location where the model has been stored in the run.\n\n# [Studio](#tab/studio)\n\n:::image type=\"content\" source=\"media/how-to-deploy-mlflow-models-online-endpoints/mlflow-register-model-output.gif\" lightbox=\"media/how-to-deploy-mlflow-models-online-endpoints/mlflow-register-model-output.gif\" alt-text=\"Screenshot showing how to download Outputs and logs from Experimentation run\":::\n\n\n## Deploy an MLflow model to an online endpoint\n\n1. First. we need to configure the endpoint where the model will be deployed. The following example configures the name and authentication mode of the endpoint:\n    \n    # [Azure CLI](#tab/cli)\n    \n    __endpoint.yaml__\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\nname: my-endpoint\nauth_mode: key\n```\n\n    # [Python (Azure ML SDK)](#tab/sdk)\n\n    ```python\n    endpoint_name = \"sklearn-diabetes-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n\n    endpoint = ManagedOnlineEndpoint(\n        name=endpoint_name,\n        description=\"An online endpoint to generate predictions for the diabetes dataset\",\n        auth_mode=\"key\",\n        tags={\"foo\": \"bar\"},\n    )\n    ```\n\n    # [Python (MLflow SDK)](#tab/mlflow)\n\n    We can configure the properties of this endpoint using a configuration file. In this case, we are configuring the authentication mode of the endpoint to be \"key\".\n    \n    ```python\n    endpoint_config = {\n        \"auth_mode\": \"key\",\n        \"identity\": {\n            \"type\": \"system_assigned\"\n        }\n    }\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-mlflow-models-online-endpoints-157.md"
                }
            }
        ]
    },
    {
        "query": "What are the most common errors that one encounter while using ManagedOnlineEndpoint along with AzureML SDK V2?",
        "result": "Without more specific information, it's difficult to provide a comprehensive list of the most common errors when using ManagedOnlineEndpoint with AzureML SDK V2. However, some potential issues that could arise include authentication errors, issues with the deployment configuration, and problems with the scoring script. It's important to review the documentation and ensure that all necessary dependencies are included in the deployment configuration. Additionally, it's recommended to test the endpoint thoroughly before deploying it to production.",
        "context": [
            {
                "page_content": "\n*Applies to: Azure CLI extensions v1 & v2*\n\nCreate a workspace connection from a YAML specification file\n\n```\naz ml connection create --file connection.yml --resource-group my-resource-group --workspace-name my-workspace\n```\n\n**Resources**\n* [Python SDK v1 workspace connections](https://aka.ms/azureml/environment/set-connection-v1)\n* [Python SDK v2 workspace connections](/python/api/azure-ai-ml/azure.ai.ml.entities.workspaceconnection)\n* [Azure CLI workspace connections](/cli/azure/ml/connection)\n\n### Forbidden blob\n<!--issueDescription-->\nThis issue can happen when an attempt to access a blob in a storage account is rejected.\n\n**Potential causes:**\n* The authorization method you're using to access the storage account is invalid\n* You're attempting to authorize via shared access signature (SAS), but the SAS token is expired or invalid\n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nRead the following to understand [how to authorize access to blob data in the Azure portal](../storage/blobs/authorize-data-operations-portal.md)\n\nRead the following to understand [how to authorize access to data in Azure storage](../storage/common/authorize-data-access.md)\n\nRead the following if you're interested in [using SAS to access Azure storage resources](../storage/common/storage-sas-overview.md)\n\n### Horovod build\n<!--issueDescription-->\nThis issue can happen when the conda environment fails to be created or updated because horovod failed to build.\n\n**Potential causes:**\n* Horovod installation requires other modules that you haven't installed\n* Horovod installation requires certain libraries that you haven't included\n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nMany issues could cause a horovod failure, and there's a comprehensive list of them in horovod's documentation\n* Review the [horovod troubleshooting guide](https://horovod.readthedocs.io/en/stable/troubleshooting_include.html#) \n* Review your Build log to see if there's an error message that surfaced when horovod failed to build\n* It's possible that the problem you're encountering is detailed in the horovod troubleshooting guide, along with a solution\n\n**Resources**\n* [horovod installation](https://aka.ms/azureml/environment/install-horovod)\n\n### Conda command not found\n<!--issueDescription-->\nThis issue can happen when the conda command isn't recognized during conda environment creation or update.\n\n**Potential causes:**\n* conda isn't installed in the base image you're using\n* conda isn't installed via your Dockerfile before you try to execute the conda command\n* conda isn't included in or wasn't added to your path\n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nEnsure that you have a conda installation step in your Dockerfile before trying to execute any conda commands\n* Review this [list of conda installers](https://docs.conda.io/en/latest/miniconda.html) to determine what you need for your scenario\n\nIf you've tried installing conda and are experiencing this issue, ensure that you've added conda to your path\n* Review this [example](https://stackoverflow.com/questions/58269375/how-to-install-packages-with-miniconda-in-dockerfile) for guidance\n* Review how to set [environment variables in a Dockerfile](https://docs.docker.com/engine/reference/builder/#env)\n\n**Resources**\n* All available conda distributions are found in the [conda repository](https://repo.anaconda.com/miniconda/)\n\n### Incompatible Python version\n<!--issueDescription-->\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-1523.md"
                }
            },
            {
                "page_content": "Automated ML normalizes regression and forecasting metrics which enable comparison between models trained on data with different ranges. A model trained on a data with a larger range has higher error than the same model trained on data with a smaller range, unless that error is normalized.\n\nWhile there is no standard method of normalizing error metrics, automated ML takes the common approach of dividing the error by the range of the data: `normalized_error = error / (y_max - y_min)`\n\n>[!Note]\n>The range of data is not saved with the model. If you do inference with the same model on a holdout test set, `y_min` and `y_max` may change according to the test data and the normalized metrics may not be directly used to compare the model's performance on training and test sets. You can pass in the value of `y_min` and `y_max` from your training set to make the comparison fair.\n\nWhen evaluating a forecasting model on time series data, automated ML takes extra steps to ensure that normalization happens per time series ID (grain), because each time series likely has a different distribution of target values.\n## Residuals\n\nThe residuals chart is a histogram of the prediction errors (residuals) generated for regression and forecasting experiments. Residuals are calculated as `y_predicted - y_true` for all samples and then displayed as a histogram to show model bias.\n\nIn this example, note that both models are slightly biased to predict lower than the actual value. This is not uncommon for a dataset with a skewed distribution of actual targets, but indicates worse model performance. A good model will have a residuals distribution that peaks at zero with few residuals at the extremes. A worse model will have a spread out residuals distribution with fewer samples around zero.\n\n### Residuals chart for a good model\n![Residuals chart for a good model](./media/how-to-understand-automated-ml/chart-residuals-good.png)\n\n### Residuals chart for a bad model\n![Residuals chart for a bad model](./media/how-to-understand-automated-ml/chart-residuals-bad.png)\n\n## Predicted vs. true\n\nFor regression and forecasting experiment the predicted vs. true chart plots the relationship between the target feature (true/actual values) and the model's predictions. The true values are binned along the x-axis and for each bin the mean predicted value is plotted with error bars. This allows you to see if a model is biased toward predicting certain values. The line displays the average prediction and the shaded area indicates the variance of predictions around that mean.\n\nOften, the most common true value will have the most accurate predictions with the lowest variance. The distance of the trend line from the ideal `y = x` line where there are few true values is a good measure of model performance on outliers. You can use the histogram at the bottom of the chart to reason about the actual data distribution. Including more data samples where the distribution is sparse can improve model performance on unseen data.\n\nIn this example, note that the better model has a predicted vs. true line that is closer to the ideal `y = x` line.\n\n### Predicted vs. true chart for a good model\n![Predicted vs. true chart for a good model](./media/how-to-understand-automated-ml/chart-predicted-true-good.png)\n\n### Predicted vs. true chart for a bad model\n![Predicted vs. true chart for a bad model](./media/how-to-understand-automated-ml/chart-predicted-true-bad.png)\n\n## Forecast horizon (preview)\n\nFor forecasting experiments, the forecast horizon chart plots the relationship between the models predicted value and the actual values mapped over time per cross validation fold, up to 5 folds. The x axis maps time based on the frequency you provided during training setup. The vertical line in the chart marks the forecast horizon point also referred to as the horizon line, which is the time period at which you would want to start generating predictions. To the left of the forecast horizon line, you can view historic training data to better visualize past trends. To the right of the forecast horizon, you can visualize the predictions (the purple line) against the actuals (the blue line) for the different cross validation folds and time series identifiers. The shaded purple area indicates the confidence intervals or variance of predictions around that mean. \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-understand-automated-ml-192.md"
                }
            },
            {
                "page_content": "\n# Troubleshooting environment image builds using troubleshooting log error messages\n\nIn this article, learn how to troubleshoot common problems you may encounter with environment image builds.\n\n## Azure Machine Learning environments\n\nAzure Machine Learning environments are an encapsulation of the environment where your machine learning training happens.\nThey specify the base docker image, Python packages, and software settings around your training and scoring scripts.\nEnvironments are managed and versioned assets within your Machine Learning workspace that enable reproducible, auditable, and portable machine learning workflows across various compute targets.\n\n### Types of environments\n\nEnvironments can broadly be divided into three categories: curated, user-managed, and system-managed.\n\nCurated environments are pre-created environments that are managed by Azure Machine Learning (AzureML) and are available by default in every workspace.\n\nIntended to be used as is, they contain collections of Python packages and settings to help you get started with various machine learning frameworks.\nThese pre-created environments also allow for faster deployment time.\n\nIn user-managed environments, you're responsible for setting up your environment and installing every package that your training script needs on the compute target.\nAlso be sure to include any dependencies needed for model deployment.\nThese types of environments are represented by two subtypes. For the first type, BYOC (bring your own container), you bring an existing Docker image to AzureML. For the second type, Docker build context based environments, AzureML materializes the image from the context that you provide.\n\nSystem-managed environments are used when you want conda to manage the Python environment for you.\nA new isolated conda environment is materialized from your conda specification on top of a base Docker image. By default, common properties are added to the derived image.\nNote that environment isolation implies that Python dependencies installed in the base image won't be available in the derived image.\n\n### Create and manage environments\n\nYou can create and manage environments from clients like AzureML Python SDK, AzureML CLI, AzureML Studio UI, VS code extension. \n\n\"Anonymous\" environments are automatically registered in your workspace when you submit an experiment without registering or referencing an already existing environment.\nThey won't be listed but may be retrieved by version or label.\n\nAzureML builds environment definitions into Docker images.\nIt also caches the environments in the Azure Container Registry associated with your AzureML Workspace so they can be reused in subsequent training jobs and service endpoint deployments.\nMultiple environments with the same definition may result the same image, so the cached image will be reused.\nRunning a training script remotely requires the creation of a Docker image.\n\n### Reproducibility and vulnerabilities\n\n#### *Vulnerabilities*\n\nVulnerabilities can be addressed by upgrading to a newer version of a dependency or migrating to a different dependency that satisfies security\nrequirements. Mitigating vulnerabilities is time consuming and costly since it can require refactoring of code and infrastructure. With the prevalence\nof open source software and the use of complicated nested dependencies, it's important to manage and keep track of vulnerabilities.\n\nThere are some ways to decrease the impact of vulnerabilities:\n\n- Reduce your number of dependencies - use the minimal set of the dependencies for each scenario.\n- Compartmentalize your environment so issues can be scoped and fixed in one place.\n- Understand flagged vulnerabilities and their relevance to your scenario.\n\n#### *Vulnerabilities vs Reproducibility*\n\nReproducibility is one of the foundations of software development. While developing production code, a repeated operation must guarantee the same\nresult. Mitigating vulnerabilities can disrupt reproducibility by changing dependencies.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-0.md"
                }
            }
        ]
    },
    {
        "query": "Generate python Code samples that shows how to create a three node pipeline that has data preparation, training a model and deploying a model to an online endpoint using AzureML SDK V2?",
        "result": "This tutorial provides a step-by-step guide on how to create a three-node pipeline that has data preparation, model training, and model deployment to an online endpoint using AzureML SDK V2. You'll learn how to use the AzureML Python SDK v2 to connect to your Azure ML workspace, create Azure ML data assets, create reusable Azure ML components, create, validate and run Azure ML pipelines, deploy the newly-trained model as an endpoint, and call the Azure ML endpoint for inferencing. The tutorial also includes code samples and instructions on how to set up the required resources.",
        "context": [
            {
                "page_content": "\n# Tutorial: Create production ML pipelines with Python SDK v2 in a Jupyter notebook\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n> [!NOTE]\n> For a tutorial that uses SDK v1 to build a pipeline, see [Tutorial: Build an Azure Machine Learning pipeline for image classification](v1/tutorial-pipeline-python-sdk.md)\n> \n\nIn this tutorial, you'll use Azure Machine Learning (Azure ML) to create a production ready machine learning (ML) project, using AzureML Python SDK v2.\n\nYou'll learn how to use the AzureML Python SDK v2 to:\n\n> [!div class=\"checklist\"]\n> \n> * Connect to your Azure ML workspace\n> * Create Azure ML data assets\n> * Create reusable Azure ML components\n> * Create, validate and run Azure ML pipelines\n> * Deploy the newly-trained model as an endpoint\n> * Call the Azure ML endpoint for inferencing\n\n## Prerequisites\n\n* Complete the [Quickstart: Get started with Azure Machine Learning](quickstart-create-resources.md) to:\n    * Create a workspace.\n    * Create a cloud-based compute instance to use for your development environment.\n    * Create a cloud-based compute cluster to use for training your model.\n* Complete the [Quickstart: Run Jupyter notebooks in studio](quickstart-run-notebooks.md) to clone the **SDK v2/tutorials** folder.\n\n\n## Open the notebook\n\n1. Open the **tutorials** folder that was cloned into your **Files** section from the [Quickstart: Run Jupyter notebooks in studio](quickstart-run-notebooks.md).\n    \n1. Select the **e2e-ml-workflow.ipynb** file from your **tutorials/azureml-examples/tutorials/e2e-ds-experience/** folder. \n\n    :::image type=\"content\" source=\"media/tutorial-pipeline-python-sdk/expand-folder.png\" alt-text=\"Screenshot shows the open tutorials folder.\":::\n\n1. On the top bar, select the compute instance you created during the  [Quickstart: Get started with Azure Machine Learning](quickstart-create-resources.md) to use for running the notebook.\n\n> [!Important]\n> The rest of this article contains the same content as you see in the notebook.  \n>\n> Switch to the Jupyter Notebook now if you want to run the code while you read along.\n> To run a single code cell in a notebook, click the code cell and hit **Shift+Enter**. Or, run the entire notebook by choosing **Run all** from the top toolbar\n\n## Introduction\n\nIn this tutorial, you'll create an Azure ML pipeline to train a model for credit default prediction. The pipeline handles the data preparation, training and registering the trained model.  You'll then run the pipeline, deploy the model and use it.\n\nThe image below shows the pipeline as you'll see it in the AzureML portal once submitted. It's a rather simple pipeline we'll use to walk you through the AzureML SDK v2.\n\nThe two steps are first data preparation and second training. \n\n:::image type=\"content\" source=\"media/tutorial-pipeline-python-sdk/pipeline-overview.jpg\" alt-text=\"Diagram shows overview of the pipeline.\":::\n\n## Set up the pipeline resources\n\nThe Azure ML framework can be used from CLI, Python SDK, or studio interface. In this example, you'll use the AzureML Python SDK v2 to create a pipeline. \n\nBefore creating the pipeline, you'll set up the resources the pipeline will use:\n\n* The data asset for training\n* The software environment to run the pipeline\n* A compute resource to where the job will run\n\n## Connect to the workspace\n\nBefore we dive in the code, you'll need to connect to your Azure ML workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. \n\n[!Notebook-python[] (~/azureml-examples-main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb?name=import-mlclient)]\n\nIn the next cell, enter your Subscription ID, Resource Group name and Workspace name. To find your Subscription ID:\n1. In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n1. You'll see the values you need for **<SUBSCRIPTION_ID>**, **<RESOURCE_GROUP>**, and **<AML_WORKSPACE_NAME>**.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-pipeline-python-sdk-0.md"
                }
            },
            {
                "page_content": "In case of compute instance, `max_concurrent_trials` can be set to be the same as number of cores on the compute instance VM.\n\n## Explore models and metrics\n\nAutomated ML offers options for you to monitor and evaluate your training results. \n\n* For definitions and examples of the performance charts and metrics provided for each run, see [Evaluate automated machine learning experiment results](how-to-understand-automated-ml.md).\n\n* To get a featurization summary and understand what features were added to a particular model, see [Featurization transparency](how-to-configure-auto-features.md#featurization-transparency). \n\nFrom Azure Machine Learning UI at the model's page you can also view the hyperparameters used when training a particular model and also view and customize the internal model's training code used. \n\n## Register and deploy models\n\nAfter you test a model and confirm you want to use it in production, you can register it for later use.\n\n\n> [!TIP]\n> For registered models, one-click deployment is available via the [Azure Machine Learning studio](https://ml.azure.com). See [how to deploy registered models from the studio](how-to-use-automated-ml-for-ml-models.md#deploy-your-model). \n\n## AutoML in pipelines\n\nTo leverage AutoML in your MLOps workflows, you can add AutoML Job steps to your [AzureML Pipelines](./how-to-create-component-pipeline-python.md). This allows you to automate your entire workflow by hooking up your data prep scripts to AutoML and then registering and validating the resulting best model.\n\nBelow is a [sample pipeline](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/pipelines/1h_automl_in_pipeline/automl-classification-bankmarketing-in-pipeline) with an AutoML classification component and a command component that shows the resulting AutoML output. Note how the inputs (training & validation data) and the outputs (best model) are referenced in different steps.\n\n``` python\n# Define pipeline\n@pipeline(\n    description=\"AutoML Classification Pipeline\",\n    )\ndef automl_classification(\n    classification_train_data,\n    classification_validation_data\n):\n    # define the automl classification task with automl function\n    classification_node = classification(\n        training_data=classification_train_data,\n        validation_data=classification_validation_data,\n        target_column_name=\"y\",\n        primary_metric=\"accuracy\",\n        # currently need to specify outputs \"mlflow_model\" explictly to reference it in following nodes \n        outputs={\"best_model\": Output(type=\"mlflow_model\")},\n    )\n    # set limits and training\n    classification_node.set_limits(max_trials=1)\n    classification_node.set_training(enable_stack_ensemble=False, enable_vote_ensemble=False)\n\n    command_func = command(\n        inputs=dict(\n            automl_output=Input(type=\"mlflow_model\")\n        ),\n        command=\"ls ${{inputs.automl_output}}\",\n        environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:latest\"\n    )\n    show_output = command_func(automl_output=classification_node.outputs.best_model)\n\n\npipeline_classification = automl_classification(\n    classification_train_data=Input(path=\"./training-mltable-folder/\", type=\"mltable\"),\n    classification_validation_data=Input(path=\"./validation-mltable-folder/\", type=\"mltable\"),\n)\n\n# ...\n# Note that the above is only a snippet from the bankmarketing example you can find in our examples repo -> https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/pipelines/1h_automl_in_pipeline/automl-classification-bankmarketing-in-pipeline\n\n```\n\nFor more examples on how to do include AutoML in your pipelines, please check out our [examples repo](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/pipelines/1h_automl_in_pipeline/).\n\n## Next steps\n\n+ Learn more about [how and where to deploy a model](./how-to-deploy-online-endpoints.md).",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-configure-auto-train-357.md"
                }
            },
            {
                "page_content": "\n# Progressive rollout of MLflow models to Online Endpoints\n\nIn this article, you'll learn how you can progressively update and deploy MLflow models to Online Endpoints without causing service disruption. You'll use blue-green deployment, also known as a safe rollout strategy, to introduce a new version of a web service to production. This strategy will allow you to roll out your new version of the web service to a small subset of users or requests before rolling it out completely.\n\n## About this example\n\nOnline Endpoints have the concept of __Endpoint__ and __Deployment__. An endpoint represents the API that customers use to consume the model, while the deployment indicates the specific implementation of that API. This distinction allows users to decouple the API from the implementation and to change the underlying implementation without affecting the consumer. This example will use such concepts to update the deployed model in endpoints without introducing service disruption. \n\nThe model we will deploy is based on the [UCI Heart Disease Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). The database contains 76 attributes, but we are using a subset of 14 of them. The model tries to predict the presence of heart disease in a patient. It is integer valued from 0 (no presence) to 1 (presence). It has been trained using an `XGBBoost` classifier and all the required preprocessing has been packaged as a `scikit-learn` pipeline, making this model an end-to-end pipeline that goes from raw data to predictions.\n\nThe information in this article is based on code samples contained in the [azureml-examples](https://github.com/azure/azureml-examples) repository. To run the commands locally without having to copy/paste files, clone the repo, and then change directories to `sdk/using-mlflow/deploy`.\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in the following notebooks. In the cloned repository, open the notebook: [mlflow_sdk_online_endpoints_progresive.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/using-mlflow/deploy/mlflow_sdk_online_endpoints_progresive.ipynb).\n\n## Prerequisites\n\nBefore following the steps in this article, make sure you have the following prerequisites:\n\n- An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n- Azure role-based access controls (Azure RBAC) are used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned the owner or contributor role for the Azure Machine Learning workspace, or a custom role allowing Microsoft.MachineLearningServices/workspaces/onlineEndpoints/*. For more information, see [Manage access to an Azure Machine Learning workspace](how-to-assign-roles.md).\n\nAdditionally, you will need to:\n\n# [Azure CLI](#tab/cli)\n\n- Install the Azure CLI and the ml extension to the Azure CLI. For more information, see [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n\n# [Python (Azure ML SDK)](#tab/sdk)\n\n- Install the Azure Machine Learning SDK for Python\n    \n    ```bash\n    pip install azure-ai-ml\n    ```\n    \n# [Python (MLflow SDK)](#tab/mlflow)\n\n- Install the Mlflow SDK package `mlflow` and the Azure Machine Learning plug-in for MLflow `azureml-mlflow`.\n\n    ```bash\n    pip install mlflow azureml-mlflow\n    ```\n\n- If you are not running in Azure Machine Learning compute, configure the MLflow tracking URI or MLflow's registry URI to point to the workspace you are working on. See [Configure MLflow for Azure Machine Learning](how-to-use-mlflow-configure-tracking.md) for more details.\n\n\n### Connect to your workspace\n\nFirst, let's connect to Azure Machine Learning workspace where we are going to work on.\n\n# [Azure CLI](#tab/cli)\n\n```azurecli\naz account set --subscription <subscription>\naz configure --defaults workspace=<workspace> group=<resource-group> location=<location>\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-mlflow-models-online-progressive-0.md"
                }
            }
        ]
    },
    {
        "query": "How to enable public access to a private link workspace using AzureML SDK V2?",
        "result": "To enable public access to a private link workspace using AzureML SDK V2, you need to configure the workspace to allow public access. You can do this by setting the `public_network_access` flag to `enabled` when creating the workspace. If you have already created the workspace, you can update the workspace configuration to allow public access by using the `az ml workspace update` command with the `--public-network-access` flag set to `enabled`. Note that enabling public access to a private link workspace is only recommended if you plan on using managed online endpoint deployments that use public outbound.",
        "context": [
            {
                "page_content": "\n# Use network isolation with managed online endpoints\n\n[!INCLUDE [SDK/CLI v2](../../includes/machine-learning-dev-v2.md)]\n\nWhen deploying a machine learning model to a managed online endpoint, you can secure communication with the online endpoint by using [private endpoints](../private-link/private-endpoint-overview.md).\n\nYou can secure the inbound scoring requests from clients to an _online endpoint_. You can also secure the outbound communications between a _deployment_ and the Azure resources it uses. Security for inbound and outbound communication are configured separately. For more information on endpoints and deployments, see [What are endpoints and deployments](concept-endpoints.md#what-are-endpoints-and-deployments).\n\nThe following diagram shows how communications flow through private endpoints to the managed online endpoint. Incoming scoring requests from clients are received through the workspace private endpoint from your virtual network. Outbound communication with services is handled through private endpoints to those service instances from the deployment:\n\n:::image type=\"content\" source=\"./media/how-to-secure-online-endpoint/endpoint-network-isolation-ingress-egress.png\" alt-text=\"Diagram of overall ingress/egress communication.\":::\n\n## Prerequisites\n\n* To use Azure machine learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/) today.\n\n* You must install and configure the Azure CLI and `ml` extension or the AzureML Python SDK v2. For more information, see the following articles:\n\n    * [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n    * [Install the Python SDK v2](https://aka.ms/sdk-v2-install).\n\n* You must have an Azure Resource Group, in which you (or the service principal you use) need to have `Contributor` access. You'll have such a resource group if you configured your `ml` extension per the above article.\n\n* You must have an Azure Machine Learning workspace, and the workspace must use a private endpoint. If you don't have one, the steps in this article create an example workspace, VNet, and VM. For more information, see [Configure a private endpoint for Azure Machine Learning workspace](./how-to-configure-private-link.md).\n\n    The workspace configuration can either allow or disallow public network access. If you plan on using managed online endpoint deployments that use __public outbound__, then you must also [configure the workspace to allow public access](how-to-configure-private-link.md#enable-public-access).\n\n    Outbound communication from managed online endpoint deployment is to the _workspace API_. When the endpoint is configured to use __public outbound__, then the workspace must be able to accept that public communication (allow public access).\n\n* When the workspace is configured with a private endpoint, the Azure Container Registry for the workspace must be configured for __Premium__ tier. For more information, see [Azure Container Registry service tiers](../container-registry/container-registry-skus.md).\n\n* The Azure Container Registry and Azure Storage Account must be in the same Azure Resource Group as the workspace.\n\n* If you want to use a [user-assigned managed identity](../active-directory/managed-identities-azure-resources/how-manage-user-assigned-managed-identities.md?pivots=identity-mi-methods-azp) to create and manage online endpoints and online deployments, the identity should have the proper permissions. For details about the required permissions, see [Set up service authentication](./how-to-identity-based-service-authentication.md#workspace). For example, you need to assign the proper RBAC permission for Azure Key Vault on the identity.\n\n> [!IMPORTANT]\n> The end-to-end example in this article comes from the files in the __azureml-examples__ GitHub repository. To clone the samples repository and switch to the repository's `cli/` directory, use the following commands: \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-secure-online-endpoint-0.md"
                }
            },
            {
                "page_content": "\n<!-- # Virtual network isolation and privacy overview -->\n# Secure Azure Machine Learning workspace resources using virtual networks (VNets)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n> [!div class=\"op_single_selector\" title1=\"Select the Azure Machine Learning SDK or CLI version you are using:\"]\n> * [SDK/CLI v1](v1/how-to-network-security-overview.md)\n> * [SDK/CLI v2 (current version)](how-to-network-security-overview.md)\n\nSecure Azure Machine Learning workspace resources and compute environments using virtual networks (VNets). This article uses an example scenario to show you how to configure a complete virtual network.\n\n> [!TIP]\n> This article is part of a series on securing an Azure Machine Learning workflow. See the other articles in this series:\n>\n> * [Secure the workspace resources](how-to-secure-workspace-vnet.md)\n> * [Secure the training environment](how-to-secure-training-vnet.md)\n> * [Secure the inference environment](how-to-secure-inferencing-vnet.md)\n> * [Enable studio functionality](how-to-enable-studio-virtual-network.md)\n> * [Use custom DNS](how-to-custom-dns.md)\n> * [Use a firewall](how-to-access-azureml-behind-firewall.md)\n> * [API platform network isolation](how-to-configure-network-isolation-with-v2.md)\n>\n> For a tutorial on creating a secure workspace, see [Tutorial: Create a secure workspace](tutorial-create-secure-workspace.md) or [Tutorial: Create a secure workspace using a template](tutorial-create-secure-workspace-template.md).\n\n## Prerequisites\n\nThis article assumes that you have familiarity with the following topics:\n+ [Azure Virtual Networks](../virtual-network/virtual-networks-overview.md)\n+ [IP networking](../virtual-network/ip-services/public-ip-addresses.md)\n+ [Azure Machine Learning workspace with private endpoint](how-to-configure-private-link.md)\n+ [Network Security Groups (NSG)](../virtual-network/network-security-groups-overview.md)\n+ [Network firewalls](../firewall/overview.md)\n## Example scenario\n\nIn this section, you learn how a common network scenario is set up to secure Azure Machine Learning communication with private IP addresses.\n\nThe following table compares how services access different parts of an Azure Machine Learning network with and without a VNet:\n\n| Scenario | Workspace | Associated resources | Training compute environment | Inferencing compute environment |\n|-|-|-|-|-|-|\n|**No virtual network**| Public IP | Public IP | Public IP | Public IP |\n|**Public workspace, all other resources in a virtual network** | Public IP | Public IP (service endpoint) <br> **- or -** <br> Private IP (private endpoint) | Public IP | Private IP  |\n|**Secure resources in a virtual network**| Private IP (private endpoint) | Public IP (service endpoint) <br> **- or -** <br> Private IP (private endpoint) | Private IP | Private IP  | \n\n* **Workspace** - Create a private endpoint for your workspace. The private endpoint connects the workspace to the vnet through several private IP addresses.\n    * **Public access** - You can optionally enable public access for a secured workspace.\n* **Associated resource** - Use service endpoints or private endpoints to connect to workspace resources like Azure storage, Azure Key Vault. For Azure Container Services, use a private endpoint.\n    * **Service endpoints** provide the identity of your virtual network to the Azure service. Once you enable service endpoints in your virtual network, you can add a virtual network rule to secure the Azure service resources to your virtual network. Service endpoints use public IP addresses.\n    * **Private endpoints** are network interfaces that securely connect you to a service powered by Azure Private Link. Private endpoint uses a private IP address from your VNet, effectively bringing the service into your VNet.\n* **Training compute access** - Access training compute targets like Azure Machine Learning Compute Instance and Azure Machine Learning Compute Clusters with public or private IP addresses.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-network-security-overview-0.md"
                }
            },
            {
                "page_content": "> The end-to-end example in this article comes from the files in the __azureml-examples__ GitHub repository. To clone the samples repository and switch to the repository's `cli/` directory, use the following commands: \n>\n> ```azurecli\n> git clone https://github.com/Azure/azureml-examples\n> cd azureml-examples/cli\n> ```\n\n## Limitations\n\n* The `v1_legacy_mode` flag must be disabled (false) on your Azure Machine Learning workspace. If this flag is enabled, you won't be able to create a managed online endpoint. For more information, see [Network isolation with v2 API](how-to-configure-network-isolation-with-v2.md).\n\n* If your Azure Machine Learning workspace has a private endpoint that was created before May 24, 2022, you must recreate the workspace's private endpoint before configuring your online endpoints to use a private endpoint. For more information on creating a private endpoint for your workspace, see [How to configure a private endpoint for Azure Machine Learning workspace](how-to-configure-private-link.md).\n\n* Secure outbound communication creates three private endpoints per deployment. One to the Azure Blob storage, one to the Azure Container Registry, and one to your workspace.\n\n* When you use network isolation with a deployment, Azure Log Analytics is partially supported. All metrics and the `AMLOnlineEndpointTrafficLog` table are supported via Azure Log Analytics. `AMLOnlineEndpointConsoleLog` and `AMLOnlineEndpointEventLog` tables are currently not supported. As a workaround, you can use the [az ml online-deployment get_logs](/cli/azure/ml/online-deployment#az-ml-online-deployment-get-logs) CLI command, the [OnlineDeploymentOperations.get_logs()](/python/api/azure-ai-ml/azure.ai.ml.operations.onlinedeploymentoperations#azure-ai-ml-operations-onlinedeploymentoperations-get-logs) Python SDK, or the Deployment log tab in the Azure Machine Learning studio instead. For more information, see [Monitoring online endpoints](how-to-monitor-online-endpoints.md).\n\n* You can configure public access to a __managed online endpoint__ (_inbound_ and _outbound_). You can also configure [public access to an Azure Machine Learning workspace](how-to-configure-private-link.md#enable-public-access).\n\n    Outbound communication from a managed online endpoint deployment is to the _workspace API_. When the endpoint is configured to use __public outbound__, then the workspace must be able to accept that public communication (allow public access).\n\n> [!NOTE]\n> Requests to create, update, or retrieve the authentication keys are sent to the Azure Resource Manager over the public network.\n \n## Inbound (scoring)\n\nTo secure scoring requests to the online endpoint to your virtual network, set the `public_network_access` flag for the endpoint to `disabled`:\n\n# [Azure CLI](#tab/cli)\n\n```azurecli\naz ml online-endpoint create -f endpoint.yml --set public_network_access=disabled\n```\n\n# [Python](#tab/python)\n\n```python\nfrom azure.ai.ml.entities import ManagedOnlineEndpoint\n\nendpoint = ManagedOnlineEndpoint(name='my-online-endpoint',  \n                         description='this is a sample online endpoint', \n                         tags={'foo': 'bar'}, \n                         auth_mode=\"key\", \n                         public_network_access=\"disabled\" \n                         # public_network_access=\"enabled\" \n)\n```\n\n# [Studio](#tab/azure-studio)\n\n1. Go to the [Azure Machine Learning studio](https://ml.azure.com).\n1. Select the **Workspaces** page from the left navigation bar.\n1. Enter a workspace by clicking its name.\n1. Select the **Endpoints** page from the left navigation bar.\n1. Select **+ Create** to open the **Create deployment** setup wizard.\n1. Disable the **Public network access** flag at the **Create endpoint** step.\n\n    :::image type=\"content\" source=\"media/how-to-secure-online-endpoint/endpoint-disable-public-network-access.png\" alt-text=\"A screenshot of how to disable public network access for an endpoint.\" lightbox=\"media/how-to-secure-online-endpoint/endpoint-disable-public-network-access.png\":::\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-secure-online-endpoint-38.md"
                }
            }
        ]
    },
    {
        "query": "Generate a Python code sample to register a private Container registry to a workspace using AzureML SDK V2?",
        "result": "Here's a Python code sample to register a private Container registry to a workspace using AzureML SDK V2:\n\n```python\nfrom azureml.core import Workspace\nfrom azureml.core.container_registry import ContainerRegistry\n\nws = Workspace.from_config()\n\n# Define the container registry\ncontainer_registry = ContainerRegistry(\n    workspace=ws,\n    name='myregistry',\n    address='myregistry.azurecr.io',\n    username='myregistryusername',\n    password='myregistrypassword'\n)\n\n# Register the container registry\ncontainer_registry = ws.container_registries.register(\n    workspace=ws,\n    name='myregistry',\n    container_registry=container_registry\n)\n```",
        "context": [
            {
                "page_content": "\n*Applies to: all scenarios*\n\nIf you're using a YAML for your conda specification, specify a version for pip\n\n```yaml\nname: project_environment\ndependencies:\n  - python=3.8\n  - pip=22.3.1\n  - pip:\n      - numpy=1.24.1\nchannels:\n  - anaconda\n  - conda-forge\n```\n\n**Resources**\n* See [conda package pinning](https://aka.ms/azureml/environment/how-to-pin-conda-packages)\n\n## *Miscellaneous environment issues*\n### R section is deprecated\n<!--issueDescription-->\n**Potential causes:**\n* You specified an R section in your environment definition\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nThe AzureML SDK for R was deprecated at the end of 2021 to make way for an improved R training and deployment experience using the Azure CLI v2\n\n*Applies to: Python SDK v1*\n\nRemove the R section from your environment definition\n\n```python\nenv.r = None\n```\n\n*Applies to: all scenarios*\n\nSee the [samples repository](https://aka.ms/azureml/environment/train-r-models-cli-v2) to get started training R models using the Azure CLI v2\n\n### No definition exists for environment\n<!--issueDescription-->\n**Potential causes:**\n* You specified an environment that doesn't exist or hasn't been registered\n* There was a misspelling or syntactical error in the way you specified your environment name or environment version\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nEnsure that you are specifying your environment name correctly, along with the correct version\n* `path-to-resource:version-number`\n\nThe 'latest' version of your environment is specified in a slightly different way\n* `path-to-resource@latest`\n\n## **Image build problems**\n\n## *ACR issues*\n### ACR unreachable\n<!--issueDescription-->\nThis issue can happen by failing to access a workspace's associated Azure Container Registry (ACR) resource.\n\n**Potential causes:**\n* Workspace's ACR is behind a virtual network (VNet) (private endpoint or service endpoint), and no compute cluster is used to build images.\n* Workspace's ACR is behind a virtual network (private endpoint or service endpoint), and the compute cluster used for building images have no access to the workspace's ACR.\n\n**Affected areas (symptoms):**\n* Failure in building environments from UI, SDK, and CLI.\n* Failure in running jobs because it will implicitly build the environment in the first step.\n* Pipeline job failures.\n* Model deployment failures.\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nUpdate the workspace image build compute property using SDK:\n\n```python\nfrom azureml.core import Workspace\nws = Workspace.from_config()\nws.update(image_build_compute = 'mycomputecluster')\n```\n\n*Applies to: Azure CLI extensions v1 & v2*\n\nUpdate the workspace image build compute property using Azure CLI:\n\n```\naz ml workspace update --name myworkspace --resource-group myresourcegroup --image-build-compute mycomputecluster\n```\n\n> [!NOTE]\n> * Only Azure Machine Learning compute clusters are supported. Compute, Azure Kubernetes Service (AKS), or other instance types are not supported for image build compute.\n> * Make sure the compute cluster's VNet that's used for the image build compute has access to the workspace's ACR.\n> * Make sure the compute cluster is CPU based.\n\n**Resources**\n* [Enable Azure Container Registry (ACR)](https://aka.ms/azureml/environment/acr-private-endpoint)\n* [How To Use Environments](https://aka.ms/azureml/environment/how-to-use-environments)\n\n## *Docker pull issues*\n### Failed to pull Docker image\n<!--issueDescription-->\nThis issue can happen when a Docker image pull fails during an image build.\n\n**Potential causes:**\n* The path name to the container registry is incorrect\n* A container registry behind a virtual network is using a private endpoint in an [unsupported region](https://aka.ms/azureml/environment/private-link-availability)\n* The image you're trying to reference doesn't exist in the container registry you specified \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-1072.md"
                }
            },
            {
                "page_content": "Azure Machine Learning uses an associated Key Vault instance to store the following credentials:\n* The associated storage account connection string\n* Passwords to Azure Container Repository instances\n* Connection strings to data stores\n\nAzure key vault can be configured to use either a private endpoint or service endpoint. To use Azure Machine Learning experimentation capabilities with Azure Key Vault behind a virtual network, use the following steps:\n\n> [!TIP]\n> Regardless of whether you use a private endpoint or service endpoint, the key vault must be in the same network as the private endpoint of the workspace.\n\n# [Private endpoint](#tab/pe)\n\nFor information on using a private endpoint with Azure Key Vault, see [Integrate Key Vault with Azure Private Link](../key-vault/general/private-link-service.md#establish-a-private-link-connection-to-key-vault-using-the-azure-portal).\n\n\n# [Service endpoint](#tab/se)\n\n1. Go to the Key Vault that's associated with the workspace.\n\n1. On the __Key Vault__ page, in the left pane, select __Networking__.\n\n1. On the __Firewalls and virtual networks__ tab, do the following actions:\n    1. Under __Allow access from__, select __Allow public access from specific virtual networks and IP addresses__.\n    1. Under __Virtual networks__, select __Add a virtual network__, __Add existing virtual networks__, and add the virtual network/subnet where your experimentation compute resides.\n    1. Verify that __Allow trusted Microsoft services to bypass this firewall__ is checked, and then select __Apply__.\n\n    :::image type=\"content\" source=\"./media/how-to-enable-virtual-network/key-vault-firewalls-and-virtual-networks-page.png\" alt-text=\"The Firewalls and virtual networks section in the Key Vault pane\":::\n\nFor more information, see [Configure Azure Key Vault network settings](../key-vault/general/how-to-azure-key-vault-network-security.md).\n\n\n## Enable Azure Container Registry (ACR)\n\n> [!TIP]\n> If you did not use an existing Azure Container Registry when creating the workspace, one may not exist. By default, the workspace will not create an ACR instance until it needs one. To force the creation of one, train or deploy a model using your workspace before using the steps in this section.\n\nAzure Container Registry can be configured to use a private endpoint. Use the following steps to configure your workspace to use ACR when it is in the virtual network:\n\n1. Find the name of the Azure Container Registry for your workspace, using one of the following methods:\n\n    # [Azure CLI](#tab/cli)\n\n    [!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n    If you've [installed the Machine Learning extension v2 for Azure CLI](how-to-configure-cli.md), you can use the `az ml workspace show` command to show the workspace information. The v1 extension does not return this information.\n\n    ```azurecli-interactive\n    az ml workspace show -n yourworkspacename -g resourcegroupname --query 'container_registry'\n    ```\n\n    This command returns a value similar to `\"/subscriptions/{GUID}/resourceGroups/{resourcegroupname}/providers/Microsoft.ContainerRegistry/registries/{ACRname}\"`. The last part of the string is the name of the Azure Container Registry for the workspace.\n\n    # [Python SDK](#tab/python)\n\n    [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n    The following code snippet demonstrates how to get the container registry information using the [Azure Machine Learning SDK](/python/api/overview/azure/ai-ml-readme):\n\n   ```python\n    # import required libraries\n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n\n    subscription_id = \"<your subscription ID>\"\n    resource_group = \"<your resource group name>\"\n    workspace = \"<your workspace name>\"\n\n    ml_client = MLClient(\n        DefaultAzureCredential(), subscription_id, resource_group, workspace\n    )\n    \n    # Get workspace info\n    ws=ml_client.workspaces.get(name=workspace)\n    print(ws.container_registry)\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-secure-workspace-vnet-160.md"
                }
            },
            {
                "page_content": "\n# Use network isolation with managed online endpoints\n\n[!INCLUDE [SDK/CLI v2](../../includes/machine-learning-dev-v2.md)]\n\nWhen deploying a machine learning model to a managed online endpoint, you can secure communication with the online endpoint by using [private endpoints](../private-link/private-endpoint-overview.md).\n\nYou can secure the inbound scoring requests from clients to an _online endpoint_. You can also secure the outbound communications between a _deployment_ and the Azure resources it uses. Security for inbound and outbound communication are configured separately. For more information on endpoints and deployments, see [What are endpoints and deployments](concept-endpoints.md#what-are-endpoints-and-deployments).\n\nThe following diagram shows how communications flow through private endpoints to the managed online endpoint. Incoming scoring requests from clients are received through the workspace private endpoint from your virtual network. Outbound communication with services is handled through private endpoints to those service instances from the deployment:\n\n:::image type=\"content\" source=\"./media/how-to-secure-online-endpoint/endpoint-network-isolation-ingress-egress.png\" alt-text=\"Diagram of overall ingress/egress communication.\":::\n\n## Prerequisites\n\n* To use Azure machine learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/) today.\n\n* You must install and configure the Azure CLI and `ml` extension or the AzureML Python SDK v2. For more information, see the following articles:\n\n    * [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n    * [Install the Python SDK v2](https://aka.ms/sdk-v2-install).\n\n* You must have an Azure Resource Group, in which you (or the service principal you use) need to have `Contributor` access. You'll have such a resource group if you configured your `ml` extension per the above article.\n\n* You must have an Azure Machine Learning workspace, and the workspace must use a private endpoint. If you don't have one, the steps in this article create an example workspace, VNet, and VM. For more information, see [Configure a private endpoint for Azure Machine Learning workspace](./how-to-configure-private-link.md).\n\n    The workspace configuration can either allow or disallow public network access. If you plan on using managed online endpoint deployments that use __public outbound__, then you must also [configure the workspace to allow public access](how-to-configure-private-link.md#enable-public-access).\n\n    Outbound communication from managed online endpoint deployment is to the _workspace API_. When the endpoint is configured to use __public outbound__, then the workspace must be able to accept that public communication (allow public access).\n\n* When the workspace is configured with a private endpoint, the Azure Container Registry for the workspace must be configured for __Premium__ tier. For more information, see [Azure Container Registry service tiers](../container-registry/container-registry-skus.md).\n\n* The Azure Container Registry and Azure Storage Account must be in the same Azure Resource Group as the workspace.\n\n* If you want to use a [user-assigned managed identity](../active-directory/managed-identities-azure-resources/how-manage-user-assigned-managed-identities.md?pivots=identity-mi-methods-azp) to create and manage online endpoints and online deployments, the identity should have the proper permissions. For details about the required permissions, see [Set up service authentication](./how-to-identity-based-service-authentication.md#workspace). For example, you need to assign the proper RBAC permission for Azure Key Vault on the identity.\n\n> [!IMPORTANT]\n> The end-to-end example in this article comes from the files in the __azureml-examples__ GitHub repository. To clone the samples repository and switch to the repository's `cli/` directory, use the following commands: \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-secure-online-endpoint-0.md"
                }
            }
        ]
    },
    {
        "query": "Generate a python code to create a workspace with Customer Managed Keys using AzureML SDK V2 ?",
        "result": "Here is an example Python code to create a workspace with Customer Managed Keys using AzureML SDK V2:\n\n```python\nfrom azureml.core import Workspace\nfrom azureml.core.authentication import InteractiveLoginAuthentication\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.customer_managed_key import CustomerManagedKey\n\n# Define the workspace parameters\nsubscription_id = '<subscription_id>'\nresource_group = '<resource_group>'\nworkspace_name = '<workspace_name>'\nlocation = '<location>'\n\n# Define the customer-managed key parameters\nkey_vault = '<key_vault>'\nkey_name = '<key_name>'\nkey_version = '<key_version>'\n\n# Authenticate using interactive login\ninteractive_auth = InteractiveLoginAuthentication()\n\n# Create the workspace object with customer-managed key\nws = Workspace.create(name=workspace_name,\n                      subscription_id=subscription_id,\n                      resource_group=resource_group,\n                      location=location,\n                      auth=interactive_auth,\n                      customer_managed_key=CustomerManagedKey(key_vault=key_vault,\n                                                              key_name=key_name,\n                                                              key_version=key_version))\n\n# Save the workspace configuration\nws.write_config()\n```\n\nMake sure to replace the placeholders with your own values.",
        "context": [
            {
                "page_content": "1. When you're finished configuring networking, you can select __Review + Create__, or advance to the optional __Advanced__ configuration.\n\n\n### Advanced\n\nBy default, metadata for the workspace is stored in an Azure Cosmos DB instance that Microsoft maintains. This data is encrypted using Microsoft-managed keys.\n\nTo limit the data that Microsoft collects on your workspace, select __High business impact workspace__ in the portal, or set `hbi_workspace=true ` in Python. For more information on this setting, see [Encryption at rest](concept-data-encryption.md#encryption-at-rest).\n\n> [!IMPORTANT]\t\n> Selecting high business impact can only be done when creating a workspace. You cannot change this setting after workspace creation.\t\n\n#### Use your own data encryption key\n\nYou can provide your own key for data encryption. Doing so creates the Azure Cosmos DB instance that stores metadata in your Azure subscription. For more information, see [Customer-managed keys](concept-customer-managed-keys.md).\n\n\nUse the following steps to provide your own key:\n\n> [!IMPORTANT]\t\n> Before following these steps, you must first perform the following actions:\t\n>\n> Follow the steps in [Configure customer-managed keys](how-to-setup-customer-managed-keys.md) to:\n> * Register the Azure Cosmos DB provider\n> * Create and configure an Azure Key Vault\n> * Generate a key\n\n# [Python SDK](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\n\nfrom azure.ai.ml.entities import Workspace, CustomerManagedKey\n\n# specify the workspace details\nws = Workspace(\n    name=\"my_workspace\",\n    location=\"eastus\",\n    display_name=\"My workspace\",\n    description=\"This example shows how to create a workspace\",\n    customer_managed_key=CustomerManagedKey(\n        key_vault=\"/subscriptions/<SUBSCRIPTION_ID>/resourcegroups/<RESOURCE_GROUP>/providers/microsoft.keyvault/vaults/<VAULT_NAME>\"\n        key_uri=\"<KEY-IDENTIFIER>\"\n    )\n    tags=dict(purpose=\"demo\")\n)\n\nml_client.workspaces.begin_create(ws)\n```\n\n# [Portal](#tab/azure-portal)\n\n1. Select __Customer-managed keys__, and then select __Click to select key__.\n\n    :::image type=\"content\" source=\"media/how-to-manage-workspace/advanced-workspace.png\" alt-text=\"Customer-managed keys\":::\n\n1. On the __Select key from Azure Key Vault__ form, select an existing Azure Key Vault, a key that it contains, and the version of the key. This key is used to encrypt the data stored in Azure Cosmos DB. Finally, use the __Select__ button to use this key.\n\n   :::image type=\"content\" source=\"media/how-to-manage-workspace/select-key-vault.png\" alt-text=\"Select the key\":::\n\n\n### Download a configuration file\n\nIf you'll be running your code on a [compute instance](quickstart-create-resources.md), skip this step.  The compute instance will create and store copy of this file for you.\n\nIf you plan to use code on your local environment that references this workspace, download the file:\n1. Select your workspace in [Azure studio](https://ml.azure.com)\n1. At the top right, select the workspace name, then select  **Download config.json**\n\n   ![Download config.json](./media/how-to-manage-workspace/configure.png)\n\nPlace the file into  the directory structure with your Python scripts or Jupyter Notebooks. It can be in the same directory, a subdirectory named *.azureml*, or in a parent directory. When you create a compute instance, this file is added to the correct directory on the VM for you.\n\n## Connect to a workspace\n\nWhen running machine learning tasks using the SDK, you require a MLClient object that specifies the connection to your workspace. You can create an `MLClient` object from parameters, or with a configuration file.\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n* **With a configuration file:** This code will read the contents of the configuration file to find your workspace.  You'll get a prompt to sign in if you aren't already authenticated.\n\n    ```python\n    from azure.ai.ml import MLClient\n    \n    # read the config from the current directory\n    ws_from_config = MLClient.from_config()\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-workspace-231.md"
                }
            },
            {
                "page_content": "> [!IMPORTANT]\n> If your subscription does not have enough quota for these services, a failure will occur.\n\n> [!WARNING]\n> __Don't delete the resource group__ that contains this Azure Cosmos DB instance, or any of the resources automatically created in this group. If you need to delete the resource group or Microsoft-managed services in it, you must delete the Azure Machine Learning workspace that uses it. The resource group resources are deleted when the associated workspace is deleted.\n\nFor more information on customer-managed keys with Azure Cosmos DB, see [Configure customer-managed keys for your Azure Cosmos DB account](../cosmos-db/how-to-setup-cmk.md).\n\n### Azure Container Instance\n\n> [!IMPORTANT]\n> Deploying to Azure Container Instances is not available in SDK or CLI v2. Only through SDK & CLI v1.\n\nWhen __deploying__ a trained model to an Azure Container instance (ACI), you can encrypt the deployed resource using a customer-managed key. For information on generating a key, see [Encrypt data with a customer-managed key](../container-instances/container-instances-encrypt-data.md#generate-a-new-key).\n\nTo use the key when deploying a model to Azure Container Instance, create a new deployment configuration using `AciWebservice.deploy_configuration()`. Provide the key information using the following parameters:\n\n* `cmk_vault_base_url`: The URL of the key vault that contains the key.\n* `cmk_key_name`: The name of the key.\n* `cmk_key_version`: The version of the key.\n\nFor more information on creating and using a deployment configuration, see the following articles:\n\n* [AciWebservice.deploy_configuration()](/python/api/azureml-core/azureml.core.webservice.aci.aciwebservice#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none--dns-name-label-none--primary-key-none--secondary-key-none--collect-model-data-none--cmk-vault-base-url-none--cmk-key-name-none--cmk-key-version-none-) reference\n* [Where and how to deploy](how-to-deploy-online-endpoints.md)\n* [Deploy a model to Azure Container Instances (SDK/CLI v1)](v1/how-to-deploy-azure-container-instance.md)\n\n    For more information on using a customer-managed key with ACI, see [Encrypt deployment data](../container-instances/container-instances-encrypt-data.md).\n\n### Azure Kubernetes Service\n\nYou may encrypt a deployed Azure Kubernetes Service resource using customer-managed keys at any time. For more information, see [Bring your own keys with Azure Kubernetes Service](../aks/azure-disk-customer-managed-keys.md). \n\nThis process allows you to encrypt both the Data and the OS Disk of the deployed virtual machines in the Kubernetes cluster.\n\n> [!IMPORTANT]\n> This process only works with AKS K8s version 1.17 or higher.\n\n## Next steps\n\n* [Customer-managed keys with Azure Machine Learning](concept-customer-managed-keys.md)\n* [Create a workspace with Azure CLI](how-to-manage-workspace-cli.md#customer-managed-key-and-high-business-impact-workspace) |\n* [Create and manage a workspace](how-to-manage-workspace.md#use-your-own-data-encryption-key) |\n* [Create a workspace with a template](how-to-create-workspace-template.md#deploy-an-encrypted-workspace) |\n* [Create, run, and delete Azure ML resources with REST](how-to-manage-rest.md#create-a-workspace-using-customer-managed-encryption-keys) |\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-setup-customer-managed-keys-112.md"
                }
            },
            {
                "page_content": "Azure Database for MySQL is a relational database service in the Microsoft cloud based on the MySQL Community Edition database engine. The Azure Database for MySQL service uses the FIPS 140-2 validated cryptographic module for storage encryption of data at-rest. \n\nTo encrypt data using customer managed keys, see [Azure Database for MySQL data encryption with a customer-managed key](../mysql/single-server/concepts-data-encryption-mysql.md) .\n\n\n### Azure Cosmos DB\n\nAzure Machine Learning stores metadata in an Azure Cosmos DB instance. This instance is associated with a Microsoft subscription managed by Azure Machine Learning. All the data stored in Azure Cosmos DB is encrypted at rest with Microsoft-managed keys.\n\nWhen using your own (customer-managed) keys to encrypt the Azure Cosmos DB instance, a Microsoft managed Azure Cosmos DB instance is created in your subscription. This instance is created in a Microsoft-managed resource group, which is different than the resource group for your workspace. For more information, see [Customer-managed keys](concept-customer-managed-keys.md).\n\n### Azure Container Registry\n\nAll container images in your registry (Azure Container Registry) are encrypted at rest. Azure automatically encrypts an image before storing it and decrypts it when Azure Machine Learning pulls the image.\n\nTo use customer-managed keys to encrypt your Azure Container Registry, you need to create your own ACR and attach it while provisioning the workspace. You can encrypt the default instance that gets created at the time of workspace provisioning.\n\n> [!IMPORTANT]\n> Azure Machine Learning requires the admin account be enabled on your Azure Container Registry. By default, this setting is disabled when you create a container registry. For information on enabling the admin account, see [Admin account](../container-registry/container-registry-authentication.md#admin-account).\n>\n> Once an Azure Container Registry has been created for a workspace, do not delete it. Doing so will break your Azure Machine Learning workspace.\n\nFor an example of creating a workspace using an existing Azure Container Registry, see the following articles:\n\n* [Create a workspace for Azure Machine Learning with Azure CLI](how-to-manage-workspace-cli.md).\n* [Create a workspace with Python SDK](how-to-manage-workspace.md?tabs=python#create-a-workspace).\n* [Use an Azure Resource Manager template to create a workspace for Azure Machine Learning](how-to-create-workspace-template.md)\n\n### Azure Container Instance\n\n> [!IMPORTANT]\n> Deployments to ACI rely on the Azure Machine Learning Python SDK and CLI v1.\n\nYou may encrypt a deployed Azure Container Instance (ACI) resource using customer-managed keys. The customer-managed key used for ACI can be stored in the Azure Key Vault for your workspace. For information on generating a key, see [Encrypt data with a customer-managed key](../container-instances/container-instances-encrypt-data.md#generate-a-new-key).\n\n[!INCLUDE [sdk v1](../../includes/machine-learning-sdk-v1.md)]\n\nTo use the key when deploying a model to Azure Container Instance, create a new deployment configuration using `AciWebservice.deploy_configuration()`. Provide the key information using the following parameters:\n\n* `cmk_vault_base_url`: The URL of the key vault that contains the key.\n* `cmk_key_name`: The name of the key.\n* `cmk_key_version`: The version of the key.\n\nFor more information on creating and using a deployment configuration, see the following articles:\n\n* [AciWebservice.deploy_configuration()](/python/api/azureml-core/azureml.core.webservice.aci.aciwebservice#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none--dns-name-label-none--primary-key-none--secondary-key-none--collect-model-data-none--cmk-vault-base-url-none--cmk-key-name-none--cmk-key-version-none-) reference\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-data-encryption-51.md"
                }
            }
        ]
    },
    {
        "query": "How to Archive an existing environment using AzureML SDK V2?",
        "result": "To archive an existing environment using AzureML SDK V2, you can use the `ml_client.environments.archive()` method. If you want to archive all versions of an environment, you can simply pass the name of the environment to the method. If you want to archive a specific version of an environment, you can pass both the name and version of the environment to the method. Once an environment is archived, it will be hidden by default from list queries, but you can still continue to reference and use it in your workflows.",
        "context": [
            {
                "page_content": "\n### Update\n\nUpdate mutable properties of a specific environment:\n\n# [Azure CLI](#tab/cli)\n\n```cli\naz ml environment update --name docker-image-example --version 1 --set description=\"This is an updated description.\"\n```\n\n# [Python SDK](#tab/python)\n\n```python\nenv.description=\"This is an updated description.\"\nml_client.environments.create_or_update(environment=env)\n```\n\n> [!IMPORTANT]\n> For environments, only `description` and `tags` can be updated. All other properties are immutable; if you need to change any of those properties you should create a new version of the environment.\n\n### Archive\n\nArchiving an environment will hide it by default from list queries (`az ml environment list`). You can still continue to reference and use an archived environment in your workflows. You can archive either all versions of an environment or only a specific version.\n\nIf you don't specify a version, all versions of the environment under that given name will be archived. If you create a new environment version under an archived environment container, that new version will automatically be set as archived as well.\n\nArchive all versions of an environment:\n\n# [Azure CLI](#tab/cli)\n\n```cli\naz ml environment archive --name docker-image-example\n```\n\n# [Python SDK](#tab/python)\n\n```python\nml_client.environments.archive(name=\"docker-image-example\")\n```\n\n            \nArchive a specific environment version:\n\n# [Azure CLI](#tab/cli)\n\n```cli\naz ml environment archive --name docker-image-example --version 1\n```\n\n# [Python SDK](#tab/python)\n\n```python\nml_client.environments.archive(name=\"docker-image-example\", version=\"1\")\n```\n\n\n\n## Use environments for training\n\n# [Azure CLI](#tab/cli)\n\nTo use an environment for a training job, specify the `environment` field of the job YAML configuration. You can either reference an existing registered Azure ML environment via `environment: azureml:<environment-name>:<environment-version>` or `environment: azureml:<environment-name>@latest` (to reference the latest version of an environment), or define an environment specification inline. If defining an environment inline, don't specify the `name` and `version` fields, as these environments are treated as \"unregistered\" environments and aren't tracked in your environment asset registry.\n\n# [Python SDK](#tab/python)\n\nTo use an environment for a training job, specify the `environment` property of the [command](/python/api/azure-ai-ml/azure.ai.ml#azure-ai-ml-command).\n\nFor examples of submitting jobs, see the examples at [https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs).\n\nWhen you submit a training job, the building of a new environment can take several minutes. The duration depends on the size of the required dependencies. The environments are cached by the service. So as long as the environment definition remains unchanged, you incur the full setup time only once.\n\n\nFor more information on how to use environments in jobs, see [Train models](how-to-train-model.md).\n\n## Use environments for model deployments\n\n# [Azure CLI](#tab/cli)\n\nYou can also use environments for your model deployments for both online and batch scoring. To do so, specify the `environment` field in the deployment YAML configuration.\n\nFor more information on how to use environments in deployments, see [Deploy and score a machine learning model by using an online endpoint](how-to-deploy-online-endpoints.md).\n\n# [Python SDK](#tab/python)\n\nYou can also use environments for your model deployments. For more information, see [Deploy and score a machine learning model](how-to-deploy-managed-online-endpoint-sdk-v2.md).\n\n\n## Next steps\n\n- [Train models (create jobs)](how-to-train-model.md)\n- [Deploy and score a machine learning model by using an online endpoint](how-to-deploy-online-endpoints.md)\n- [Environment YAML schema reference](reference-yaml-environment.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-environments-v2-266.md"
                }
            },
            {
                "page_content": "\n# Train scikit-learn models at scale with Azure Machine Learning\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n> [!div class=\"op_single_selector\" title1=\"Select the Azure Machine Learning SDK version you are using:\"]\n> * [v1](v1/how-to-train-scikit-learn.md)\n> * [v2 (current version)](how-to-train-scikit-learn.md)\n\nIn this article, learn how to run your scikit-learn training scripts with Azure Machine Learning Python SDK v2.\n\nThe example scripts in this article are used to classify iris flower images to build a machine learning model based on scikit-learn's [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris).\n\nWhether you're training a machine learning scikit-learn model from the ground-up or you're bringing an existing model into the cloud, you can use Azure Machine Learning to scale out open-source training jobs using elastic cloud compute resources. You can build, deploy, version, and monitor production-grade models with Azure Machine Learning.\n\n## Prerequisites\n\nYou can run the code for this article in either an Azure Machine Learning compute instance, or your own Jupyter Notebook.\n\n - Azure Machine Learning compute instance\n    - Complete the [Quickstart: Get started with Azure Machine Learning](quickstart-create-resources.md) to create a compute instance. Every compute instance includes a dedicated notebook server pre-loaded with the SDK and the notebooks sample repository.\n    - Select the notebook tab in the Azure Machine Learning studio. In the samples training folder, find a completed and expanded notebook by navigating to this directory: **v2  > sdk > jobs > single-step > scikit-learn > train-hyperparameter-tune-deploy-with-sklearn**.\n    - You can use the pre-populated code in the sample training folder to complete this tutorial.\n\n - Your Jupyter notebook server.\n    - [Install the Azure Machine Learning SDK (v2)](https://aka.ms/sdk-v2-install).\n\n\n## Set up the job\n\nThis section sets up the job for training by loading the required Python packages, connecting to a workspace, creating a compute resource to run a command job, and creating an environment to run the job.\n\n### Connect to the workspace\n\nFirst, you'll need to connect to your AzureML workspace. The [AzureML workspace](concept-workspace.md) is the top-level resource for the service. It provides you with a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n\nWe're using `DefaultAzureCredential` to get access to the workspace. This credential should be capable of handling most Azure SDK authentication scenarios.\n\nIf `DefaultAzureCredential` does not work for you, see [`azure-identity reference documentation`](/python/api/azure-identity/azure.identity) or [`Set up authentication`](how-to-setup-authentication.md?tabs=sdk) for more available credentials.\n\n```python\n# Handle to the workspace\nfrom azure.ai.ml import MLClient\n\n# Authentication package\nfrom azure.identity import DefaultAzureCredential\n\ncredential = DefaultAzureCredential()\n```\n\nIf you prefer to use a browser to sign in and authenticate, you should remove the comments in the following code and use it instead.\n\n```python\n# Handle to the workspace\n# from azure.ai.ml import MLClient\n\n# Authentication package\n# from azure.identity import InteractiveBrowserCredential\n# credential = InteractiveBrowserCredential()\n```\n\nNext, get a handle to the workspace by providing your Subscription ID, Resource Group name, and workspace name. To find these parameters:\n\n1. Look in the upper-right corner of the Azure Machine Learning studio toolbar for your workspace name.\n2. Select your workspace name to show your Resource Group and Subscription ID.\n3. Copy the values for Resource Group and Subscription ID into the code.\n\n```python\n# Get a handle to the workspace\nml_client = MLClient(\n    credential=credential,\n    subscription_id=\"<SUBSCRIPTION_ID>\",\n    resource_group_name=\"<RESOURCE_GROUP>\",\n    workspace_name=\"<AML_WORKSPACE_NAME>\",\n)\n```\n\nThe result of running this script is a workspace handle that you'll use to manage other resources and jobs.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-train-scikit-learn-0.md"
                }
            },
            {
                "page_content": "\n# Azure Machine Learning CLI (v2) release notes\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n\n\nIn this article, learn about Azure Machine Learning CLI (v2) releases.\n\n__RSS feed__: Get notified when this page is updated by copying and pasting the following URL into your feed reader:\n`https://learn.microsoft.com/api/search/rss?search=%22Azure+machine+learning+release+notes-v2%22&locale=en-us`\n\n## 2022-11-08\n\n### Azure Machine Learning CLI (v2) v2.11.0\n\n- The CLI is depending on azure-ai-ml 1.1.0.\n- `az ml registry`\n  - Added `ml registry delete` command.\n  - Adjusted registry experimental tags and imports to avoid warning printouts for unrelated operations.\n- `az ml environment`\n   - Prevented registering an already existing environment that references conda file.\n\n## 2022-10-10\n\n### Azure Machine Learning CLI (v2) v2.10.0\n\n- The CLI is depending on GA version of azure-ai-ml.\n- Dropped support for Python 3.6.\n- `az ml registry`\n  - New command group added to manage ML asset registries.\n- `az ml job`\n  - Added `az ml job show-services` command.\n  - Added model sweeping and hyperparameter tuning to AutoML NLP jobs.\n- `az ml schedule`\n  - Added `month_days` property in recurrence schedule.\n- `az ml compute`\n  - Added custom setup scripts support for compute instances.\n\n## 2022-09-22\n\n### Azure Machine Learning CLI (v2) v2.8.0\n\n- `az ml job`\n  - Added spark job support.\n  - Added shm_size and docker_args to job.\n- `az ml compute`\n  - Compute instance supports managed identity.\n  - Added idle shutdown time support for compute instance.\n- `az ml online-deployment`\n  - Added support for data collection for eventhub and data storage.\n  - Added syntax validation for scoring script.\n- `az ml batch-deployment`\n  - Added syntax validation for scoring script.\n\n## 2022-08-10\n\n### Azure Machine Learning CLI (v2) v2.7.0\n\n- `az ml component`\n  - Added AutoML component.\n- `az ml dataset`\n  - Deprecated command group (Use `az ml data` instead).\n\n## 2022-07-16\n\n### Azure Machine Learning CLI (v2) v2.6.0\n\n- Added MoonCake cloud support.\n- `az ml job`\n  - Allow Git repo URLs to be used as code.\n  - AutoML jobs use the same input schema as other job types.\n  - Pipeline jobs now supports registry assets.\n- `az ml component`\n  - Allow Git repo URLs to be used as code.\n- `az ml online-endpoint`\n  - MIR now supports registry assets.\n\n## 2022-05-24\n\n### Azure Machine Learning CLI (v2) v2.4.0\n\n- The Azure Machine Learning CLI (v2) is now GA.\n- `az ml job`\n  - The command group is marked as GA.\n  - Added AutoML job type in public preview.\n  - Added `schedules` property to pipeline job in public preview.\n  - Added an option to list only archived jobs.\n  - Improved reliability of `az ml job download` command.\n- `az ml data`\n  - The command group is marked as GA.\n  - Added MLTable data type in public preview.\n  - Added an option to list only archived data assets.\n- `az ml environment`\n  - Added an option to list only archived environments.\n- `az ml model`\n  - The command group is marked as GA.\n  - Allow models to be created from job outputs.\n  - Added an option to list only archived models.\n- `az ml online-deployment`\n  - The command group is marked as GA.\n  - Removed timeout waiting for deployment creation.\n  - Improved online deployment list view.\n- `az ml online-endpoint`\n  - The command group is marked as GA.\n  - Added `mirror_traffic` property to online endpoints in public preview.\n  - Improved online endpoint list view.\n- `az ml batch-deployment`\n  - The command group is marked as GA.\n  - Added support for `uri_file` and `uri_folder` as invocation input.\n  - Fixed a bug in batch deployment update.\n  - Fixed a bug in batch deployment list-jobs output.\n- `az ml batch-endpoint`\n  - The command group is marked as GA.\n  - Added support for `uri_file` and `uri_folder` as invocation input.\n  - Fixed a bug in batch endpoint update.\n  - Fixed a bug in batch endpoint list-jobs output.\n- `az ml component`\n  - The command group is marked as GA.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/azure-machine-learning-release-notes-cli-v2-0.md"
                }
            }
        ]
    },
    {
        "query": "How to assign managed identity to an a workspace in AzureML ?",
        "result": "You can assign a system- or user-assigned managed identity to a compute instance in AzureML to authenticate against other Azure resources such as storage. You can create a compute instance with managed identity from Azure ML Studio or use V2 CLI with a yaml file. Once the managed identity is created, grant the managed identity at least Storage Blob Data Reader role on the storage account of the datastore. Then, when you work on the compute instance, the managed identity is used automatically to authenticate against datastores.",
        "context": [
            {
                "page_content": "\n## Assign managed identity\n\nYou can assign a system- or user-assigned [managed identity](../active-directory/managed-identities-azure-resources/overview.md) to a compute instance, to authenticate against other Azure resources such as storage. Using managed identities for authentication helps improve workspace security and management. For example, you can allow users to access training data only when logged in to a compute instance. Or use a common user-assigned managed identity to permit access to a specific storage account. \n\nYou can create compute instance with managed identity from Azure ML Studio:\n\n1.\tFill out the form to [create a new compute instance](?tabs=azure-studio#create).\n1.\tSelect **Next: Advanced Settings**.\n1.\tEnable **Assign a managed identity**.\n1.  Select **System-assigned** or **User-assigned** under **Identity type**.\n1.  If you selected **User-assigned**, select subscription and name of the identity.\n\nYou can use V2 CLI to create compute instance with assign system-assigned managed identity:\n\n```azurecli\naz ml compute create --name myinstance --identity-type SystemAssigned --type ComputeInstance --resource-group my-resource-group --workspace-name my-workspace\n```\n\nYou can also use V2 CLI with yaml file, for example to create a compute instance with user-assigned managed identity:\n\n```azurecli\nazure ml compute create --file compute.yaml --resource-group my-resource-group --workspace-name my-workspace\n```\n\nThe identity definition is contained in compute.yaml file:\n\n```yaml\nhttps://azuremlschemas.azureedge.net/latest/computeInstance.schema.json\nname: myinstance\ntype: computeinstance\nidentity:\n  type: user_assigned\n  user_assigned_identities: \n    - resource_id: identity_resource_id\n```\n\nOnce the managed identity is created, grant the managed identity at least Storage Blob Data Reader role on the storage account of the datastore, see [Accessing storage services](how-to-identity-based-service-authentication.md?tabs=cli#accessing-storage-services). Then, when you work on the compute instance, the managed identity is used automatically to authenticate against datastores.\n\n> [!NOTE]\n> The name of the created system managed identity will be in the format /workspace-name/computes/compute-instance-name in your Azure Active Directory. \n\nYou can also use the managed identity manually to authenticate against other Azure resources. The following example shows how to use it to get an Azure Resource Manager access token:\n\n```python\nimport requests\n\ndef get_access_token_msi(resource):\n    client_id = os.environ.get(\"DEFAULT_IDENTITY_CLIENT_ID\", None)\n    resp = requests.get(f\"{os.environ['MSI_ENDPOINT']}?resource={resource}&clientid={client_id}&api-version=2017-09-01\", headers={'Secret': os.environ[\"MSI_SECRET\"]})\n    resp.raise_for_status()\n    return resp.json()[\"access_token\"]\n\narm_access_token = get_access_token_msi(\"https://management.azure.com\")\n```\n\nTo use Azure CLI with the managed identity for authentication, specify the identity client ID as the username when logging in: \n```azurecli\naz login --identity --username $DEFAULT_IDENTITY_CLIENT_ID\n```\n\n> [!NOTE]\n> You cannot use ```azcopy``` when trying to use managed identity. ```azcopy login --identity``` will not work.\n\n## Add custom applications such as RStudio or Posit Workbench (preview)\n\n> [!IMPORTANT]\n> Items marked (preview) below are currently in public preview.\n> The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.\n> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\n\nYou can set up other applications, such as RStudio, or Posit Workbench (formerly RStudio Workbench), when creating a compute instance. Follow these steps in studio to set up a custom application on your compute instance\n\n1.\tFill out the form to [create a new compute instance](?tabs=azure-studio#create)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-534.md"
                }
            },
            {
                "page_content": "::: code language=\"azurecli\" source=\"~/azureml-examples-main/cli/deploy-managed-online-endpoint-access-resource-uai.sh\" id=\"get_user_identity_id\" :::\n\nGet the container registry associated with workspace.\n\n::: code language=\"azurecli\" source=\"~/azureml-examples-main/cli/deploy-managed-online-endpoint-access-resource-uai.sh\" id=\"get_container_registry_id\" :::\n\nRetrieve the default storage of the workspace.\n\n::: code language=\"azurecli\" source=\"~/azureml-examples-main/cli/deploy-managed-online-endpoint-access-resource-uai.sh\" id=\"get_workspace_storage_id\" :::\n\nGive permission of storage account to the user-assigned managed identity.  \n\n::: code language=\"azurecli\" source=\"~/azureml-examples-main/cli/deploy-managed-online-endpoint-access-resource-uai.sh\" id=\"give_permission_to_user_storage_account\" :::\n\nGive permission of container registry to user assigned managed identity.\n\n::: code language=\"azurecli\" source=\"~/azureml-examples-main/cli/deploy-managed-online-endpoint-access-resource-uai.sh\" id=\"give_permission_to_container_registry\" :::\n\nGive permission of default workspace storage to user-assigned managed identity.\n\n::: code language=\"azurecli\" source=\"~/azureml-examples-main/cli/deploy-managed-online-endpoint-access-resource-uai.sh\" id=\"give_permission_to_workspace_storage_account\" :::\n\n# [System-assigned (Python)](#tab/system-identity-python)\n\nFirst, make an `AuthorizationManagementClient` to list Role Definitions: \n\n```python\nfrom azure.mgmt.authorization import AuthorizationManagementClient\nfrom azure.mgmt.authorization.v2018_01_01_preview.models import RoleDefinition\nimport uuid\n\nrole_definition_client = AuthorizationManagementClient(\n    credential=credential,\n    subscription_id=subscription_id,\n    api_version=\"2018-01-01-preview\",\n)\n```\n\nNow, initialize one to make Role Assignments: \n\n```python\nfrom azure.mgmt.authorization.v2020_10_01_preview.models import (\n    RoleAssignment,\n    RoleAssignmentCreateParameters,\n)\n\nrole_assignment_client = AuthorizationManagementClient(\n    credential=credential,\n    subscription_id=subscription_id,\n    api_version=\"2020-10-01-preview\",\n)\n```\n\n\nThen, get the Principal ID of the System-assigned managed identity: \n\n```python\nendpoint = ml_client.online_endpoints.get(endpoint_name)\nsystem_principal_id = endpoint.identity.principal_id\n```\n\nNext, give assign the `Storage Blob Data Reader` role to the endpoint. The Role Definition is retrieved by name and passed along with the Principal ID of the endpoint. The role is applied at the scope of the storage account created above and allows the endpoint to read the file. \n\n```python\nrole_name = \"Storage Blob Data Reader\"\nscope = storage_account.id\n\nrole_defs = role_definition_client.role_definitions.list(scope=scope)\nrole_def = next((r for r in role_defs if r.role_name == role_name))\n\nrole_assignment_client.role_assignments.create(\n    scope=scope,\n    role_assignment_name=str(uuid.uuid4()),\n    parameters=RoleAssignmentCreateParameters(\n        role_definition_id=role_def.id, principal_id=system_principal_id\n    ),\n)\n```\n\n\n# [User-assigned (Python)](#tab/user-identity-python)\n\nFirst, make an `AuthorizationManagementClient` to list Role Definitions: \n\n```python\nfrom azure.mgmt.authorization import AuthorizationManagementClient\nfrom azure.mgmt.authorization.v2018_01_01_preview.models import RoleDefinition\nimport uuid\n\nrole_definition_client = AuthorizationManagementClient(\n    credential=credential,\n    subscription_id=subscription_id,\n    api_version=\"2018-01-01-preview\",\n)\n```\n\nNow, initialize one to make Role Assignments: \n\n```python\nfrom azure.mgmt.authorization.v2020_10_01_preview.models import (\n    RoleAssignment,\n    RoleAssignmentCreateParameters,\n)\n\nrole_assignment_client = AuthorizationManagementClient(\n    credential=credential,\n    subscription_id=subscription_id,\n    api_version=\"2020-10-01-preview\",\n)\n```\n\nThen, get the Principal ID and Client ID of the User-assigned managed identity. To assign roles, we only need the Principal ID. However, we will use the Client ID to fill the `UAI_CLIENT_ID` placeholder environment variable before creating the deployment.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-access-resources-from-endpoints-managed-identities-778.md"
                }
            },
            {
                "page_content": "To attach a Synapse Compute using Python SDK, first create an instance of [azure.ai.ml.MLClient class](/python/api/azure-ai-ml/azure.ai.ml.mlclient). This provides convenient functions for interaction with Azure Machine Learning services. The following code sample uses `azure.identity.DefaultAzureCredential` for connecting to a workspace in resource group of a specified Azure subscription. In the following code sample, define the `SynapseSparkCompute` with the parameters:\n- `name` - user-defined name of the new attached Synapse Spark pool. \n- `resource_id` - resource ID of the Synapse Spark pool created earlier in the Azure Synapse Analytics workspace.\n\nAn [azure.ai.ml.MLClient.begin_create_or_update()](/python/api/azure-ai-ml/azure.ai.ml.mlclient#azure-ai-ml-mlclient-begin-create-or-update) function call attaches the defined Synapse Spark pool to the Azure Machine Learning workspace.\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import SynapseSparkCompute\nfrom azure.identity import DefaultAzureCredential\n\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace_name = \"<AML_WORKSPACE_NAME>\"\n\nml_client = MLClient(\n    DefaultAzureCredential(), subscription_id, resource_group, workspace_name\n)\n\nsynapse_name = \"<ATTACHED_SPARK_POOL_NAME>\"\nsynapse_resource = \"/subscriptions/<SUBSCRIPTION_ID>/resourceGroups/<RESOURCE_GROUP>/providers/Microsoft.Synapse/workspaces/<SYNAPSE_WORKSPACE_NAME>/bigDataPools/<SPARK_POOL_NAME>\"\n\nsynapse_comp = SynapseSparkCompute(name=synapse_name, resource_id=synapse_resource)\nml_client.begin_create_or_update(synapse_comp)\n```\n\nTo attach a Synapse Spark pool that uses system-assigned identity, pass [IdentityConfiguration](/python/api/azure-ai-ml/azure.ai.ml.entities.identityconfiguration), with type set to `SystemAssigned`, as the `identity` parameter of the `SynapseSparkCompute` class. This code snippet attaches a Synapse Spark pool that uses system-assigned identity.\n\n```python\n# import required libraries\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import SynapseSparkCompute, IdentityConfiguration\nfrom azure.identity import DefaultAzureCredential\n\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace_name = \"<AML_WORKSPACE_NAME>\"\n\nml_client = MLClient(\n    DefaultAzureCredential(), subscription_id, resource_group, workspace_name\n)\n\nsynapse_name = \"<ATTACHED_SPARK_POOL_NAME>\"\nsynapse_resource = \"/subscriptions/<SUBSCRIPTION_ID>/resourceGroups/<RESOURCE_GROUP>/providers/Microsoft.Synapse/workspaces/<SYNAPSE_WORKSPACE_NAME>/bigDataPools/<SPARK_POOL_NAME>\"\nsynapse_identity = IdentityConfiguration(type=\"SystemAssigned\")\n\nsynapse_comp = SynapseSparkCompute(\n    name=synapse_name, resource_id=synapse_resource, identity=synapse_identity\n)\nml_client.begin_create_or_update(synapse_comp)\n```\n\nA Synapse Spark pool can also use a user-assigned identity. For a user-assigned identity, you can pass a managed identity definition, using the [IdentityConfiguration](/python/api/azure-ai-ml/azure.ai.ml.entities.identityconfiguration) class, as the `identity` parameter of the `SynapseSparkCompute` class. For the managed identity definition used in this way, set the `type` to `UserAssigned`. In addition, pass a `user_assigned_identities` parameter. The parameter `user_assigned_identities` is a list of objects of the [UserAssignedIdentity](/python/api/azure-ai-ml/azure.ai.ml.entities.userassignedidentity) class. The `resource_id`of the user-assigned identity populates each `UserAssignedIdentity` class object. This code snippet attaches a Synapse Spark pool that uses a user-assigned identity:\n\n```python\n# import required libraries\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import (\n    SynapseSparkCompute,\n    IdentityConfiguration,\n    UserAssignedIdentity,\n)\nfrom azure.identity import DefaultAzureCredential\n\nsubscription_id = \"<SUBSCRIPTION_ID>\"\nresource_group = \"<RESOURCE_GROUP>\"\nworkspace_name = \"<AML_WORKSPACE_NAME>\"\n\nml_client = MLClient(\n    DefaultAzureCredential(), subscription_id, resource_group, workspace_name\n)\n\nsynapse_name = \"<ATTACHED_SPARK_POOL_NAME>\"\nsynapse_resource = \"/subscriptions/<SUBSCRIPTION_ID>/resourceGroups/<RESOURCE_GROUP>/providers/Microsoft.Synapse/workspaces/<SYNAPSE_WORKSPACE_NAME>/bigDataPools/<SPARK_POOL_NAME>\"\nsynapse_identity = IdentityConfiguration(\n    type=\"UserAssigned\",\n    user_assigned_identities=[\n        UserAssignedIdentity(\n            resource_id=\"/subscriptions/<SUBSCRIPTION_ID/resourceGroups/<RESOURCE_GROUP>/providers/Microsoft.ManagedIdentity/userAssignedIdentities/<AML_USER_MANAGED_ID>\"\n        )\n    ],\n)\n\nsynapse_comp = SynapseSparkCompute(\n    name=synapse_name, resource_id=synapse_resource, identity=synapse_identity\n)\nml_client.begin_create_or_update(synapse_comp)\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-synapse-spark-pool-261.md"
                }
            }
        ]
    },
    {
        "query": "How to enable create an object detection model using Azuomated ML in AzureML SDK V2 ?",
        "result": "To create an object detection model using Automated ML in AzureML SDK V2, you need to follow these steps:\n1. Download and transform data\n2. Train an automated machine learning object detection model\n3. Specify hyperparameter values for your model\n4. Perform a hyperparameter sweep\n5. Deploy your model\n6. Visualize detections\n\nYou can find detailed code examples and use cases in the GitHub notebook repository for automated machine learning samples.",
        "context": [
            {
                "page_content": "\n# Tutorial: Train an object detection model with AutoML and Python\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\n> [!div class=\"op_single_selector\" title1=\"Select the version of Azure Machine Learning you are using:\"]\n> * [v1](v1/tutorial-auto-train-image-models-v1.md)\n> * [v2 (current version)](tutorial-auto-train-image-models.md)\n\n\nIn this tutorial, you learn how to train an object detection model using Azure Machine Learning automated ML with the Azure Machine Learning CLI extension v2 or the Azure Machine Learning Python SDK v2.\nThis object detection model identifies whether the image contains objects, such as a can, carton, milk bottle, or water bottle.\n\nAutomated ML accepts training data and configuration settings, and automatically iterates through combinations of different feature normalization/standardization methods, models, and hyperparameter settings to arrive at the best model.\n\n\nYou'll write code using the Python SDK in this tutorial and learn the following tasks:\n\n> [!div class=\"checklist\"]\n> * Download and transform data\n> * Train an automated machine learning object detection model\n> * Specify hyperparameter values for your model\n> * Perform a hyperparameter sweep\n> * Deploy your model\n> * Visualize detections\n\n## Prerequisites\n\n* If you don\u2019t have an Azure subscription, create a free account before you begin. Try the [free or paid version](https://azure.microsoft.com/free/) of Azure Machine Learning today.\n\n* Python 3.6 or 3.7 are supported for this feature\n\n* Complete the [Quickstart: Get started with Azure Machine Learning](quickstart-create-resources.md#create-the-workspace) if you don't already have an Azure Machine Learning workspace.\n\n* Download and unzip the [**odFridgeObjects.zip*](https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip) data file. The dataset is annotated in Pascal VOC format, where each image corresponds to an xml file. Each xml file contains information on where its corresponding image file is located and also contains information about the bounding boxes and the object labels. In order to use this data, you first need to convert it to the required JSONL format as seen in the [Convert the downloaded data to JSONL](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb) section of the notebook. \n\n# [Azure CLI](#tab/cli)\n\n [!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nThis tutorial is also available in the [azureml-examples repository on GitHub](https://github.com/Azure/azureml-examples/tree/sdk-preview/cli/jobs/automl-standalone-jobs/cli-automl-image-object-detection-task-fridge-items). If you wish to run it in your own local environment, setup using the following instructions\n\n* Install and [set up CLI (v2)](how-to-configure-cli.md#prerequisites) and make sure you install the `ml` extension.\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n\nThis tutorial is also available in the [azureml-examples repository on GitHub](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items). If you wish to run it in your own local environment, setup using the following instructions\n\n* Use the following commands to install Azure ML Python SDK v2:\n   * Uninstall previous preview version:\n   ```python\n   pip uninstall azure-ai-ml\n   ```\n   * Install the Azure ML Python SDK v2:\n   ```python\n   pip install azure-ai-ml\n   ```\n\n    > [!NOTE]\n    > Only Python 3.6 and 3.7 are compatible with automated ML support for computer vision tasks. \n\n\n## Compute target setup\n\nYou first need to set up a compute target to use for your automated ML model training. Automated ML models for image tasks require GPU SKUs.\n\nThis tutorial uses the NCsv3-series (with V100 GPUs) as this type of compute target leverages multiple GPUs to speed up training. Additionally, you can set up multiple nodes to take advantage of parallelism when tuning hyperparameters for your model.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-auto-train-image-models-0.md"
                }
            },
            {
                "page_content": "\n# Train a small object detection model with AutoML\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n> [!div class=\"op_single_selector\" title1=\"Select the version of Azure Machine Learning CLI extension you are using:\"]\n> * [v1](v1/how-to-use-automl-small-object-detect-v1.md)\n> * [v2 (current version)](how-to-use-automl-small-object-detect.md)\n\n\nIn this article, you'll learn how to train an object detection model to detect small objects in high-resolution images with [automated ML](concept-automated-ml.md) in Azure Machine Learning.\n\nTypically, computer vision models for object detection work well for datasets with relatively large objects. However, due to memory and computational constraints, these models tend to under-perform when tasked to detect small objects in high-resolution images. Because high-resolution images are typically large, they are resized before input into the model, which limits their capability to detect smaller objects--relative to the initial image size.\n\nTo help with this problem, automated ML supports tiling as part of the computer vision capabilities. The tiling capability in automated ML is based on the concepts in [The Power of Tiling for Small Object Detection](https://openaccess.thecvf.com/content_CVPRW_2019/papers/UAVision/Unel_The_Power_of_Tiling_for_Small_Object_Detection_CVPRW_2019_paper.pdf).\n\nWhen tiling, each image is divided into a grid of tiles. Adjacent tiles overlap with each other in width and height dimensions. The tiles are cropped from the original as shown in the following image.\n\n:::image type=\"content\" source=\"./media/how-to-use-automl-small-object-detect/tiles-generation.png\" alt-text=\"Diagram that shows an image being divided into a grid of overlapping tiles.\":::\n\n## Prerequisites\n\n* An Azure Machine Learning workspace. To create the workspace, see [Create workspace resources](quickstart-create-resources.md).\n\n* This article assumes some familiarity with how to configure an [automated machine learning experiment for computer vision tasks](how-to-auto-train-image-models.md).\n\n## Supported models\n\nSmall object detection using tiling is supported for all models supported by Automated ML for images for object detection task.\n\n## Enable tiling during training\n\nTo enable tiling, you can set the `tile_grid_size` parameter to a value like '3x2'; where 3 is the number of tiles along the width dimension and 2 is the number of tiles along the height dimension. When this parameter is set to '3x2', each image is split into a grid of 3 x 2 tiles. Each tile overlaps with the adjacent tiles, so that any objects that fall on the tile border are included completely in one of the tiles. This overlap can be controlled by the `tile_overlap_ratio` parameter, which defaults to 25%.\n\nWhen tiling is enabled, the entire image and the tiles generated from it are passed through the model. These images and tiles are resized according to the `min_size` and `max_size` parameters before feeding to the model. The computation time increases proportionally because of processing this extra data.\n\nFor example, when the `tile_grid_size` parameter is '3x2', the computation time would be approximately seven times higher than without tiling.\n\nYou can specify the value for `tile_grid_size` in your training parameters as a string.\n\n# [CLI v2](#tab/CLI-v2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\ntraining_parameters:\n  tile_grid_size: '3x2'\n```\n\n# [Python SDK v2](#tab/SDK-v2)\n\n```python\nimage_object_detection_job.set_training_parameters(\n\ttile_grid_size='3x2'\n)\n```\n\nThe value for `tile_grid_size` parameter depends on the image dimensions and size of objects within the image. For example, larger number of tiles would be helpful when there are smaller objects in the images.\n\nTo choose the optimal value for this parameter for your dataset, you can use hyperparameter search. To do so, you can specify a choice of values for this parameter in your hyperparameter space.\n\n# [CLI v2](#tab/CLI-v2)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-automl-small-object-detect-0.md"
                }
            },
            {
                "page_content": "By default, each model trains on a single VM. If training a model is taking too much time, using VMs that contain multiple GPUs may help. The time to train a model on large datasets should decrease in roughly linear proportion to the number of GPUs used. (For instance, a model should train roughly twice as fast on a VM with two GPUs as on a VM with one GPU.) If the time to train a model is still high on a VM with multiple GPUs, you can increase the number of VMs used to train each model. Similar to multi-GPU training, the time to train a model on large datasets should also decrease in roughly linear proportion to the number of VMs used. When training a model across multiple VMs, be sure to use a compute SKU that supports [InfiniBand](how-to-train-distributed-gpu.md#accelerating-distributed-gpu-training-with-infiniband) for best results. You can configure the number of VMs used to train a single model by setting the `node_count_per_trial` property of the AutoML job.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nproperties:\n  node_count_per_trial: \"2\"\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nMulti-node training is supported for all tasks. The `node_count_per_trial` property can be specified using the task-specific `automl` functions. For instance, for object detection:\n\n```python\nfrom azure.ai.ml import automl\n\nimage_object_detection_job = automl.image_object_detection(\n    ...,\n    properties={\"node_count_per_trial\": 2}\n)\n```\n\n### Streaming image files from storage\n\nBy default, all image files are downloaded to disk prior to model training. If the size of the image files is greater than available disk space, the job will fail. Instead of downloading all images to disk, you can select to stream image files from Azure storage as they're needed during training. Image files are streamed from Azure storage directly to system memory, bypassing disk. At the same time, as many files as possible from storage are cached on disk to minimize the number of requests to storage.\n\n> [!NOTE]\n> If streaming is enabled, ensure the Azure storage account is located in the same region as compute to minimize cost and latency.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\ntraining_parameters:\n  advanced_settings: >\n    {\"stream_image_files\": true}\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nfrom azure.ai.ml import automl\n\nimage_object_detection_job = automl.image_object_detection(...)\n\nimage_object_detection_job.set_training_parameters(\n    ...,\n    advanced_settings='{\"stream_image_files\": true}'\n)\n```\n\n\n## Example notebooks\nReview detailed code examples and use cases in the [GitHub notebook repository for automated machine learning samples](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/automl-standalone-jobs). Please check the folders with 'automl-image-' prefix for samples specific to building computer vision models.\n\n\n## Code examples\n\n# [Azure CLI](#tab/cli)\n\nReview detailed code examples and use cases in the [azureml-examples repository for automated machine learning samples](https://github.com/Azure/azureml-examples/tree/sdk-preview/cli/jobs/automl-standalone-jobs). \n\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nReview detailed code examples and use cases in the [GitHub notebook repository for automated machine learning samples](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/automl-standalone-jobs). \n\n## Next steps\n\n* [Tutorial: Train an object detection model with AutoML and Python](tutorial-auto-train-image-models.md).\n* [Troubleshoot automated ML experiments](how-to-troubleshoot-auto-ml.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-image-models-1042.md"
                }
            }
        ]
    },
    {
        "query": "Generate a Python code to show how to run an NLP job using Azuomated ML in AzureML SDK V2 ?",
        "result": "Here is an example of how to run an NLP job using Automated ML in AzureML SDK V2:\n\n```python\nfrom azureml.core import Workspace, Experiment\nfrom azureml.train.automl import AutoMLConfig\n\nws = Workspace.from_config()\n\nautoml_config = AutoMLConfig(task='text_classification',\n                             primary_metric='accuracy',\n                             training_data=train_data,\n                             validation_data=val_data,\n                             label_column_name='label',\n                             n_cross_validations=5,\n                             iterations=3,\n                             max_concurrent_iterations=3,\n                             enable_distributed_dnn_training=True)\n\nexperiment = Experiment(ws, 'nlp_experiment')\nrun = experiment.submit(automl_config)\n```\n\nThis code sets up an NLP experiment using Automated ML with the specified configuration, submits it to the workspace, and returns a run object that can be used to monitor the experiment.",
        "context": [
            {
                "page_content": "You can also run your NLP experiments with distributed training on an Azure ML compute cluster. \n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n\nThis is handled automatically by automated ML when the parameters `max_concurrent_iterations = number_of_vms` and `enable_distributed_dnn_training = True` are provided in your `AutoMLConfig` during experiment setup. Doing so, schedules distributed training of the NLP models and automatically scales to every GPU on your virtual machine or cluster of virtual machines. The max number of virtual machines allowed is 32. The training is scheduled with number of virtual machines that is in powers of two.\n\n```python\nmax_concurrent_iterations = number_of_vms\nenable_distributed_dnn_training = True\n```\n\nIn AutoML NLP only hold-out validation is supported and it requires a validation dataset.\n\n\n## Submit the AutoML job\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nTo submit your AutoML job, you can run the following CLI v2 command with the path to your .yml file, workspace name, resource group and subscription ID.\n\n```azurecli\n\naz ml job create --file ./hello-automl-job-basic.yml --workspace-name [YOUR_AZURE_WORKSPACE] --resource-group [YOUR_AZURE_RESOURCE_GROUP] --subscription [YOUR_AZURE_SUBSCRIPTION]\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n\nWith the `MLClient` created earlier, you can run this `CommandJob` in the workspace.\n\n```python\nreturned_job = ml_client.jobs.create_or_update(\n    text_classification_job\n)  # submit the job to the backend\n\nprint(f\"Created job: {returned_job}\")\nml_client.jobs.stream(returned_job.name)\n```\n\n\n## Code examples\n\n# [Azure CLI](#tab/cli)\n\n [!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n\nSee the following sample YAML files for each NLP task.\n\n* [Multi-class text classification](https://github.com/Azure/azureml-examples/blob/main/cli/jobs/automl-standalone-jobs/cli-automl-text-classification-newsgroup/cli-automl-text-classification-newsgroup.yml)\n* [Multi-label text classification](https://github.com/Azure/azureml-examples/blob/main/cli/jobs/automl-standalone-jobs/cli-automl-text-classification-multilabel-paper-cat/cli-automl-text-classification-multilabel-paper-cat.yml)\n* [Named entity recognition](https://github.com/Azure/azureml-examples/blob/main/cli/jobs/automl-standalone-jobs/cli-automl-text-ner-conll/cli-automl-text-ner-conll2003.yml)\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nSee the sample notebooks for detailed code examples for each NLP task. \n\n* [Multi-class text classification](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/automl-standalone-jobs/automl-nlp-text-classification-multiclass-task-sentiment-analysis/automl-nlp-multiclass-sentiment.ipynb)\n* [Multi-label text classification](\nhttps://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/automl-standalone-jobs/automl-nlp-text-classification-multilabel-task-paper-categorization/automl-nlp-multilabel-paper-cat.ipynb)\n* [Named entity recognition](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/automl-standalone-jobs/automl-nlp-text-named-entity-recognition-task/automl-nlp-text-ner-task.ipynb)\n\n\n## Model sweeping and hyperparameter tuning (preview) \n\n[!INCLUDE [preview disclaimer](../../includes/machine-learning-preview-generic-disclaimer.md)]\n\nAutoML NLP allows you to provide a list of models and combinations of hyperparameters, via the hyperparameter search space in the config. Hyperdrive generates several child runs, each of which is a fine-tuning run for a given NLP model and set of hyperparameter values that were chosen and swept over based on the provided search space.\n\n## Supported model algorithms  \n\nAll the pre-trained text DNN models currently available in AutoML NLP for fine-tuning are listed below: \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-nlp-models-245.md"
                }
            },
            {
                "page_content": "\n# What is automated machine learning (AutoML)?\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n> [!div class=\"op_single_selector\" title1=\"Select the version of the Azure Machine Learning Python SDK you are using:\"]\n> * [v1](./v1/concept-automated-ml-v1.md)\n> * [v2 (current version)](concept-automated-ml.md)\n\nAutomated machine learning, also referred to as automated ML or AutoML, is the process of automating the time-consuming, iterative tasks of machine learning model development. It allows data scientists, analysts, and developers to build ML models with high scale, efficiency, and productivity all while sustaining model quality. Automated ML in Azure Machine Learning is based on a breakthrough from our [Microsoft Research division](https://www.microsoft.com/research/project/automl/).\n\n* For code-experienced customers, [Azure Machine Learning Python SDK](https://aka.ms/sdk-v2-install).  Get started with [Tutorial: Train an object detection model (preview) with AutoML and Python](tutorial-auto-train-image-models.md).\n\n\n## How does AutoML work?\n\nDuring training, Azure Machine Learning creates a number of pipelines in parallel that try different algorithms and parameters for you. The service iterates through ML algorithms paired with feature selections, where each iteration produces a model with a training score. The better the score for the metric you want to optimize for, the better the model is considered to \"fit\" your data.  It will stop once it hits the exit criteria defined in the experiment. \n\nUsing **Azure Machine Learning**, you can design and run your automated ML training experiments with these steps:\n\n1. **Identify the ML problem** to be solved: classification, forecasting, regression, computer vision or NLP.\n\n1. **Choose whether you want a code-first experience or a no-code studio web experience**: Users who prefer a code-first experience can use the [AzureML SDKv2](how-to-configure-auto-train.md) or the [AzureML CLIv2](how-to-train-cli.md). Get started with [Tutorial: Train an object detection model with AutoML and Python](tutorial-auto-train-image-models.md). Users who prefer a limited/no-code experience can use the [web interface](how-to-use-automated-ml-for-ml-models.md) in Azure Machine Learning studio at [https://ml.azure.com](https://ml.azure.com/).  Get started with [Tutorial: Create a classification model with automated ML in Azure Machine Learning](tutorial-first-experiment-automated-ml.md).\n    \n1. **Specify the source of the labeled training data**: You can bring your data to AzureML in [many different ways](concept-data.md).\n\n1. **Configure the automated machine learning parameters** that determine how many iterations over different models, hyperparameter settings, advanced preprocessing/featurization, and what metrics to look at when determining the best model.  \n1. **Submit the training job.**\n\n1. **Review the results** \n\nThe following diagram illustrates this process. \n![Automated Machine learning](./media/concept-automated-ml/automl-concept-diagram2.png)\n\n\nYou can also inspect the logged job information, which [contains metrics](how-to-understand-automated-ml.md) gathered during the job. The training job produces a Python serialized object (`.pkl` file) that contains the model and data preprocessing.\n\nWhile model building is automated, you can also [learn how important or relevant features are](./v1/how-to-configure-auto-train-v1.md#explain) to the generated models.\n\n## When to use AutoML: classification, regression, forecasting, computer vision & NLP\n\nApply automated ML when you want Azure Machine Learning to train and tune a model for you using the target metric you specify. Automated ML democratizes the machine learning model development process, and empowers its users, no matter their data science expertise, to identify an end-to-end machine learning pipeline for any problem.\n\nML professionals and developers across industries can use automated ML to:\n+ Implement ML solutions without extensive programming knowledge\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-automated-ml-0.md"
                }
            },
            {
                "page_content": "Support for computer vision tasks allows you to easily generate models trained on image data for scenarios like image classification and object detection. \n\nWith this capability you can: \n \n* Seamlessly integrate with the [Azure Machine Learning data labeling](./how-to-create-image-labeling-projects.md) capability\n* Use labeled data for generating image models\n* Optimize model performance by specifying the model algorithm and tuning the hyperparameters. \n* Download or deploy the resulting model as a web service in Azure Machine Learning. \n* Operationalize at scale, leveraging Azure Machine Learning [MLOps](concept-model-management-and-deployment.md) and [ML Pipelines](concept-ml-pipelines.md) capabilities. \n\nAuthoring AutoML models for vision tasks is supported via the Azure ML Python SDK. The resulting experimentation jobs, models, and outputs can be accessed from the Azure Machine Learning studio UI.\n\nLearn how to [set up AutoML training for computer vision models](how-to-auto-train-image-models.md).\n\n![Computer vision tasks examples. Image from: http://cs231n.stanford.edu/slides/2021/lecture_15.pdf ](./media/concept-automated-ml/automl-computer-vision-tasks.png)\nImage from: http://cs231n.stanford.edu/slides/2021/lecture_15.pdf\n\nAutomated ML for images supports the following computer vision tasks: \n\nTask | Description\n----|----\nMulti-class image classification | Tasks where an image is classified with only a single label from a set of classes - e.g. each image is classified as either an image of a 'cat' or a 'dog' or a 'duck'\nMulti-label image classification | Tasks where an image could have one or more labels from a set of labels - e.g. an image could be labeled with both 'cat' and 'dog'\nObject detection| Tasks to identify objects in an image and locate each object with a bounding box e.g. locate all dogs and cats in an image and draw a bounding box around each.\nInstance segmentation | Tasks to identify objects in an image at the pixel level, drawing a polygon around each object in the image.\n\n<a name=\"nlp\"></a>\n\n### Natural language processing: NLP\n\nSupport for natural language processing (NLP) tasks in automated ML allows you to easily generate models trained on text data for text classification and named entity recognition scenarios. Authoring automated ML trained NLP models is supported via the Azure Machine Learning Python SDK. The resulting experimentation jobs, models, and outputs can be accessed from the Azure Machine Learning studio UI.\n\nThe NLP capability supports:\n\n* End-to-end deep neural network NLP training with the latest pre-trained BERT models\n* Seamless integration with [Azure Machine Learning data labeling](how-to-create-text-labeling-projects.md)\n* Use labeled data for generating NLP models\n* Multi-lingual support with 104 languages\n* Distributed training with Horovod\n\nLearn how to [set up AutoML training for NLP models](how-to-auto-train-nlp-models.md). \n\n\n## Training, validation and test data \n\nWith automated ML you provide the **training data** to train ML models, and you can specify what type of model validation to perform. Automated ML performs model validation as part of training. That is, automated ML uses **validation data** to tune model hyperparameters based on the applied algorithm to find the combination that best fits the training data. However, the same validation data is used for each iteration of tuning, which introduces model evaluation bias since the model continues to improve and fit to the validation data. \n\nTo help confirm that such bias isn't applied to the final recommended model, automated ML supports the use of **test data** to evaluate the final model that automated ML recommends at the end of your experiment. When you provide test data as part of your AutoML experiment configuration, this recommended model is tested by default at the end of your experiment (preview). \n\n>[!IMPORTANT]\n> Testing your models with a test dataset to evaluate generated models is a preview feature. This capability is an [experimental](/python/api/overview/azure/ml/#stable-vs-experimental) preview feature, and may change at any time.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-automated-ml-84.md"
                }
            }
        ]
    },
    {
        "query": "Generate Python Code Samples to create an environment using docker build context using AzureML SDK V2?",
        "result": "Here's an example of Python code that creates an environment using Docker build context with AzureML SDK V2:\n\n```python\nfrom azureml.core import Environment\nfrom azureml.core.environment import DockerSection, EnvironmentFile\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nenv = Environment(name=\"myenv\")\nenv.docker.enabled = True\nenv.docker.base_image = None\nenv.docker.build = DockerSection(\n    context_path=\"./docker-context\",\n    dockerfile=\"Dockerfile\"\n)\n\nconda_dep = CondaDependencies.create(pip_packages=[\"scikit-learn\"])\nenv.python.conda_dependencies = conda_dep\n\nenv_file = EnvironmentFile.from_environment(env)\nenv_file.save_to_directory(\"./env-files\")\n\nrun_config = RunConfiguration()\nrun_config.environment = env\n```\n\nFor more information, see the [official AzureML SDK V2 documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py).",
        "context": [
            {
                "page_content": "To create a new environment, you must use one of the following approaches (see [DockerSection](https://aka.ms/azureml/environment/environment-docker-section)):\n* Base image\n    * Provide base image name, repository from which to pull it, and credentials if needed\n    * Provide a conda specification\n* Base Dockerfile \n    * Provide a Dockerfile\n    * Provide a conda specification\n* Docker build context\n    * Provide the location of the build context (URL)\n    * The build context must contain at least a Dockerfile, but may contain other files as well\n\n*Applies to: Azure CLI & Python SDK v2*\n\nTo create a new environment, you must use one of the following approaches:\n* Docker image\n    * Provide the image URI of the image hosted in a registry such as Docker Hub or Azure Container Registry\n    * [Sample here](https://aka.ms/azureml/environment/create-env-docker-image-v2)\n* Docker build context\n    * Specify the directory that will serve as the build context\n    * The directory should contain a Dockerfile and any other files needed to build the image\n    * [Sample here](https://aka.ms/azureml/environment/create-env-build-context-v2)\n* Conda specification \n    * You must specify a base Docker image for the environment; the conda environment will be built on top of the Docker image provided\n    * Provide the relative path to the conda file\n    * [Sample here](https://aka.ms/azureml/environment/create-env-conda-spec-v2)\n\n### Missing Docker definition\n*Applies to: Python SDK v1*\n<!--issueDescription-->\nThis issue can happen when your environment definition is missing a `DockerSection.` This section configures settings related to the final Docker image built from your environment specification.\n \n**Potential causes:**\n* The `DockerSection` of your environment definition isn't defined (null)\n \n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n \n**Troubleshooting steps**\n\n Add a `DockerSection` to your environment definition, specifying either a base image, base dockerfile, or docker build context.\n\n```python\nfrom azureml.core import Environment\nmyenv = Environment(name=\"myenv\")\n# Specify docker steps as a string.\ndockerfile = r'''\nFROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\nRUN echo \"Hello from custom container!\"\n'''\n\nmyenv.docker.base_dockerfile = dockerfile\n```\n \n**Resources**\n* [DockerSection](https://aka.ms/azureml/environment/environment-docker-section)\n\n### Too many Docker options\n<!--issueDescription-->\n \n**Potential causes:**\n\n*Applies to: Python SDK v1*\n\nYou have more than one of these Docker options specified in your environment definition\n* `base_image`\n* `base_dockerfile`\n* `build_context`\n* See [DockerSection](https://aka.ms/azureml/environment/docker-section-class)\n\n*Applies to: Azure CLI & Python SDK v2*\n\nYou have more than one of these Docker options specified in your environment definition\n* `image`\n* `build`\n* See [azure.ai.ml.entities.Environment](https://aka.ms/azureml/environment/environment-class-v2)\n \n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n \n**Troubleshooting steps**\n\nChoose which Docker option you'd like to use to build your environment. Then set all other specified options to None.\n\n*Applies to: Python SDK v1*\n\n```python\nfrom azureml.core import Environment\nmyenv = Environment(name=\"myEnv\")\ndockerfile = r'''\nFROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\nRUN echo \"Hello from custom container!\"\n'''\nmyenv.docker.base_dockerfile = dockerfile\nmyenv.docker.base_image = \"pytorch/pytorch:latest\"\n\n# Having both base dockerfile and base image set will cause failure. Delete the one you won't use.\nmyenv.docker.base_image = None\n```\n\n### Missing Docker option\n<!--issueDescription-->\n \n**Potential causes:**\n\n*Applies to: Python SDK v1*\n\nYou didn't specify one of the following options in your environment definition\n* `base_image`\n* `base_dockerfile`\n* `build_context`\n* See [DockerSection](https://aka.ms/azureml/environment/docker-section-class)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-128.md"
                }
            },
            {
                "page_content": "* Your Dockerfile isn't at the root of your build context directory and/or is named something other than 'Dockerfile,' and you didn't provide its path\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nIn the `build_context` of your [DockerSection](https://aka.ms/azureml/environment/docker-section-class), include a `dockerfile_path`\n* See [DockerBuildContext Class](/python/api/azureml-core/azureml.core.environment.dockerbuildcontext)\n\n*Applies to: Azure CLI & Python SDK v2*\n\nSpecify a Dockerfile path\n* [See BuildContext class](https://aka.ms/azureml/environment/build-context-class)\n* See this [sample](https://aka.ms/azureml/environment/create-env-build-context-v2)\n\n**Resources**\n* [Understand build context](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#understand-build-context)\n\n### Not allowed to specify attribute with Docker build context\n<!--issueDescription-->\nThis issue can happen when you've specified properties in your environment definition that can't be included with a Docker build context.\n\n**Potential causes:**\n* You specified a Docker build context, along with at least one of the following in your environment definition:\n\t* Environment variables\n\t* Conda dependencies\n\t* R\n\t* Spark\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nIf any of the above-listed properties are specified in your environment definition, remove them\n* If you're using a Docker build context and want to specify conda dependencies, your conda specification should reside in your build context directory\n\n**Resources**\n* [Understand build context](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#understand-build-context)\n* Python SDK v1 [Environment Class](https://aka.ms/azureml/environment/environment-class-v1)\n\n### Location type not supported/Unknown location type\n<!--issueDescription-->\n**Potential causes:**\n* You specified a location type for your Docker build context that isn't supported or is unknown\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nThe following are accepted location types:\n* Git\n\t* Git URLs can be provided to AzureML, but images can't yet be built using them. Use a storage account until builds have Git support\n* Storage account\n\t* See this [storage account overview](../storage/common/storage-account-overview.md)\n\t* See how to [create a storage account](../storage/common/storage-account-create.md)\n\t\n**Resources**\n* See [DockerBuildContext Class](/python/api/azureml-core/azureml.core.environment.dockerbuildcontext)\n* [Understand build context](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#understand-build-context)\n\n### Invalid location\n<!--issueDescription-->\n**Potential causes:**\n* The specified location of your Docker build context is invalid\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nFor scenarios in which you're storing your Docker build context in a storage account\n* The path of the build context must be specified as \n\n\t`https://<storage-account>.blob.core.windows.net/<container>/<path>`\n* Ensure that the location you provided is a valid URL\n* Ensure that you've specified a container and a path\n\t\n**Resources**\n* See [DockerBuildContext Class](/python/api/azureml-core/azureml.core.environment.dockerbuildcontext)\n* [Python SDK/Azure CLI v2 sample](https://aka.ms/azureml/environment/create-env-build-context-v2)\n* [Understand build context](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#understand-build-context)\n\n## *Base image issues*\n### Base image is deprecated\n<!--issueDescription-->\n**Potential causes:**\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-476.md"
                }
            },
            {
                "page_content": "* [Python SDK v2 workspace connections](/python/api/azure-ai-ml/azure.ai.ml.entities.workspaceconnection)\n* [Azure CLI workspace connections](/cli/azure/ml/connection)\n\n### Secrets in base image registry\n<!--issueDescription-->\n\n**Potential causes:**\n\n* You've specified credentials in your environment definition\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nSpecifying credentials in your environment definition is no longer supported. Delete credentials from your environment definition and use workspace connections instead.\n\n*Applies to: Python SDK v1*\n\nSet a workspace connection on your workspace\n\n```python\nfrom azureml.core import Workspace\nws = Workspace.from_config()\nws.set_connection(\"connection1\", \"ACR\", \"<URL>\", \"Basic\", \"{'Username': '<username>', 'Password': '<password>'}\")\n```\n\n*Applies to: Azure CLI extensions v1 & v2*\n\nCreate a workspace connection from a YAML specification file\n\n```\naz ml connection create --file connection.yml --resource-group my-resource-group --workspace-name my-workspace\n```\n \n**Resources**\n* [Python SDK v1 workspace connections](https://aka.ms/azureml/environment/set-connection-v1)\n* [Python SDK v2 workspace connections](/python/api/azure-ai-ml/azure.ai.ml.entities.workspaceconnection)\n* [Azure CLI workspace connections](/cli/azure/ml/connection)\n\n### Deprecated Docker attribute\n<!--issueDescription-->\n\n**Potential causes:**\n\n* You've specified Docker attributes in your environment definition that are now deprecated\n* The following are deprecated:\n\t* `enabled`\n\t* `arguments`\n\t* `shared_volumes`\n\t* `gpu_support`\n\t\t* AzureML now automatically detects and uses NVIDIA Docker extension when available\n\t* `smh_size`\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nInstead of specifying these attributes in the `DockerSection` of your environment definition, use [DockerConfiguration](https://aka.ms/azureml/environment/docker-configuration-class)\n \n**Resources**\n* See `DockerSection` [deprecated variables](https://aka.ms/azureml/environment/docker-section-class)\n\n### Dockerfile length over limit\n<!--issueDescription-->\n**Potential causes:**\n* Your specified Dockerfile exceeded the maximum size of 100 KB\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\nShorten your Dockerfile to get it under this limit\n \n**Resources**\n* See [best practices](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)\n\n## *Docker build context issues*\n### Missing Docker build context location\n<!--issueDescription-->\n**Potential causes:**\n* You didn't provide the path of your build context directory in your environment definition\n\n**Affected areas (symptoms):**\n* Failure in registering your environment\n<!--/issueDescription-->\n\n**Troubleshooting steps**\n\n*Applies to: Python SDK v1*\n\nInclude a path in the `build_context` of your [DockerSection](https://aka.ms/azureml/environment/docker-section-class)\n* See [DockerBuildContext Class](/python/api/azureml-core/azureml.core.environment.dockerbuildcontext)\n\n*Applies to: Azure CLI & Python SDK v2*\n\nEnsure that you include a path for your build context\n* See [BuildContext class](https://aka.ms/azureml/environment/build-context-class)\n* See this [sample](https://aka.ms/azureml/environment/create-env-build-context-v2)\n\n**Resources**\n* [Understand build context](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#understand-build-context)\n\n### Missing Dockerfile path\n<!--issueDescription-->\nThis issue can happen when AzureML fails to find your Dockerfile. As a default, AzureML will look for a Dockerfile named 'Dockerfile' at the root of your build context directory unless a Dockerfile path is specified.\n\n**Potential causes:**\n* Your Dockerfile isn't at the root of your build context directory and/or is named something other than 'Dockerfile,' and you didn't provide its path\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-environments-361.md"
                }
            }
        ]
    },
    {
        "query": "How can I find out who submitted a job using the v2 CLI?",
        "result": "To find out who submitted a job using the v2 CLI, you can use the `az ml job show` command. This command will display the details of the specified job, including the `submittedBy` field, which contains the user who submitted the job. You can also use the `--query` parameter to filter the output to only show the `submittedBy` field. For example, the command `az ml job show -n <job_name> --query submittedBy` will display only the user who submitted the specified job.",
        "context": [
            {
                "page_content": "\n# What is Azure Machine Learning?\n\nAzure Machine Learning is a cloud service for accelerating and managing the machine learning project lifecycle. Machine learning professionals, data scientists, and engineers can use it in their day-to-day workflows: Train and deploy models, and manage MLOps.\n\nYou can create a model in Azure Machine Learning or use a model built from an open-source platform, such as Pytorch, TensorFlow, or scikit-learn. MLOps tools help you monitor, retrain, and redeploy models. \n\n> [!Tip]\n> **Free trial!**  If you don\u2019t have an Azure subscription, create a free account before you begin. [Try the free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/machine-learning/search/). You get credits to spend on Azure services. After they're used up, you can keep the account and use [free Azure services](https://azure.microsoft.com/free/). Your credit card is never charged unless you explicitly change your settings and ask to be charged.\n\n## Who is Azure Machine Learning for?\n\nAzure Machine Learning is for individuals and teams implementing MLOps within their organization to bring machine learning models into production in a secure and auditable production environment.\n\nData scientists and ML engineers will find tools to accelerate and automate their day-to-day workflows. Application developers will find tools for integrating models into applications or services. Platform developers will find a robust set of tools, backed by durable Azure Resource Manager APIs, for building advanced ML tooling.\n\nEnterprises working in the Microsoft Azure cloud will find familiar security and role-based access control (RBAC) for infrastructure. You can set up a project to deny access to protected data and select operations.\n\n## Productivity for everyone on the team\n\nMachine learning projects often require a team with varied skill set to build and maintain. Azure Machine Learning has tools that help enable you to: \n\n* Collaborate with your team via shared notebooks, compute resources, data, and environments\n\n* Develop models for fairness and explainability, tracking and auditability to fulfill lineage and audit compliance requirements\n\n* Deploy ML models quickly and easily at scale, and manage and govern them efficiently with MLOps\n\n* Run machine learning workloads anywhere with built-in governance, security, and compliance\n\n### Cross-compatible platform tools that meet your needs\n\nAnyone on an ML team can use their preferred tools to get the job done. Whether you're running rapid experiments, hyperparameter-tuning, building pipelines, or managing inferences, you can use familiar interfaces including:\n\n* [Azure Machine Learning studio](https://ml.azure.com)\n* [Python SDK (v2)](https://aka.ms/sdk-v2-install)\n* [CLI (v2)](how-to-configure-cli.md))\n* [Azure Resource Manager REST APIs ](/rest/api/azureml/)\n\nAs you're refining the model and collaborating with others throughout the rest of Machine Learning development cycle, you can share and find assets, resources, and metrics for your projects on the Azure Machine Learning studio UI.\n\n### Studio\n\nThe [Azure Machine Learning studio](https://ml.azure.com) offers multiple authoring experiences depending on the type of project and the level of your past ML experience, without having to install anything.\n\n* Notebooks: write and run your own code in managed Jupyter Notebook servers that are directly integrated in the studio.\n\n* Visualize run metrics: analyze and optimize your experiments with visualization.\n\n    :::image type=\"content\" source=\"media/overview-what-is-azure-machine-learning/metrics.png\" alt-text=\"Screenshot of metrics for a training run.\":::\n\n* Azure Machine Learning designer: use the designer to train and deploy machine learning models without writing any code. Drag and drop datasets and components to create ML pipelines. Try out the [designer tutorial](tutorial-designer-automobile-price-train-score.md).\n\n* Automated machine learning UI: Learn how to create [automated ML experiments](tutorial-first-experiment-automated-ml.md) with an easy-to-use interface.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/overview-what-is-azure-machine-learning-0.md"
                }
            },
            {
                "page_content": "\n# Data administration\n\n> [!div class=\"op_single_selector\" title1=\"Select the version of Azure Machine Learning SDK or CLI extension you are using:\"]\n> * [v1](./v1/concept-network-data-access.md)\n> * [v2 (current version)](how-to-administrate-data-authentication.md)\n\nLearn how to manage data access and how to authenticate in Azure Machine Learning\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n[!INCLUDE [CLI v2](../../includes/machine-learning-CLI-v2.md)]\n\n> [!IMPORTANT]\n> The information in this article is intended for Azure administrators who are creating the infrastructure required for an Azure Machine Learning solution.\n\nIn general, data access from studio involves the following checks:\n\n* Who is accessing?\n    - There are multiple different types of authentication depending on the storage type. For example, account key, token, service principal, managed identity, and user identity.\n    - If authentication is made using a user identity, then it's important to know *which* user is trying to access storage. For more information on authenticating a _user_, see [authentication for Azure Machine Learning](how-to-setup-authentication.md). For more information on service-level authentication, see [authentication between AzureML and other services](how-to-identity-based-service-authentication.md).\n* Do they have permission?\n    - Are the credentials correct? If so, does the service principal, managed identity, etc., have the necessary permissions on the storage? Permissions are granted using Azure role-based access controls (Azure RBAC).\n    - [Reader](../role-based-access-control/built-in-roles.md#reader) of the storage account reads metadata of the storage.\n    - [Storage Blob Data Reader](../role-based-access-control/built-in-roles.md#storage-blob-data-reader) reads data within a blob container.\n    - [Contributor](../role-based-access-control/built-in-roles.md#contributor) allows write access to a storage account.\n    - More roles may be required depending on the type of storage.\n* Where is access from?\n    - User: Is the client IP address in the VNet/subnet range?\n    - Workspace: Is the workspace public or does it have a private endpoint in a VNet/subnet?\n    - Storage: Does the storage allow public access, or does it restrict access through a service endpoint or a private endpoint?\n* What operation is being performed?\n    - Create, read, update, and delete (CRUD) operations on a data store/dataset are handled by Azure Machine Learning.\n    - Archive operation on data assets in the Studio requires the following RBAC operation: Microsoft.MachineLearningServices/workspaces/datasets/registered/delete\n    - Data Access calls (such as preview or schema) go to the underlying storage and need extra permissions.\n* Where is this operation being run; compute resources in your Azure subscription or resources hosted in a Microsoft subscription?\n    - All calls to dataset and datastore services (except the \"Generate Profile\" option) use resources hosted in a __Microsoft subscription__ to run the operations.\n    - Jobs, including the \"Generate Profile\" option for datasets, run on a compute resource in __your subscription__, and access the data from there. So the compute identity needs permission to the storage rather than the identity of the user submitting the job.\n\nThe following diagram shows the general flow of a data access call. In this example, a user is trying to make a data access call through a machine learning workspace, without using any compute resource.\n\n:::image type=\"content\" source=\"./media/concept-network-data-access/data-access-flow.svg\" alt-text=\"Diagram of the logic flow when accessing data.\":::\n\n## Scenarios and identities\n\nThe following table lists what identities should be used for specific scenarios:\n\n| Scenario | Use workspace</br>Managed Service Identity (MSI) | Identity to use |\n|--|--|--|\n| Access from UI | Yes | Workspace MSI |\n| Access from UI | No | User's Identity |\n| Access from Job | Yes/No | Compute MSI |\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-administrate-data-authentication-0.md"
                }
            },
            {
                "page_content": "\nFinally use your parallel job as a step in your pipeline and bind its inputs/outputs with other steps:\n\n```python\n@pipeline()\ndef parallel_in_pipeline(pipeline_job_data_path, pipeline_score_model):\n\n    prepare_file_tabular_data = prepare_data(input_data=pipeline_job_data_path)\n    # output of file & tabular data should be type MLTable\n    prepare_file_tabular_data.outputs.file_output_data.type = AssetTypes.MLTABLE\n    prepare_file_tabular_data.outputs.tabular_output_data.type = AssetTypes.MLTABLE\n\n    batch_inference_with_file_data = file_batch_inference(\n        job_data_path=prepare_file_tabular_data.outputs.file_output_data\n    )\n    # use eval_mount mode to handle file data\n    batch_inference_with_file_data.inputs.job_data_path.mode = (\n        InputOutputModes.EVAL_MOUNT\n    )\n    batch_inference_with_file_data.outputs.job_output_path.type = AssetTypes.MLTABLE\n\n    batch_inference_with_tabular_data = tabular_batch_inference(\n        job_data_path=prepare_file_tabular_data.outputs.tabular_output_data,\n        score_model=pipeline_score_model,\n    )\n    # use direct mode to handle tabular data\n    batch_inference_with_tabular_data.inputs.job_data_path.mode = (\n        InputOutputModes.DIRECT\n    )\n\n    return {\n        \"pipeline_job_out_file\": batch_inference_with_file_data.outputs.job_output_path,\n        \"pipeline_job_out_tabular\": batch_inference_with_tabular_data.outputs.job_output_path,\n    }\n\n\npipeline_job_data_path = Input(\n    path=\"./dataset/\", type=AssetTypes.MLTABLE, mode=InputOutputModes.RO_MOUNT\n)\npipeline_score_model = Input(\n    path=\"./model/\", type=AssetTypes.URI_FOLDER, mode=InputOutputModes.DOWNLOAD\n)\n# create a pipeline\npipeline_job = parallel_in_pipeline(\n    pipeline_job_data_path=pipeline_job_data_path,\n    pipeline_score_model=pipeline_score_model,\n)\npipeline_job.outputs.pipeline_job_out_tabular.type = AssetTypes.URI_FILE\n\n# set pipeline level compute\npipeline_job.settings.default_compute = \"cpu-cluster\"\n```\n\n\n## Submit pipeline job and check parallel step in Studio UI\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nYou can submit your pipeline job with parallel step by using the CLI command:\n\n```azurecli\naz ml job create --file pipeline.yml\n```\n\n# [Python](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nYou can submit your pipeline job with parallel step by using `jobs.create_or_update` function of ml_client:\n\n```python\npipeline_job = ml_client.jobs.create_or_update(\n    pipeline_job, experiment_name=\"pipeline_samples\"\n)\npipeline_job\n```\n\n\nOnce you submit your pipeline job, the SDK or CLI widget will give you a web URL link to the Studio UI. The link will guide you to the pipeline graph view by default. Double select the parallel step to open the right panel of your parallel job.\n\nTo check the settings of your parallel job, navigate to **Parameters** tab, expand **Run settings**, and check **Parallel** section:\n\n:::image type=\"content\" source=\"./media/how-to-use-parallel-job-in-pipeline/screenshot-for-parallel-job-settings.png\" alt-text=\"Screenshot of Azure ML studio on the jobs tab showing the parallel job settings.\" lightbox =\"./media/how-to-use-parallel-job-in-pipeline/screenshot-for-parallel-job-settings.png\":::\n\nTo debug the failure of your parallel job, navigate to **Outputs + Logs** tab, expand **logs** folder from output directories on the left, and check **job_result.txt** to understand why the parallel job is failed. For more detail about logging structure of parallel job, see the **readme.txt** under the same folder.\n\n:::image type=\"content\" source=\"./media/how-to-use-parallel-job-in-pipeline/screenshot-for-parallel-job-result.png\" alt-text=\"Screenshot of Azure ML studio on the jobs tab showing the parallel job results.\" lightbox =\"./media/how-to-use-parallel-job-in-pipeline/screenshot-for-parallel-job-result.png\":::\n\n## Parallel job in pipeline examples\n\n- Azure CLI + YAML:\n    - [Iris prediction using parallel](https://github.com/Azure/azureml-examples/tree/sdk-preview/cli/jobs/pipelines/iris-batch-prediction-using-parallel) (tabular input)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-parallel-job-in-pipeline-703.md"
                }
            }
        ]
    },
    {
        "query": "can you give me a concrete yaml file for a pipeline that would upload the data at job submission?",
        "result": "Here's an example YAML file for a pipeline that uploads data at job submission:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\ndisplay_name: upload_data_pipeline\njobs:\n  upload_data_job:\n    inputs:\n      input_data: \n        path: ./data\n        mode: upload\n    command: echo \"Data uploaded successfully\"\n    environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\n    compute: azureml:cpu-cluster\n```\n\nThis YAML file defines a pipeline with a single job that uploads data at job submission. The `inputs` section specifies the input data to be uploaded, with the `path` set to the local path of the data and the `mode` set to `upload`. The `command` section specifies the command to be executed, which in this case is just a simple echo statement. The `environment` and `compute` sections specify the environment and compute target to be used for the job.",
        "context": [
            {
                "page_content": "\n# Create and run machine learning pipelines using components with the Azure Machine Learning CLI\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n\nIn this article, you learn how to create and run [machine learning pipelines](concept-ml-pipelines.md) by using the Azure CLI and components (for more, see [What is an Azure Machine Learning component?](concept-component.md)). You can create pipelines without using components, but components offer the greatest amount of flexibility and reuse. AzureML Pipelines may be defined in YAML and run from the CLI, authored in Python, or composed in AzureML Studio Designer with a drag-and-drop UI. This document focuses on the CLI.\n\n## Prerequisites\n\n- If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n\n- An Azure Machine Learning workspace. [Create workspace resources](quickstart-create-resources.md).\n\n- [Install and set up the Azure CLI extension for Machine Learning](how-to-configure-cli.md).\n\n- Clone the examples repository:\n\n    ```azurecli-interactive\n    git clone https://github.com/Azure/azureml-examples --depth 1\n    cd azureml-examples/cli/jobs/pipelines-with-components/basics\n    ```\n\n### Suggested pre-reading\n\n- [What is Azure Machine Learning pipeline](./concept-ml-pipelines.md)\n- [What is Azure Machine Learning component](./concept-component.md)\n\n## Create your first pipeline with component\n\nLet's create your first pipeline with component using an example. This section aims to give you an initial impression of what pipeline and component look like in AzureML with a concrete example.\n\nFrom the `cli/jobs/pipelines-with-components/basics` directory of the [`azureml-examples` repository](https://github.com/Azure/azureml-examples), navigate to the `3b_pipeline_with_data` subdirector. There are three types of files in this directory. Those are the files you'll need to create when building your own pipeline.\n\n- **pipeline.yml**: This YAML file defines the machine learning pipeline. This YAML file describes how to break a full machine learning task into a multistep workflow. For example, considering a simple machine learning task of using historical data to train a sales forecasting model, you may want to build a sequential workflow with data processing, model training, and model evaluation steps.  Each step is a component that has well defined interface and can be developed, tested, and optimized independently. The pipeline YAML also defines how the child steps connect to other steps in the pipeline, for example the model training step generate a model file and the model file will pass to a model evaluation step.\n\n- **component.yml**:  This YAML file defines the component. It packages following information:\n  - Metadata: name, display name, version, description, type etc. The metadata helps to describe and manage the component.\n  - Interface: inputs and outputs. For example, a model training component will take training data and number of epochs as input, and generate a trained model file as output. Once the interface is defined, different teams can develop and test the component independently.\n  - Command, code & environment: the command, code and environment to run the component.    Command is the shell command to execute the component. Code usually refers to a source  code directory. Environment could be an AzureML environment(curated or customer created), docker image or conda environment.  \n\n- **component_src**: This is the source code directory for a specific component. It contains the source code that will be executed in the component. You can use your preferred language(Python, R...). The code must be executed by a shell command. The source code can take a few inputs from shell command line to control how this step is going to be executed. For example, a training step may take training data, learning rate, number of epochs to control the training process. The argument of a shell command is used to pass inputs and outputs to the code. \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-component-pipelines-cli-0.md"
                }
            },
            {
                "page_content": "| `path` | string | The path to the data to use as input. This can be specified in a few ways: <br><br> - A local path to the data source file or folder, e.g. `path: ./iris.csv`. The data will get uploaded during job submission. <br><br> - A URI of a cloud path to the file or folder to use as the input. Supported URI types are `azureml`, `https`, `wasbs`, `abfss`, `adl`. See [Core yaml syntax](reference-yaml-core-syntax.md) for more information on how to use the `azureml://` URI format. <br><br> - An existing registered Azure ML data asset to use as the input. To reference a registered data asset use the `azureml:<data_name>:<data_version>` syntax or `azureml:<data_name>@latest` (to reference the latest version of that data asset), e.g. `path: azureml:cifar10-data:1` or `path: azureml:cifar10-data@latest`. | | |\n| `mode` | string | Mode of how the data should be delivered to the compute target. <br><br> For read-only mount (`ro_mount`), the data will be consumed as a mount path. A folder will be mounted as a folder and a file will be mounted as a file. Azure ML will resolve the input to the mount path. <br><br> For `download` mode the data will be downloaded to the compute target. Azure ML will resolve the input to the downloaded path. <br><br> If you only want the URL of the storage location of the data artifact(s) rather than mounting or downloading the data itself, you can use the `direct` mode. This will pass in the URL of the storage location as the job input. Note that in this case you are fully responsible for handling credentials to access the storage. | `ro_mount`, `download`, `direct` | `ro_mount` |\n\n### Job outputs\n\n| Key | Type | Description | Allowed values | Default value |\n| --- | ---- | ----------- | -------------- | ------------- |\n| `type` | string | The type of job output. For the default `uri_folder` type, the output will correspond to a folder. | `uri_file`, `uri_folder`, `mltable`, `mlflow_model` | `uri_folder` |\n| `mode` | string | Mode of how output file(s) will get delivered to the destination storage. For read-write mount mode (`rw_mount`) the output directory will be a mounted directory. For upload mode the file(s) written will get uploaded at the end of the job. | `rw_mount`, `upload` | `rw_mount` |\n\n### Identity configurations\n\n#### UserIdentityConfiguration\n\n| Key | Type | Description | Allowed values |\n| --- | ---- | ----------- | -------------- |\n| `type` | const | **Required.** Identity type.  | `user_identity` |\n\n#### ManagedIdentityConfiguration\n\n| Key | Type | Description | Allowed values |\n| --- | ---- | ----------- | -------------- |\n| `type` | const | **Required.** Identity type.  | `managed` or `managed_identity` |\n\n## Remarks\n\nThe `az ml job` commands can be used for managing Azure Machine Learning pipeline jobs.\n\n## Examples\n\nExamples are available in the [examples GitHub repository](https://github.com/Azure/azureml-examples/tree/main/cli/jobs). Several are shown below.\n\n## YAML: hello pipeline\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\ndisplay_name: hello_pipeline\njobs:\n  hello_job:\n    command: echo \"hello\"\n    environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\n    compute: azureml:cpu-cluster\n  world_job:\n    command: echo \"world\"\n    environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\n    compute: azureml:cpu-cluster\n \n```\n\n## YAML: input/output dependency\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\ndisplay_name: hello_pipeline_io\njobs:\n  hello_job:\n    command: echo \"hello\" && echo \"world\" > ${{outputs.world_output}}/world.txt\n    environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\n    compute: azureml:cpu-cluster\n    outputs:\n      world_output:\n  world_job:\n    command: cat ${{inputs.world_input}}/world.txt\n    environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n    compute: azureml:cpu-cluster\n    inputs:\n      world_input: ${{parent.jobs.hello_job.outputs.world_output}}\n \n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-yaml-job-pipeline-47.md"
                }
            },
            {
                "page_content": "| `mode` | string | Mode of how the data should be delivered to the compute target. <br><br> For read-only mount (`ro_mount`), the data will be consumed as a mount path. A folder will be mounted as a folder and a file will be mounted as a file. Azure ML will resolve the input to the mount path. <br><br> For `download` mode the data will be downloaded to the compute target. Azure ML will resolve the input to the downloaded path. <br><br> If you only want the URL of the storage location of the data artifact(s) rather than mounting or downloading the data itself, you can use the `direct` mode. This will pass in the URL of the storage location as the job input. In this case, you're fully responsible for handling credentials to access the storage. | `ro_mount`, `download`, `direct` | `ro_mount` |\n\n### Job outputs\n\n| Key | Type | Description | Allowed values | Default value |\n| --- | ---- | ----------- | -------------- | ------------- |\n| `type` | string | The type of job output. For the default `uri_folder` type, the output will correspond to a folder. | `uri_folder` | `uri_folder` |\n| `path` | string | The path to the data to use as input. This can be specified in a few ways: <br><br> - A local path to the data source file or folder, for example, `path: ./iris.csv`. The data will get uploaded during job submission. <br><br> - A URI of a cloud path to the file or folder to use as the input. Supported URI types are `azureml`, `https`, `wasbs`, `abfss`, `adl`. For more information on how to use the `azureml://` URI format, see [Core yaml syntax](reference-yaml-core-syntax.md). <br><br> - An existing registered Azure ML data asset to use as the input. To reference a registered data asset, use the `azureml:<data_name>:<data_version>` syntax or `azureml:<data_name>@latest` (to reference the latest version of that data asset), for example, `path: azureml:cifar10-data:1` or `path: azureml:cifar10-data@latest`. | | |\n| `mode` | string | Mode of how output file(s) will get delivered to the destination storage. For read-write mount mode (`rw_mount`) the output directory will be a mounted directory. For upload mode the file(s) written will get uploaded at the end of the job. | `rw_mount`, `upload` | `rw_mount` |\n\n## Remarks\n\nThe `az ml schedule` command can be used for managing Azure Machine Learning models.\n\n## Examples\n\nExamples are available in the [examples GitHub repository](https://github.com/Azure/azureml-examples/tree/main/cli/schedules). A couple are shown below.\n\n## YAML: Schedule with recurrence pattern\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/schedule.schema.json\nname: simple_recurrence_job_schedule\ndisplay_name: Simple recurrence job schedule\ndescription: a simple hourly recurrence job schedule\n\ntrigger:\n  type: recurrence\n  frequency: day #can be minute, hour, day, week, month\n  interval: 1 #every day\n  schedule:\n    hours: [4,5,10,11,12]\n    minutes: [0,30]\n  start_time: \"2022-07-10T10:00:00\" # optional - default will be schedule creation time\n  time_zone: \"Pacific Standard Time\" # optional - default will be UTC\n\ncreate_job: ./simple-pipeline-job.yml\n# create_job: azureml:simple-pipeline-job\n\n```\n\n## YAML: Schedule with cron expression\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/schedule.schema.json\nname: simple_cron_job_schedule\ndisplay_name: Simple cron job schedule\ndescription: a simple hourly cron job schedule\n\ntrigger:\n  type: cron\n  expression: \"0 * * * *\"\n  start_time: \"2022-07-10T10:00:00\" # optional - default will be schedule creation time\n  time_zone: \"Pacific Standard Time\" # optional - default will be UTC\n\n# create_job: azureml:simple-pipeline-job\ncreate_job: ./simple-pipeline-job.yml\n```\n\n## Appendix\n\n### Timezone\n\nCurrent schedule supports the following timezones. The key can be used directly in the Python SDK, while the value can be used in the YAML job. The table is organized by UTC(Coordinated Universal Time).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-yaml-schedule-87.md"
                }
            }
        ]
    },
    {
        "query": "I asked you how to do it in the v2 cli -- can you show me?",
        "result": "I'm sorry, I cannot answer this question without more context. Please provide more information about what you are trying to do in the v2 CLI.",
        "context": [
            {
                "page_content": "Currently there are three action rules related to schedules and you can configure in Azure portal. You can learn more details about [how to manage access to an Azure Machine Learning workspace.](how-to-assign-roles.md#create-custom-role)\n\n| Action | Description                                                                | Rule                                                          |\n|--------|----------------------------------------------------------------------------|---------------------------------------------------------------|\n| Read   | Get and list schedules in Machine Learning workspace                        | Microsoft.MachineLearningServices/workspaces/schedules/read   |\n| Write  | Create, update, disable and enable schedules in Machine Learning workspace | Microsoft.MachineLearningServices/workspaces/schedules/write  |\n| Delete | Delete a schedule in Machine Learning workspace                            | Microsoft.MachineLearningServices/workspaces/schedules/delete |\n\n## Frequently asked questions\n\n- Why my schedules created by SDK aren't listed in UI?\n\n    The schedules UI is for v2 schedules. Hence, your v1 schedules won't be listed or accessed via UI.\n\n    However, v2 schedules also support v1 pipeline jobs. You don't have to publish pipeline first, and you can directly set up schedules for a pipeline job.\n\n- Why my schedules don't trigger job at the time I set before?\n  - By default schedules will use UTC timezone to calculate trigger time. You can specify timezone in the creation wizard, or update timezone in schedule detail page.\n  - If you set the recurrence as the 31st day of every month, in months with less than 31 days, the schedule won't trigger jobs.\n  - If you're using cron expressions, MONTH isn't supported. If you pass a value, it will be ignored and treated as *. This is a known limitation.\n- Are event-based schedules supported?\n  - No, V2 schedule does not support event-based schedules.\n\n## Next steps\n\n* Learn more about the [CLI (v2) schedule YAML schema](./reference-yaml-schedule.md).\n* Learn how to [create pipeline job in CLI v2](how-to-create-component-pipelines-cli.md).\n* Learn how to [create pipeline job in SDK v2](how-to-create-component-pipeline-python.md).\n* Learn more about [CLI (v2) core YAML syntax](reference-yaml-core-syntax.md).\n* Learn more about [Pipelines](concept-ml-pipelines.md).\n* Learn more about [Component](concept-component.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-schedule-pipeline-job-871.md"
                }
            },
            {
                "page_content": "| 3/1/2001   | 20    | 2/1/2001  | 10        | 1       |\n| 3/1/2001   | 20    | 1/1/2001  | 0         | 2       |\n| 3/1/2001   | 20    | 12/1/2000 | -         | 3       |\n| 4/1/2001   | 30    | 3/1/2001  | 20        | 1       |\n| 4/1/2001   | 30    | 2/1/2001  | 10        | 2       |\n| 4/1/2001   | 30    | 1/1/2001  | 0         | 3       |\n| 5/1/2001   | 40    | 4/1/2001  | 30        | 1       |\n| 5/1/2001   | 40    | 3/1/2001  | 20        | 2       |\n| 5/1/2001   | 40    | 2/1/2001  | 10        | 3       |\n| 6/1/2001   | 50    | 4/1/2001  | 40        | 1       |\n| 6/1/2001   | 50    | 4/1/2001  | 30        | 2       |\n| 6/1/2001   | 50    | 3/1/2001  | 20        | 3       |\n\n\nIn the final table, we've changed the name of the lag column to $y_{t-1}^{(h)}$ to reflect that the lag is generated with respect to a specific horizon. The table shows that the lags we generated with respect to the horizon can be mapped to the conventional ways of generating lags in the previous tables.\n\nTable 5 is an example of the data augmentation that AutoML applies to training data to enable direct forecasting from regression models. When the configuration includes lag features, AutoML creates horizon dependent lags along with an integer-valued horizon feature. This enables AutoML's forecasting regression models to make a prediction at horizon $h$ without regard to the prediction at $h-1$, in contrast to recursively defined models like ARIMA.\n\n> [!NOTE]\n> Generation of horizon dependent lag features adds new _rows_ to the dataset. The number of new rows is proportional to forecast horizon. This dataset size growth can lead to out-of-memory errors on smaller compute nodes or when dataset size is already large. See the [frequently asked questions](./how-to-automl-forecasting-faq.md#how-do-i-fix-an-out-of-memory-error) article for solutions to this problem.       \n\nAnother consequence of this lagging strategy is that lag order and forecast horizon are decoupled. If, for example, your forecast horizon is seven, and you want AutoML to use lag features, you do not have to set the lag order to seven to ensure prediction over a full forecast horizon. Since AutoML generates lags with respect to horizon, you can set the lag order to one and AutoML will augment the data so that lags of any order are valid up to forecast horizon.\n\n## Next steps\n* Learn more about [how to set up AutoML to train a time-series forecasting model](./how-to-auto-train-forecast.md).\n* Browse [AutoML Forecasting Frequently Asked Questions](./how-to-automl-forecasting-faq.md).\n* Learn about [calendar features for time series forecasting in AutoML](./concept-automl-forecasting-calendar-features.md).\n* Learn about [how AutoML uses machine learning to build forecasting models](./concept-automl-forecasting-methods.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-automl-forecasting-lags-71.md"
                }
            },
            {
                "page_content": "* Learn more about [how to set up AutoML to train a time-series forecasting model](./how-to-auto-train-forecast.md).\n* Browse [AutoML Forecasting Frequently Asked Questions](./how-to-automl-forecasting-faq.md).\n* Learn about [AutoML Forecasting Lagged Features](./concept-automl-forecasting-lags.md).\n* Learn about [how AutoML uses machine learning to build forecasting models](./concept-automl-forecasting-methods.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-automl-forecasting-calendar-features-90.md"
                }
            }
        ]
    },
    {
        "query": "how can I upload data to the cloud for use in a pipeline with the CLI v2?",
        "result": "You can upload data to the cloud for use in a pipeline with the Azure Machine Learning CLI v2 by registering a dataset to Azure Machine Learning from a cloud service. You can also use the Import Data module in the designer to ingest data from a cloud service. Once you have registered a dataset in Azure Machine Learning, you can use it in your pipeline by dragging it onto the canvas. You can also write data to your cloud-based storage with outputs in your job.",
        "context": [
            {
                "page_content": "| `mode` | string | Mode of how the data should be delivered to the compute target. <br><br> For read-only mount (`ro_mount`), the data will be consumed as a mount path. A folder will be mounted as a folder and a file will be mounted as a file. Azure ML will resolve the input to the mount path. <br><br> For `download` mode the data will be downloaded to the compute target. Azure ML will resolve the input to the downloaded path. <br><br> If you only want the URL of the storage location of the data artifact(s) rather than mounting or downloading the data itself, you can use the `direct` mode. This will pass in the URL of the storage location as the job input. In this case, you're fully responsible for handling credentials to access the storage. | `ro_mount`, `download`, `direct` | `ro_mount` |\n\n### Job outputs\n\n| Key | Type | Description | Allowed values | Default value |\n| --- | ---- | ----------- | -------------- | ------------- |\n| `type` | string | The type of job output. For the default `uri_folder` type, the output will correspond to a folder. | `uri_folder` | `uri_folder` |\n| `path` | string | The path to the data to use as input. This can be specified in a few ways: <br><br> - A local path to the data source file or folder, for example, `path: ./iris.csv`. The data will get uploaded during job submission. <br><br> - A URI of a cloud path to the file or folder to use as the input. Supported URI types are `azureml`, `https`, `wasbs`, `abfss`, `adl`. For more information on how to use the `azureml://` URI format, see [Core yaml syntax](reference-yaml-core-syntax.md). <br><br> - An existing registered Azure ML data asset to use as the input. To reference a registered data asset, use the `azureml:<data_name>:<data_version>` syntax or `azureml:<data_name>@latest` (to reference the latest version of that data asset), for example, `path: azureml:cifar10-data:1` or `path: azureml:cifar10-data@latest`. | | |\n| `mode` | string | Mode of how output file(s) will get delivered to the destination storage. For read-write mount mode (`rw_mount`) the output directory will be a mounted directory. For upload mode the file(s) written will get uploaded at the end of the job. | `rw_mount`, `upload` | `rw_mount` |\n\n## Remarks\n\nThe `az ml schedule` command can be used for managing Azure Machine Learning models.\n\n## Examples\n\nExamples are available in the [examples GitHub repository](https://github.com/Azure/azureml-examples/tree/main/cli/schedules). A couple are shown below.\n\n## YAML: Schedule with recurrence pattern\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/schedule.schema.json\nname: simple_recurrence_job_schedule\ndisplay_name: Simple recurrence job schedule\ndescription: a simple hourly recurrence job schedule\n\ntrigger:\n  type: recurrence\n  frequency: day #can be minute, hour, day, week, month\n  interval: 1 #every day\n  schedule:\n    hours: [4,5,10,11,12]\n    minutes: [0,30]\n  start_time: \"2022-07-10T10:00:00\" # optional - default will be schedule creation time\n  time_zone: \"Pacific Standard Time\" # optional - default will be UTC\n\ncreate_job: ./simple-pipeline-job.yml\n# create_job: azureml:simple-pipeline-job\n\n```\n\n## YAML: Schedule with cron expression\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/schedule.schema.json\nname: simple_cron_job_schedule\ndisplay_name: Simple cron job schedule\ndescription: a simple hourly cron job schedule\n\ntrigger:\n  type: cron\n  expression: \"0 * * * *\"\n  start_time: \"2022-07-10T10:00:00\" # optional - default will be schedule creation time\n  time_zone: \"Pacific Standard Time\" # optional - default will be UTC\n\n# create_job: azureml:simple-pipeline-job\ncreate_job: ./simple-pipeline-job.yml\n```\n\n## Appendix\n\n### Timezone\n\nCurrent schedule supports the following timezones. The key can be used directly in the Python SDK, while the value can be used in the YAML job. The table is organized by UTC(Coordinated Universal Time).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-yaml-schedule-87.md"
                }
            },
            {
                "page_content": ":::image type=\"content\" source=\"./media/how-to-create-data-assets/data-assets-create.png\" alt-text=\"Screenshot highlights Create in the Data assets tab.\":::\n\n1. Give your data asset a name and optional description. Then, select the **Tabular** option under **Type**, in the **Dataset types** section of the dropdown.\n    > [!NOTE]\n    > You can also upload ZIP files as data assets. To upload a ZIP file, select **File** for **Type**, in the **Dataset types** section of the dropdown.\n:::image type=\"content\" source=\"./media/migrate-register-dataset/create-data-asset.png\" alt-text=\"Screenshot shows data asset source choices.\":::\n\n1. For data source, select the \"From local files\" option to upload your dataset.\n\n1. For file selection, first choose where you want your data to be stored in Azure. You will be selecting an Azure Machine Learning datastore. For more information on datastores, see [Connect to storage services](v1/how-to-access-data.md). Next, upload the dataset you downloaded earlier.\n\n1. Follow the steps to set the data parsing settings and schema for your data asset.\n\n1. Once you reach the Review step, click Create on the last page\n\n## Import data from cloud sources\n\nIf your data is already in a cloud storage service, and you want to keep your data in its native location. You can use either of the following options:\n\n|Ingestion method|Description|\n|---| --- |\n|Register an Azure Machine Learning dataset|Ingest data from local and online data sources (Blob, ADLS Gen1, ADLS Gen2, File share, SQL DB). <br><br>Creates a reference to the data source, which is lazily evaluated at runtime. Use this option if you repeatedly access this dataset and want to enable advanced data features like data versioning and monitoring.\n|Import Data module|Ingest data from online data sources (Blob, ADLS Gen1, ADLS Gen2, File share, SQL DB). <br><br> The dataset is only imported to the current designer pipeline run.\n\n\n>[!Note]\n> Studio (classic) users should note that the following cloud sources are not natively supported in Azure Machine Learning:\n> - Hive Query\n> - Azure Table\n> - Azure Cosmos DB\n> - On-premises SQL Database\n>\n> We recommend that users migrate their data to a supported storage services using Azure Data Factory.  \n\n### Register an Azure Machine Learning dataset\n\nUse the following steps to register a dataset to Azure Machine Learning from a cloud service: \n\n1. [Create a datastore](v1/how-to-connect-data-ui.md#create-datastores), which links the cloud storage service to your Azure Machine Learning workspace. \n\n1. [Register a dataset](v1/how-to-connect-data-ui.md#create-data-assets). If you are migrating a Studio (classic) dataset, select the **Tabular** dataset setting.\n\nAfter you register a dataset in Azure Machine Learning, you can use it in designer:\n \n1. Create a new designer pipeline draft.\n1. In the module palette to the left, expand the **Datasets** section.\n1. Drag your registered dataset onto the canvas. \n\n### Use the Import Data module\n\nUse the following steps to import data directly to your designer pipeline:\n\n1. [Create a datastore](v1/how-to-connect-data-ui.md#create-datastores), which links the cloud storage service to your Azure Machine Learning workspace. \n\nAfter you create the datastore, you can use the [**Import Data**](algorithm-module-reference/import-data.md) module in the designer to ingest data from it:\n\n1. Create a new designer pipeline draft.\n1. In the module palette to the left, find the **Import Data** module and drag it to the canvas.\n1. Select the **Import Data** module, and use the settings in the right panel to configure your data source.\n\n## Next steps\n\nIn this article, you learned how to migrate a Studio (classic) dataset to Azure Machine Learning. The next step is to [rebuild a Studio (classic) training pipeline](migrate-rebuild-experiment.md).\n\n\nSee the other articles in the Studio (classic) migration series:\n\n1. [Migration overview](migrate-overview.md).\n1. **Migrate datasets**.\n1. [Rebuild a Studio (classic) training pipeline](migrate-rebuild-experiment.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-register-dataset-72.md"
                }
            },
            {
                "page_content": "\n\n## Write data in a job\n\nIn your job, you can write data to your cloud-based storage with *outputs*. The [Supported modes](#supported-modes) section showed that only job *outputs* can write data, because the mode can be either `rw_mount` or `upload`.\n\n# [Azure CLI](#tab/cli)\n\nCreate a job specification YAML file (`<file-name>.yml`), with the `outputs` section populated with the type and path where you'd like to write your data:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/CommandJob.schema.json\n\n# Possible Paths for Data:\n# Blob: https://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>\n# Datastore: azureml://datastores/paths/<folder>/<file>\n# Data Asset: azureml:<my_data>:<version>\n\ncode: src\ncommand: >-\n  python prep.py \n  --raw_data ${{inputs.raw_data}} \n  --prep_data ${{outputs.prep_data}}\ninputs:\n  raw_data: \n    type: <type> # uri_file, uri_folder, mltable\n    path: <path>\noutputs:\n  prep_data: \n    type: <type> # uri_file, uri_folder, mltable\n    path: <path>\nenvironment: azureml:<environment_name>@latest\ncompute: azureml:cpu-cluster\n```\n\nNext, create a job with the CLI:\n\n```azurecli\naz ml job create --file <file-name>.yml\n```\n\n# [Python SDK](#tab/python)\n\n```python\nfrom azure.ai.ml import command\nfrom azure.ai.ml.entities import Data\nfrom azure.ai.ml import Input, Output\nfrom azure.ai.ml.constants import AssetTypes\n\n# Possible Asset Types for Data:\n# AssetTypes.URI_FILE\n# AssetTypes.URI_FOLDER\n# AssetTypes.MLTABLE\n\n# Possible Paths for Data:\n# Blob: https://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>\n# Datastore: azureml://datastores/paths/<folder>/<file>\n# Data Asset: azureml:<my_data>:<version>\n\nmy_job_inputs = {\n    \"raw_data\": Input(type=AssetTypes.URI_FOLDER, path=\"<path>\")\n}\n\nmy_job_outputs = {\n    \"prep_data\": Output(type=AssetTypes.URI_FOLDER, path=\"<path>\")\n}\n\njob = command(\n    code=\"./src\",  # local path where the code is stored\n    command=\"python process_data.py --raw_data ${{inputs.raw_data}} --prep_data ${{outputs.prep_data}}\",\n    inputs=my_job_inputs,\n    outputs=my_job_outputs,\n    environment=\"<environment_name>:<version>\",\n    compute=\"cpu-cluster\",\n)\n\n# submit the command\nreturned_job = ml_client.create_or_update(job)\n# get a URL for the status of the job\nreturned_job.services[\"Studio\"].endpoint\n\n```\n\n\n## Data in pipelines\n\nIf you work with Azure Machine Learning pipelines, you can read data into and move data between pipeline components with the Azure Machine Learning CLI v2 extension, or the Python SDK v2.\n\n### Azure Machine Learning CLI v2\nThis YAML file shows how to use the output data from one component as the input for another component of the pipeline, with the Azure Machine Learning CLI v2 extension:\n\n[!INCLUDE [CLI v2](../../includes/machine-learning-CLI-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\n\ndisplay_name: 3b_pipeline_with_data\ndescription: Pipeline with 3 component jobs with data dependencies\n\nsettings:\n  default_compute: azureml:cpu-cluster\n\noutputs:\n  final_pipeline_output:\n    mode: rw_mount\n\njobs:\n  component_a:\n    type: command\n    component: ./componentA.yml\n    inputs:\n      component_a_input: \n        type: uri_folder\n        path: ./data\n\n    outputs:\n      component_a_output: \n        mode: rw_mount\n  component_b:\n    type: command\n    component: ./componentB.yml\n    inputs:\n      component_b_input: ${{parent.jobs.component_a.outputs.component_a_output}}\n    outputs:\n      component_b_output: \n        mode: rw_mount\n  component_c:\n    type: command\n    component: ./componentC.yml\n    inputs:\n      component_c_input: ${{parent.jobs.component_b.outputs.component_b_output}}\n    outputs:\n      component_c_output: ${{parent.outputs.final_pipeline_output}}\n      #  mode: upload\n\n\n```\n\n### Python SDK v2\n\nThis example defines a pipeline that contains three nodes, and moves data between each node.\n\n* `prepare_data_node` loads the image and labels from Fashion MNIST data set into `mnist_train.csv` and `mnist_test.csv`.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-read-write-data-v2-259.md"
                }
            }
        ]
    },
    {
        "query": "how do I shut down a compute instance using the v2 cli?",
        "result": "To shut down a compute instance using the v2 CLI, use the `az ml compute stop` command followed by the name of the compute instance and the resource group it belongs to. For example, to stop a compute instance named \"mycompute\" in the \"myresourcegroup\" resource group, use the following command:\n\n```\naz ml compute stop -n mycompute -g myresourcegroup\n```\n\nThis will stop the compute instance and deallocate its resources.",
        "context": [
            {
                "page_content": "Administrators can use a built-in [Azure Policy](./../governance/policy/overview.md) definition to enforce auto-stop on all compute instances in a given subscription/resource-group. \n\n1. Navigate to Azure Policy in the Azure portal.\n2. Under \"Definitions\", look for the idle shutdown policy.\n\n      :::image type=\"content\" source=\"media/how-to-create-attach-studio/idle-shutdown-policy.png\" alt-text=\"Screenshot for the idle shutdown policy in Azure portal.\" lightbox=\"media/how-to-create-attach-studio/idle-shutdown-policy.png\":::\n\n3. Assign policy to the necessary scope.\n\nYou can also create your own custom Azure policy. For example, if the below policy is assigned, all new compute instances will have auto-stop configured with a 60-minute inactivity period. \n\n```json\n{\n  \"mode\": \"All\",\n  \"policyRule\": {\n    \"if\": {\n      \"allOf\": [\n        {\n          \"field\": \"type\",\n          \"equals\": \"Microsoft.MachineLearningServices/workspaces/computes\"\n        },\n        {\n          \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/computeType\",\n          \"equals\": \"ComputeInstance\"\n        },\n        {\n          \"anyOf\": [\n            {\n              \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/idleTimeBeforeShutdown\",\n              \"exists\": false\n            },\n            {\n              \"value\": \"[empty(field('Microsoft.MachineLearningServices/workspaces/computes/idleTimeBeforeShutdown'))]\",\n              \"equals\": true\n            }\n          ]\n        }\n      ]\n    },\n    \"then\": {\n      \"effect\": \"append\",\n      \"details\": [\n        {\n          \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/idleTimeBeforeShutdown\",\n          \"value\": \"PT60M\"\n        }\n      ]\n    }\n  },\n  \"parameters\": {}\n}\n```\n\n## Schedule automatic start and stop\n\nDefine multiple schedules for auto-shutdown and auto-start. For instance, create a schedule to start at 9 AM and stop at 6 PM from Monday-Thursday, and a second schedule to start at 9 AM and stop at 4 PM for Friday.  You can create a total of four schedules per compute instance.\n\nSchedules can also be defined for [create on behalf of](#create-on-behalf-of-preview) compute instances. You can create a schedule that creates the compute instance in a stopped state. Stopped compute instances are useful when you create a compute instance on behalf of another user.\n\nPrior to a scheduled shutdown, users will see a notification alerting them that the Compute Instance is about to shut down. At that point, the user can choose to dismiss the upcoming shutdown event, if for example they are in the middle of using their Compute Instance.\n\n### Create a schedule in studio\n\n1. [Fill out the form](?tabs=azure-studio#create).\n1. On the second page of the form, open **Show advanced settings**.\n1. Select **Add schedule** to add a new schedule.\n\n    :::image type=\"content\" source=\"media/how-to-create-attach-studio/create-schedule.png\" alt-text=\"Screenshot: Add schedule in advanced settings.\":::\n\n1. Select **Start compute instance** or **Stop compute instance**.\n1. Select the **Time zone**.\n1. Select the **Startup time** or **Shutdown time**.\n1. Select the days when this schedule is active.\n\n    :::image type=\"content\" source=\"media/how-to-create-attach-studio/stop-compute-schedule.png\" alt-text=\"Screenshot: schedule a compute instance to shut down.\":::\n\n1. Select **Add schedule** again if you want to create another schedule.\n\nOnce the compute instance is created, you can view, edit, or add new schedules from the compute instance details section.\n\n\n> [!NOTE]\n> Timezone labels don't account for day light savings. For instance,  (UTC+01:00) Amsterdam, Berlin, Bern, Rome, Stockholm, Vienna is actually UTC+02:00 during day light savings.\n\n### Create a schedule with CLI\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```azurecli\naz ml compute create -f create-instance.yml\n```\n\nWhere the file *create-instance.yml* is:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/computeInstance.schema.json \nname: schedule-example-i\ntype: computeinstance\nsize: STANDARD_DS3_v2\nschedules:\n   compute_start_stop:\n      - action: stop\n        trigger:\n         type: cron\n         start_time: \"2021-03-10T21:21:07\"\n         time_zone: Pacific Standard Time\n         expression: 0 18 * * *\n         \n\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-273.md"
                }
            },
            {
                "page_content": "\n# Manage notebook and terminal sessions\n\nNotebook and terminal sessions run on the compute and maintain your current working state.\n\nWhen you reopen a notebook, or reconnect to a terminal session, you can reconnect to the previous session state (including command history, execution history, and defined variables). However, too many active sessions may slow down the performance of your compute. With too many active sessions, you may find your terminal or notebook cell typing lags, or terminal or notebook command execution may feel slower than expected.\n\nUse the session management panel in Azure Machine Learning studio to help you manage your active sessions and optimize the performance of your compute instance. Navigate to this session management panel from the compute toolbar of either a terminal tab or a notebook tab.\n\n> [!NOTE]\n> For optimal performance, we recommend you don\u2019t keep more than six active sessions - and the fewer the better.\n\n:::image type=\"content\" source=\"media/how-to-manage-compute-sessions/compute-session-management-panel.png\" alt-text=\"Screenshot of compute session management panel.\" lightbox=\"media/how-to-manage-compute-sessions/compute-session-management-panel.png\":::\n\n## Notebook sessions\n\nIn the session management panel, select a linked notebook name in the notebook sessions section to reopen a notebook with its previous state.\n\nNotebook sessions are kept active when you close a notebook tab in the Azure Machine Learning studio. So, when you reopen a notebook you'll have access to previously defined variables and execution state - in this case, you're benefitting from the active notebook session.\n\nHowever, keeping too many active notebook sessions can slow down the performance of your compute. So, you should use the session management panel to shut down any notebook sessions you no longer need.\n\nSelect **Manage active sessions** in the terminal toolbar to open the session management panel and shut down the sessions you no longer need. In the following image, you can see that the tooltip shows the count of active notebook sessions.\n\n:::image type=\"content\" source=\"media/how-to-manage-compute-sessions/notebook-sessions-button.png\" alt-text=\"Screenshot of notebooks sessions button in toolbar.\" lightbox=\"media/how-to-manage-compute-sessions/notebook-sessions-button.png\":::\n\n## Terminal sessions\n\nIn the session management panel, you can select on a terminal link to reopen a terminal tab connected to that previous terminal session.\n\nIn contrast to notebook sessions, terminal sessions are terminated when you close a terminal tab. However, if you navigate away from the Azure Machine Learning studio without closing a terminal tab, the session may remain open. You should be shut down any terminal sessions you no longer need by using the session management panel.\n\nSelect **Manage active sessions** in the terminal toolbar to open the session management panel and shut down the sessions you no longer need. In the following image, you can see that the tooltip shows the count of active terminal sessions.\n\n:::image type=\"content\" source=\"media/how-to-manage-compute-sessions/terminal-sessions-button.png\" alt-text=\"Screenshot of terminal sessions button in toolbar.\" lightbox=\"media/how-to-manage-compute-sessions/terminal-sessions-button.png\":::\n\n## Next steps\n\n* [How to create and manage files in your workspace](how-to-manage-files.md)\n* [Run Jupyter notebooks in your workspace](how-to-run-jupyter-notebooks.md)\n* [Access a compute instance terminal in your workspace](how-to-access-terminal.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-compute-sessions-0.md"
                }
            },
            {
                "page_content": "> While customizing the compute instance, make sure you do not delete the **azureml_py36** or **azureml_py38** conda environments.  Also do not delete **Python 3.6 - AzureML** or **Python 3.8 - AzureML** kernels. These are needed for Jupyter/JupyterLab functionality.\n\nTo add a new Jupyter kernel to the compute instance:\n\n1. Use the terminal window to create a new environment.  For example, the code below creates `newenv`:\n\n    ```shell\n    conda create --name newenv\n    ```\n\n1. Activate the environment.  For example, after creating `newenv`:\n\n    ```shell\n    conda activate newenv\n    ```\n\n1. Install pip and ipykernel package to the new environment and create a kernel for that conda env\n\n    ```shell\n    conda install pip\n    conda install ipykernel\n    python -m ipykernel install --user --name newenv --display-name \"Python (newenv)\"\n    ```\n\nAny of the [available Jupyter Kernels](https://github.com/jupyter/jupyter/wiki/Jupyter-kernels) can be installed.\n\n### Remove added kernels\n\n> [!WARNING]\n> While customizing the compute instance, make sure you do not delete the **azureml_py36** or **azureml_py38** conda environments.  Also do not delete **Python 3.6 - AzureML** or **Python 3.8 - AzureML** kernels. These are needed for Jupyter/JupyterLab functionality.\n\nTo remove an added Jupyter kernel from the compute instance, you must remove the kernelspec, and (optionally) the conda environment. You can also choose to keep the conda environment. You must remove the kernelspec, or your kernel will still be selectable and cause unexpected behavior.\n\nTo remove the kernelspec:\n\n1. Use the terminal window to list and find the kernelspec:\n\n    ```shell\n    jupyter kernelspec list\n    ```\n\n1. Remove the kernelspec, replacing UNWANTED_KERNEL with the kernel you'd like to remove:\n\n    ```shell\n    jupyter kernelspec uninstall UNWANTED_KERNEL\n    ```\n\nTo also remove the conda environment:\n\n1. Use the terminal window to list and find the conda environment:\n\n    ```shell\n    conda env list\n    ```\n\n1. Remove the conda environment, replacing ENV_NAME with the conda environment you'd like to remove:\n\n    ```shell\n    conda env remove -n ENV_NAME\n    ```\n\nUpon refresh, the kernel list in your notebooks view should reflect the changes you have made.\n\n## Manage terminal sessions\n\nTerminal sessions can stay active if terminal tabs are not properly closed. Too many active terminal sessions can impact the performance of your compute instance.\n\nSelect **Manage active sessions** in the terminal toolbar to see a list of all active terminal sessions and shut down the sessions you no longer need.\n\nLearn more about how to manage sessions running on your compute at [Managing notebook and terminal sessions](how-to-manage-compute-sessions.md).\n\n> [!WARNING]\n> Make sure you close any sessions you no longer need to preserve your compute instance's resources and optimize your performance.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-access-terminal-68.md"
                }
            }
        ]
    },
    {
        "query": "How can I get a Containerized Jupyter Notebook to run as code in a pipeline using the v2 CLI?",
        "result": "The tutorial \"Create production ML pipelines with Python SDK v2 in a Jupyter notebook\" explains how to use Azure Machine Learning (Azure ML) to create a production-ready machine learning (ML) project, using AzureML Python SDK v2. You'll learn how to use the AzureML Python SDK v2 to create, validate and run Azure ML pipelines. The tutorial includes how to create reusable Azure ML components, deploy the newly-trained model as an endpoint, and call the Azure ML endpoint for inferencing. The tutorial also includes how to connect to your Azure ML workspace, create Azure ML data assets, and create a cloud-based compute cluster to use for training your model.",
        "context": [
            {
                "page_content": "\n# Tutorial: Create production ML pipelines with Python SDK v2 in a Jupyter notebook\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n> [!NOTE]\n> For a tutorial that uses SDK v1 to build a pipeline, see [Tutorial: Build an Azure Machine Learning pipeline for image classification](v1/tutorial-pipeline-python-sdk.md)\n> \n\nIn this tutorial, you'll use Azure Machine Learning (Azure ML) to create a production ready machine learning (ML) project, using AzureML Python SDK v2.\n\nYou'll learn how to use the AzureML Python SDK v2 to:\n\n> [!div class=\"checklist\"]\n> \n> * Connect to your Azure ML workspace\n> * Create Azure ML data assets\n> * Create reusable Azure ML components\n> * Create, validate and run Azure ML pipelines\n> * Deploy the newly-trained model as an endpoint\n> * Call the Azure ML endpoint for inferencing\n\n## Prerequisites\n\n* Complete the [Quickstart: Get started with Azure Machine Learning](quickstart-create-resources.md) to:\n    * Create a workspace.\n    * Create a cloud-based compute instance to use for your development environment.\n    * Create a cloud-based compute cluster to use for training your model.\n* Complete the [Quickstart: Run Jupyter notebooks in studio](quickstart-run-notebooks.md) to clone the **SDK v2/tutorials** folder.\n\n\n## Open the notebook\n\n1. Open the **tutorials** folder that was cloned into your **Files** section from the [Quickstart: Run Jupyter notebooks in studio](quickstart-run-notebooks.md).\n    \n1. Select the **e2e-ml-workflow.ipynb** file from your **tutorials/azureml-examples/tutorials/e2e-ds-experience/** folder. \n\n    :::image type=\"content\" source=\"media/tutorial-pipeline-python-sdk/expand-folder.png\" alt-text=\"Screenshot shows the open tutorials folder.\":::\n\n1. On the top bar, select the compute instance you created during the  [Quickstart: Get started with Azure Machine Learning](quickstart-create-resources.md) to use for running the notebook.\n\n> [!Important]\n> The rest of this article contains the same content as you see in the notebook.  \n>\n> Switch to the Jupyter Notebook now if you want to run the code while you read along.\n> To run a single code cell in a notebook, click the code cell and hit **Shift+Enter**. Or, run the entire notebook by choosing **Run all** from the top toolbar\n\n## Introduction\n\nIn this tutorial, you'll create an Azure ML pipeline to train a model for credit default prediction. The pipeline handles the data preparation, training and registering the trained model.  You'll then run the pipeline, deploy the model and use it.\n\nThe image below shows the pipeline as you'll see it in the AzureML portal once submitted. It's a rather simple pipeline we'll use to walk you through the AzureML SDK v2.\n\nThe two steps are first data preparation and second training. \n\n:::image type=\"content\" source=\"media/tutorial-pipeline-python-sdk/pipeline-overview.jpg\" alt-text=\"Diagram shows overview of the pipeline.\":::\n\n## Set up the pipeline resources\n\nThe Azure ML framework can be used from CLI, Python SDK, or studio interface. In this example, you'll use the AzureML Python SDK v2 to create a pipeline. \n\nBefore creating the pipeline, you'll set up the resources the pipeline will use:\n\n* The data asset for training\n* The software environment to run the pipeline\n* A compute resource to where the job will run\n\n## Connect to the workspace\n\nBefore we dive in the code, you'll need to connect to your Azure ML workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. \n\n[!Notebook-python[] (~/azureml-examples-main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb?name=import-mlclient)]\n\nIn the next cell, enter your Subscription ID, Resource Group name and Workspace name. To find your Subscription ID:\n1. In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n1. You'll see the values you need for **<SUBSCRIPTION_ID>**, **<RESOURCE_GROUP>**, and **<AML_WORKSPACE_NAME>**.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-pipeline-python-sdk-0.md"
                }
            },
            {
                "page_content": "\n# Use MLflow models in batch deployments\n\n[!INCLUDE [cli v2](../../includes/machine-learning-dev-v2.md)]\n\nIn this article, learn how to deploy your [MLflow](https://www.mlflow.org) model to Azure ML for both batch inference using batch endpoints. Azure Machine Learning supports no-code deployment of models created and logged with MLflow. This means that you don't have to provide a scoring script or an environment.\n\nFor no-code-deployment, Azure Machine Learning \n\n* Provides a MLflow base image/curated environment that contains the required dependencies to run an Azure Machine Learning Batch job.\n* Creates a batch job pipeline with a scoring script for you that can be used to process data using parallelization.\n\n> [!NOTE]\n> For more information about the supported file types in batch endpoints with MLflow, view [Considerations when deploying to batch inference](#considerations-when-deploying-to-batch-inference).\n\n## About this example\n\nThis example shows how you can deploy an MLflow model to a batch endpoint to perform batch predictions. This example uses an MLflow model based on the [UCI Heart Disease Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). The database contains 76 attributes, but we are using a subset of 14 of them. The model tries to predict the presence of heart disease in a patient. It is integer valued from 0 (no presence) to 1 (presence).\n\nThe model has been trained using an `XGBBoost` classifier and all the required preprocessing has been packaged as a `scikit-learn` pipeline, making this model an end-to-end pipeline that goes from raw data to predictions.\n\nThe information in this article is based on code samples contained in the [azureml-examples](https://github.com/azure/azureml-examples) repository. To run the commands locally without having to copy/paste YAML and other files, clone the repo and then change directories to the `cli/endpoints/batch` if you are using the Azure CLI or `sdk/endpoints/batch` if you are using our SDK for Python.\n\n```azurecli\ngit clone https://github.com/Azure/azureml-examples --depth 1\ncd azureml-examples/cli/endpoints/batch\n```\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in the following notebooks. In the cloned repository, open the notebook: [mlflow-for-batch-tabular.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/batch/mlflow-for-batch-tabular.ipynb).\n\n## Prerequisites\n\n[!INCLUDE [basic cli prereqs](../../includes/machine-learning-cli-prereqs.md)]\n\n* You must have a MLflow model. If your model is not in MLflow format and you want to use this feature, you can [convert your custom ML model to MLflow format](how-to-convert-custom-model-to-mlflow.md).\n\n## Steps\n\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch inference over new data:\n\n1. First, let's connect to Azure Machine Learning workspace where we are going to work on.\n\n   # [Azure CLI](#tab/cli)\n   \n   ```azurecli\n   az account set --subscription <subscription>\n   az configure --defaults workspace=<workspace> group=<resource-group> location=<location>\n   ```\n   \n   # [Python](#tab/sdk)\n   \n   The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section, we'll connect to the workspace in which you'll perform deployment tasks.\n   \n   1. Import the required libraries:\n   \n   ```python\n   from azure.ai.ml import MLClient, Input\n   from azure.ai.ml.entities import BatchEndpoint, BatchDeployment, Model, AmlCompute, Data, BatchRetrySettings\n   from azure.ai.ml.constants import AssetTypes, BatchDeploymentOutputAction\n   from azure.identity import DefaultAzureCredential\n   ```\n   \n   2. Configure workspace details and get a handle to the workspace:\n   \n   ```python\n   subscription_id = \"<subscription>\"\n   resource_group = \"<resource-group>\"\n   workspace = \"<workspace>\"\n   \n   ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)\n   ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-mlflow-batch-0.md"
                }
            },
            {
                "page_content": "The machine learning CLI is an extension for the Azure CLI. It provides cross-platform CLI commands for working with Azure Machine Learning. Typically, you use the CLI to automate tasks, such as training a machine learning model.\n\n* [Use the CLI extension for Azure Machine Learning](how-to-configure-cli.md)\n* [MLOps on Azure](https://github.com/Azure/mlops-v2)\n* [Train models](how-to-train-model.md)\n\n## VS Code\n\nYou can use the VS Code extension to run and manage your training jobs. See the [VS Code resource management how-to guide](how-to-manage-resources-vscode.md#experiments) to learn more.\n\n## Next steps\n\nLearn how to [Tutorial: Create production ML pipelines with Python SDK v2 in a Jupyter notebook](tutorial-pipeline-python-sdk.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-train-machine-learning-model-95.md"
                }
            }
        ]
    },
    {
        "query": "how can I upload a csv file to my compute instance?",
        "result": "You can upload a CSV file to your compute instance by using the JupyterLab interface. First, open JupyterLab and navigate to the folder where you want to upload the file. Then, click on the \"Upload\" button in the file browser and select the CSV file you want to upload. Alternatively, you can use the `scp` command to transfer the file from your local machine to the compute instance.",
        "context": [
            {
                "page_content": "    * Append \"**/*.*\" to include all the data in the current container and its subfolders.\n1. Select **Create**.\n1. Select the data asset you created.\n\n### Create a dataset from uploaded data\n\nTo directly upload your data:\n\n1. Select **+ Create**.\n1. Assign a **Name** to your dataset, and optionally a description.\n1. Choose the **Dataset type**:\n    * Select **Tabular** if you use a .csv or .tsv file, where each row contains a response.\n    * Select **File** if you use separate .txt files for each response.\n1. Select **Next**.\n1. Select **From local files**, then select **Next**.\n1. (Optional) Select a datastore; the default uploads to the default blob store (\"workspaceblobstore\") of your Machine Learning workspace.\n1. Select **Next**.\n1. Select **Upload > Upload files** or **Upload > Upload folder** to select the local files or folder(s) to upload.\n1. Find your files or folder in the browser window, then select **Open**.\n1. Continue to use **Upload** until you specify all of your files/folders.\n1. Check the **Overwrite if already exists** box if you want. Verify the list of files/folders.\n1. Select **Next**.\n1. Confirm the details. Select **Back** to modify the settings, or **Create** to create the dataset.\n1. Finally, select the data asset you created.\n\n## Configure incremental refresh\n\n[!INCLUDE [refresh](../../includes/machine-learning-data-labeling-refresh.md)]\n\n> [!NOTE]\n> Projects that use tabular (.csv or .tsv) dataset input have incremental refresh available to them. However, this adds only new tabular files. The refresh will not recognize changes to existing tabular files.\n\n\n## Specify label categories\n\n[!INCLUDE [classes](../../includes/machine-learning-data-labeling-classes.md)]\n\n## Describe the text labeling task\n\n[!INCLUDE [describe](../../includes/machine-learning-data-labeling-describe.md)]\n\n>[!NOTE]\n> Labelers can select the first 9 labels with number keys 1-9.\n\n## Quality control (preview)\n\n[!INCLUDE [describe](../../includes/machine-learning-data-labeling-quality-control.md)]\n\n## Use ML-assisted data labeling\n\nTo accelerate labeling tasks, the **ML-assisted labeling** page can trigger automatic machine learning models. ML-assisted labeling can handle both file (.txt) and tabular (.csv) text data inputs.\nTo use **ML-assisted labeling**:\n\n* Select **Enable ML assisted labeling**.\n* Select the **Dataset language** for the project. This list shows all languages that the [TextDNNLanguages Class](/python/api/azureml-automl-core/azureml.automl.core.constants.textdnnlanguages?view=azure-ml-py&preserve-view=true) supports.\n* Specify a compute target to use. If you don't have a compute target in your workspace, this creates a compute cluster, and adds that compute cluster to your workspace. The cluster is created with a minimum of zero nodes, and it costs nothing when not in use.\n\n### ML-assisted labeling - more information\n\nAt the start of your labeling project, the items are shuffled into a random order to reduce potential bias. However, the trained model reflects any biases present in the dataset. For example, if 80% of your items are of a single class, then approximately 80% of the data used to train the model lands in that class.\n\nTo train the text DNN model that ML-assist uses, the input text per training example is limited to approximately the first 128 words in the document. For tabular input, all text columns are first concatenated before applying this limit. This practical limit allows for the model training to complete in a reasonable amount of time. The actual text in a document (for file input) or set of text columns (for tabular input) can exceed 128 words. The limit only pertains to what the model internally uses during the training process.\n\nThe exact number of labeled items necessary to start assisted labeling isn't a fixed number. This number can vary significantly from one labeling project to another. The variance depends on many factors, including the number of label classes, and the label distribution.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-text-labeling-projects-74.md"
                }
            },
            {
                "page_content": "\n# [Python](#tab/Python-SDK)\n\n```python\nfrom azure.ai.ml.entities import Data\nfrom azure.ai.ml.constants import AssetTypes\n\nmy_path = './data'\n\nmy_data = Data(\n    path=my_path,\n    type=AssetTypes.MLTABLE,\n    name=\"green-sample\",\n    version='1'\n)\n\nml_client.data.create_or_update(my_data)\n```\n\n> [!NOTE]\n> Your local data folder - containing the parquet file and MLTable - will automatically upload to cloud storage (default Azure ML datastore) on asset create.\n\n### Step 4: Create a job\n\nCreate a Python script called `read-mltable.py` in an `src` folder that contains:\n\n```python\n# ./src/read-mltable.py\nimport argparse\nimport mltable\n\n# parse arguments\nparser = argparse.ArgumentParser()\nparser.add_argument('--input', help='mltable artifact to read')\nargs = parser.parse_args()\n\n# load mltable\ntbl = mltable.load(args.input)\n\n# show table\nprint(tbl.show())\n```\n\nTo keep things simple, we only show how to read the table into Pandas, and print the first few records.\n\nYour job will need a Conda file that includes the Python package dependencies. Save that Conda file as `conda_dependencies.yml`:\n\n```yml\n# ./conda_dependencies.yml\ndependencies:\n  - python=3.10\n  - pip=21.2.4\n  - pip:\n      - mltable\n      - azureml-dataprep[pandas]\n```\n\nNext, submit the job:\n\n# [CLI](#tab/cli)\n\nCreate the following job YAML file:\n\n```yml\n# mltable-job.yml\n$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n\ncode: ./src\n\ncommand: python read-mltable.py --input ${{inputs.my_mltable}}\ninputs:\n    my_mltable:\n      type: mltable\n      path: azureml:green-sample:1\n\ncompute: cpu-cluster\n\nenvironment:\n  image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n  conda_file: conda_dependencies.yml\n```\n\nIn the CLI, create the job:\n\n```azurecli\naz ml job create -f mltable-job.yml\n```\n\n# [Python](#tab/Python-SDK)\n\n```python\nfrom azure.ai.ml import MLClient, command, Input\nfrom azure.ai.ml.entities import Environment\nfrom azure.identity import DefaultAzureCredential\n\n# Create a client\nml_client = MLClient.from_config(credential=DefaultAzureCredential())\n\n# get the data asset\ndata_asset = ml_client.data.get(name=\"green-sample\", version=\"1\")\n\njob = command(\n    command=\"python read-mltable.py --input ${{inputs.my_mltable}}\",\n    inputs={\n        \"my_mltable\": Input(type=\"mltable\",path=data_asset.id)\n    },\n    compute=\"cpu-cluster\",\n    environment=Environment(\n        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n        conda_file=\"./conda_dependencies.yml\"\n    ),\n    code=\"./src\"\n)\n\nml_client.jobs.create_or_update(job)\n```\n\n\n## `MLTable` file examples\n\n### Delimited Text (a CSV file)\n\nThis example assumes you have a CSV file stored in the following Azure Data Lake location:\n\n- `abfss://<filesystem>@<account_name>.dfs.core.windows.net/<folder>/<file-name>.csv`\n\n> [!NOTE]\n> You must update the `<>` placeholders for your Azure Data Lake filesystem and account name, along with the path on Azure Data lake to your CSV file.\n\n# [CLI](#tab/cli)\nCreate an `MLTable` file in the `abfss://<filesystem>@<account_name>.dfs.core.windows.net/<folder>/` location:\n    \n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/MLTable.schema.json\n\ntype: mltable\n\npaths:\n  - file: ./<file-name>.csv\n\ntransformations:\n  - read_delimited:\n      delimiter: ',' \n      empty_as_string: false \n      encoding: utf8 \n      header: all_files_same_headers\n      include_path_column: false \n      infer_column_types: true \n      support_multi_line: false\n```\n\nIf you don't already use [Option 1: Directly author `MLTable` in cloud storage with VSCode](#option-1-directly-author-mltable-in-cloud-storage-with-vscode), then you can upload your `MLTable` file with `azcopy`:\n\n```bash\nSOURCE=<local_path-to-mltable-file>\nDEST=https://<account_name>.blob.core.windows.net/<filesystem>/<folder>\nazcopy cp $SOURCE $DEST\n```\n\n# [Python](#tab/Python-SDK)\n\n```python\nimport mltable\nfrom mltable import MLTableHeaders, MLTableFileEncoding\nfrom azure.storage.blob import BlobClient\nfrom azure.identity import DefaultAzureCredential\n\n# update the file name\nmy_path = {\n    'file': './<file_name>.csv'\n}\n\ntbl = mltable.from_delimited_files(\n    paths=[my_path],\n    header=MLTableHeaders.all_files_same_headers,\n    delimiter=',',\n    encoding=MLTableFileEncoding.utf8,\n    empty_as_string=False,\n    include_path_column=False,\n    infer_column_types=True,\n    support_multi_line=False)\n\n# save the table to the local file system\nlocal_folder = \"local\"\ntbl.save(local_folder)\n\n# upload the MLTable file to your storage account so that you have an artifact\nstorage_account_url = \"https://<account_name>.blob.core.windows.net\"\ncontainer_name = \"<filesystem>\"\ndata_folder_on_storage = '<folder>'\n\n# get a blob client using default credential\nblob_client = BlobClient(\n    credential=DefaultAzureCredential(), \n    account_url=storage_account_url, \n    container_name=container_name,\n    blob_name=f'{data_folder_on_storage}/MLTable'\n)\n\n# upload to cloud storage\nwith open(f'{local_folder}/MLTable', \"rb\") as mltable_file:\n    blob_client.upload_blob(mltable_file)\n\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-mltable-530.md"
                }
            },
            {
                "page_content": "| `mode` | string | Mode of how the data should be delivered to the compute target. <br><br> For read-only mount (`ro_mount`), the data will be consumed as a mount path. A folder will be mounted as a folder and a file will be mounted as a file. Azure ML will resolve the input to the mount path. <br><br> For `download` mode the data will be downloaded to the compute target. Azure ML will resolve the input to the downloaded path. <br><br> If you only want the URL of the storage location of the data artifact(s) rather than mounting or downloading the data itself, you can use the `direct` mode. This will pass in the URL of the storage location as the job input. In this case you're fully responsible for handling credentials to access the storage. | `ro_mount`, `download`, `direct` | `ro_mount` |\n\n### Job outputs\n\n| Key | Type | Description | Allowed values | Default value |\n| --- | ---- | ----------- | -------------- | ------------- |\n| `type` | string | The type of job output. For the default `uri_folder` type, the output will correspond to a folder. | `uri_file`, `uri_folder`, `mltable`, `mlflow_model`  | `uri_folder` |\n| `mode` | string | Mode of how output file(s) will get delivered to the destination storage. For read-write mount mode (`rw_mount`) the output directory will be a mounted directory. For upload mode the file(s) written will get uploaded at the end of the job. | `rw_mount`, `upload` | `rw_mount` |\n\n### Identity configurations\n\n#### UserIdentityConfiguration\n\n| Key | Type | Description | Allowed values |\n| --- | ---- | ----------- | -------------- |\n| `type` | const | **Required.** Identity type.  | `user_identity` |\n\n#### ManagedIdentityConfiguration\n\n| Key | Type | Description | Allowed values |\n| --- | ---- | ----------- | -------------- |\n| `type` | const | **Required.** Identity type.  | `managed` or `managed_identity` |\n\n## Remarks\n\nThe `az ml job` command can be used for managing Azure Machine Learning jobs.\n\n## Examples\n\nExamples are available in the [examples GitHub repository](https://github.com/Azure/azureml-examples/tree/main/cli/jobs). Several are shown below.\n\n## YAML: hello sweep\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/sweepJob.schema.json\ntype: sweep\ntrial:\n  command: >-\n    python hello-sweep.py\n    --A ${{inputs.A}}\n    --B ${{search_space.B}}\n    --C ${{search_space.C}}\n  code: src\n  environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\ninputs:\n  A: 0.5\ncompute: azureml:cpu-cluster\nsampling_algorithm: random\nsearch_space:\n  B:\n    type: choice\n    values: [\"hello\", \"world\", \"hello_world\"]\n  C:\n    type: uniform\n    min_value: 0.1\n    max_value: 1.0\nobjective:\n  goal: minimize\n  primary_metric: random_metric\nlimits:\n  max_total_trials: 4\n  max_concurrent_trials: 2\n  timeout: 3600\ndisplay_name: hello-sweep-example\nexperiment_name: hello-sweep-example\ndescription: Hello sweep job example.\n\n```\n\n## YAML: basic Python model hyperparameter tuning\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/sweepJob.schema.json\ntype: sweep\ntrial:\n  code: src\n  command: >-\n    python main.py \n    --iris-csv ${{inputs.iris_csv}}\n    --C ${{search_space.C}}\n    --kernel ${{search_space.kernel}}\n    --coef0 ${{search_space.coef0}}\n  environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\ninputs:\n  iris_csv: \n    type: uri_file\n    path: wasbs://datasets@azuremlexamples.blob.core.windows.net/iris.csv\ncompute: azureml:cpu-cluster\nsampling_algorithm: random\nsearch_space:\n  C:\n    type: uniform\n    min_value: 0.5\n    max_value: 0.9\n  kernel:\n    type: choice\n    values: [\"rbf\", \"linear\", \"poly\"]\n  coef0:\n    type: uniform\n    min_value: 0.1\n    max_value: 1\nobjective:\n  goal: minimize\n  primary_metric: training_f1_score\nlimits:\n  max_total_trials: 20\n  max_concurrent_trials: 10\n  timeout: 7200\ndisplay_name: sklearn-iris-sweep-example\nexperiment_name: sklearn-iris-sweep-example\ndescription: Sweep hyperparemeters for training a scikit-learn SVM on the Iris dataset.\n\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-yaml-job-sweep-197.md"
                }
            }
        ]
    },
    {
        "query": "please provide a sample job yaml",
        "result": "The following is a sample job YAML for an AutoML image object detection task. It includes settings for the search space, sweep settings, and job limits. It also includes an example of how to turn off certain data augmentations using the `advanced_settings` parameter. \n\n```yaml\n# Sample job YAML for AutoML image object detection task\n\n# Search space settings\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\n\n# Sweep settings\nsweep:\n  sampling_algorithm: random\n  early_termination:\n    type: bandit\n    evaluation_interval: 2\n    slack_factor: 0.2\n    delay_evaluation: 6\n\n# Job limits\nlimits:\n  timeout_minutes: 60\n  max_trials: 10\n  max_concurrent_trials: 2\n\n# Advanced settings to turn off certain data augmentations\ntraining_parameters:\n  advanced_settings: >\n    {\"apply_automl_train_augmentations\": false, \"apply_mosaic_for_yolo\": false}\n```",
        "context": [
            {
                "page_content": "The Bandit early termination policy is also used. This policy terminates poor performing trials; that is, those trials that are not within 20% slack of the best performing trial, which significantly saves compute resources.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nsweep:\n  sampling_algorithm: random\n  early_termination:\n    type: bandit\n    evaluation_interval: 2\n    slack_factor: 0.2\n    delay_evaluation: 6\n```\n\n```yaml\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\n```\n\n# [Python SDK](#tab/python)\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=sweep-settings)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=search-space-settings)]\n\n\nOnce the search space and sweep settings are defined, you can then submit the job to train an image model using your training dataset.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nTo submit your AutoML job, you run the following CLI v2 command with the path to your .yml file, workspace name, resource group and subscription ID.\n\n```azurecli\naz ml job create --file ./hello-automl-job-basic.yml --workspace-name [YOUR_AZURE_WORKSPACE] --resource-group [YOUR_AZURE_RESOURCE_GROUP] --subscription [YOUR_AZURE_SUBSCRIPTION]\n```\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nWhen you've configured your AutoML Job to the desired settings, you can submit the job.\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=submit-run)]\n\n\nWhen doing a hyperparameter sweep, it can be useful to visualize the different trials that were tried using the HyperDrive UI. You can navigate to this UI by going to the 'Child jobs' tab in the UI of the main automl_image_job from above, which is the HyperDrive parent job. Then you can go into the 'Child jobs' tab of this one.\n\nAlternatively, here below you can see directly the HyperDrive parent job and navigate to its 'Child jobs' tab:\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n```yaml\nCLI example not available, please use Python SDK.\n```\n\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nhd_job = ml_client.jobs.get(returned_job.name + '_HD')\nhd_job\n```\n\n\n## Register and deploy model\n\nOnce the job completes, you can register the model that was created from the best trial (configuration that resulted in the best primary metric). You can either register the model after downloading or by specifying the azureml path with corresponding jobid.  \n\n### Get the best trial\n\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n```yaml\nCLI example not available, please use Python SDK.\n```\n\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=best_run)] \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-auto-train-image-models-387.md"
                }
            },
            {
                "page_content": "* [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) for image classification\n* [Intersection over union](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) for image classification multilabel\n* [Mean average precision](how-to-understand-automated-ml.md#object-detection-and-instance-segmentation-metrics) for image object detection\n* [Mean average precision](how-to-understand-automated-ml.md#object-detection-and-instance-segmentation-metrics) for image instance segmentation\n    \n### Job limits\n\nYou can control the resources spent on your AutoML Image training job by specifying the `timeout_minutes`, `max_trials` and the `max_concurrent_trials` for the job in limit settings as described in the below example.\n\nParameter | Detail\n-----|----\n`max_trials` |  Parameter for maximum number of trials to sweep. Must be an integer between 1 and 1000. When exploring just the default hyperparameters for a given model architecture, set this parameter to 1. The default value is 1.\n`max_concurrent_trials`| Maximum number of trials that can run concurrently. If specified, must be an integer between 1 and 100.  The default value is 1. <br><br> **NOTE:** <li> The number of concurrent trials is gated on the resources available in the specified compute target. Ensure that the compute target has the available resources for the desired concurrency.  <li> `max_concurrent_trials` is capped at `max_trials` internally. For example, if user sets `max_concurrent_trials=4`, `max_trials=2`, values would be internally updated as `max_concurrent_trials=2`, `max_trials=2`.\n`timeout_minutes`| The amount of time in minutes before the experiment terminates. If none specified, default experiment timeout_minutes is seven days (maximum 60 days)\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nlimits:\n  timeout_minutes: 60\n  max_trials: 10\n  max_concurrent_trials: 2\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=limit-settings)]\n\n\n### Automatically sweeping model hyperparameters (AutoMode)\n\n> [!IMPORTANT]\n> This feature is currently in public preview. This preview version is provided without a service-level agreement. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\n\nIt is generally hard to predict the best model architecture and hyperparameters for a dataset. Also, in some cases the human time allocated to tuning hyperparameters may be limited. For computer vision tasks, you can specify a number of trials and the system will automatically determine the region of the hyperparameter space to sweep. You do not have to define a hyperparameter search space, a sampling method or an early termination policy.\n\n#### Triggering AutoMode\n\nYou can run automatic sweeps by setting `max_trials` to a value greater than 1 in `limits` and by not specifying the search space, sampling method and termination policy. We call this functionality AutoMode; please see an example below.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nlimits:\n  max_trials: 10\n  max_concurrent_trials: 2\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nimage_object_detection_job.set_limits(max_trials=10, max_concurrent_trials=2)\n```\n\nA number of trials between 10 and 20 will likely work well on many datasets. The [time budget](#job-limits) for the AutoML job can still be set, but we recommend doing this only if each trial may take a long time.\n\n> [!Warning]\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-image-models-270.md"
                }
            },
            {
                "page_content": "[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=pass-arguments)]\n\n\n\n## Data augmentation \n\nIn general, deep learning model performance can often improve with more data. Data augmentation is a practical technique to amplify the data size and variability of a dataset which helps to prevent overfitting and improve the model\u2019s generalization ability on unseen data. Automated ML applies different data augmentation techniques based on the computer vision task, before feeding input images to the model. Currently, there's no exposed hyperparameter to control data augmentations. \n\n|Task | Impacted dataset | Data augmentation technique(s) applied |\n|-------|----------|---------|\n|Image classification (multi-class and multi-label) | Training <br><br><br> Validation & Test| Random resize and crop, horizontal flip, color jitter (brightness, contrast, saturation, and hue), normalization using channel-wise ImageNet\u2019s mean and standard deviation <br><br><br>Resize, center crop, normalization |\n|Object detection, instance segmentation| Training <br><br> Validation & Test |Random crop around bounding boxes, expand, horizontal flip, normalization, resize <br><br><br>Normalization, resize\n|Object detection using yolov5| Training <br><br> Validation & Test  |Mosaic, random affine (rotation, translation, scale, shear), horizontal flip <br><br><br> Letterbox resizing|\n\nCurrently the augmentations defined above are applied by default for an Automated ML for image job. To provide control over augmentations, Automated ML for images exposes below two flags to turn-off certain augmentations. Currently, these flags are only supported for object detection and instance segmentation tasks. \n 1. **apply_mosaic_for_yolo:** This flag is only specific to Yolo model. Setting it to False turns off the mosaic data augmentation which is applied at the training time.\n 2. **apply_automl_train_augmentations:** Setting this flag to false turns off the augmentation applied during training time for the object detection and instance segmentation models. For augmentations, see the details in the table above.\n    - For non-yolo object detection model and instance segmentation models, this flag turns off only the first three augmentations i.e., *Random crop around bounding boxes, expand, horizontal flip*. The normalization and resize augmentations are still applied regardless of this flag.\n    - For Yolo model, this flag turns off the random affine and horizontal flip augmentations.\n\nThese two flags are supported via *advanced_settings* under *training_parameters* and can be controlled in the following way.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\ntraining_parameters:\n  advanced_settings: >\n    {\"apply_mosaic_for_yolo\": false}\n```\n```yaml\ntraining_parameters:\n  advanced_settings: >\n    {\"apply_automl_train_augmentations\": false}\n```\n Please note that these two flags are independent of each other and can also be used in combination using the following settings.\n ```yaml\ntraining_parameters:\n  advanced_settings: >\n    {\"apply_automl_train_augmentations\": false, \"apply_mosaic_for_yolo\": false}\n```\n\n# [Python SDK](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n \n```python\nfrom azure.ai.ml import automl\n\nimage_object_detection_job = automl.image_object_detection(...)\n\nimage_object_detection_job.set_training_parameters(\n    ...,\n    advanced_settings='{\"apply_mosaic_for_yolo\": false}'\n)\n```\n\n```python\nfrom azure.ai.ml import automl\n\nimage_object_detection_job = automl.image_object_detection(...)\n\nimage_object_detection_job.set_training_parameters(\n    ...,\n    advanced_settings='{\"apply_automl_train_augmentations\": false}'\n)\n```\n Please note that these two flags are independent of each other and can also be used in combination using the following settings.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-image-models-498.md"
                }
            }
        ]
    },
    {
        "query": "how can I view all my scheduled jobs using the new CLI v2?",
        "result": "You can view all your scheduled jobs using the new CLI v2 by running the command `az ml schedule list`. This will return a list of all the schedules in your workspace, including their name, status, and the pipeline job they are associated with.",
        "context": [
            {
                "page_content": "    :::image type=\"content\" source=\"./media/how-to-schedule-pipeline-job/schedule-detail-overview.png\" alt-text=\"Screenshot of the overview tab in the schedule detail page.\" lightbox= \"./media/how-to-schedule-pipeline-job/schedule-detail-overview.png\":::\n\n- **Job definition**: defines the job triggered by this schedule.\n\n  :::image type=\"content\" source=\"./media/how-to-schedule-pipeline-job/schedule-detail-job-definition.png\" alt-text=\"Screenshot of the job definition tab in the schedule detail page.\" lightbox= \"./media/how-to-schedule-pipeline-job/schedule-detail-job-definition.png\":::\n\n- **Jobs history**: a list of all jobs triggered by this schedule.\n\n :::image type=\"content\" source=\"./media/how-to-schedule-pipeline-job/schedule-detail-jobs-history.png\" alt-text=\"Screenshot of the jobs history tab in the schedule detail page.\" lightbox= \"./media/how-to-schedule-pipeline-job/schedule-detail-jobs-history.png\":::\n\n\n### Update a schedule\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```azurecli\n# <create_schedule>\n# This action will create related resources for a schedule. It will take dozens of seconds to complete.\naz ml schedule create --file cron-schedule.yml --no-wait\n# </create_schedule>\n\n# <show_schedule>\naz ml schedule show -n simple_cron_job_schedule\n# </show_schedule>\n\n# <list_schedule>\naz ml schedule list\n# </list_schedule>\n\n# <update_schedule>\naz ml schedule update -n simple_cron_job_schedule  --set description=\"new description\" --no-wait\n# </update_schedule>\n\n# <disable_schedule>\naz ml schedule disable -n simple_cron_job_schedule --no-wait\n# </disable_schedule>\n\n# <enable_schedule>\naz ml schedule enable -n simple_cron_job_schedule --no-wait\n# </enable_schedule>\n\n# <query_triggered_jobs>\n# query triggered jobs from schedule, please replace the simple_cron_job_schedule to your schedule name\naz ml job list --query \"[?contains(display_name,'simple_cron_schedule')]\" \n# </query_triggered_jobs>\n\n# <delete_schedule>\naz ml schedule delete -n simple_cron_job_schedule\n# </delete_schedule>\n\n```\n\n> [!NOTE]\n> If you would like to update more than just tags/description, it is recomend to use `az ml schedule create --file update_schedule.yml`\n\n# [Python SDK](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\njob_schedule = ml_client.schedules.begin_create_or_update(\n    schedule=job_schedule\n).result()\nprint(job_schedule)\n```\n\n# [studio UI](#tab/ui)\n\n#### Update a new version pipeline to existing schedule\n\n\nOnce you set up a schedule to do retraining or batch inference on production regularly, you may still work on fine tuning or optimizing the model.\n\nWhen you have a new version pipeline job with optimized performance, you can update the new version pipeline to an existing schedule.\n\n1. In the new version pipeline job detail page, select **Schedule** -> **Update to existing schedule**.\n\n     :::image type=\"content\" source=\"./media/how-to-schedule-pipeline-job/update-to-existing-schedule.png\" alt-text=\"Screenshot of the jobs tab with schedule button selected showing update to existing schedule button.\" lightbox= \"./media/how-to-schedule-pipeline-job/update-to-existing-schedule.png\":::\n\n2. Select an existing schedule from the table. \n\n    :::image type=\"content\" source=\"./media/how-to-schedule-pipeline-job/update-select-schedule.png\" alt-text=\"Screenshot of update select schedule showing the select schedule tab.\" lightbox= \"./media/how-to-schedule-pipeline-job/update-select-schedule.png\":::\n\n> [!IMPORTANT]\n> Make sure you select the correct schedule you want to update. Once you finish update, the schedule will trigger different jobs.\n\n3. You can also modify the job inputs/outputs, and run time settings for the future jobs triggered by the schedule.\n\n4. Select **Review + Update** to finish the update process. There will be notification when update is completed.\n\n5. After update is completed, in the schedule detail page, you can view the new job definition.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-schedule-pipeline-job-536.md"
                }
            },
            {
                "page_content": "4. Select **Review + Update** to finish the update process. There will be notification when update is completed.\n\n5. After update is completed, in the schedule detail page, you can view the new job definition.\n\n#### Update in schedule detail page\n\nIn schedule detail page, you can select **Update settings** to update the basic settings and advanced settings (including job input/output and runtime settings) of the schedule.\n\n:::image type=\"content\" source=\"./media/how-to-schedule-pipeline-job/schedule-update-settings.png\" alt-text=\"Screenshot of update settings showing the basic settings tab.\" lightbox= \"./media/how-to-schedule-pipeline-job/schedule-update-settings.png\":::\n\n\n### Disable a schedule\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```azurecli\n# <create_schedule>\n# This action will create related resources for a schedule. It will take dozens of seconds to complete.\naz ml schedule create --file cron-schedule.yml --no-wait\n# </create_schedule>\n\n# <show_schedule>\naz ml schedule show -n simple_cron_job_schedule\n# </show_schedule>\n\n# <list_schedule>\naz ml schedule list\n# </list_schedule>\n\n# <update_schedule>\naz ml schedule update -n simple_cron_job_schedule  --set description=\"new description\" --no-wait\n# </update_schedule>\n\n# <disable_schedule>\naz ml schedule disable -n simple_cron_job_schedule --no-wait\n# </disable_schedule>\n\n# <enable_schedule>\naz ml schedule enable -n simple_cron_job_schedule --no-wait\n# </enable_schedule>\n\n# <query_triggered_jobs>\n# query triggered jobs from schedule, please replace the simple_cron_job_schedule to your schedule name\naz ml job list --query \"[?contains(display_name,'simple_cron_schedule')]\" \n# </query_triggered_jobs>\n\n# <delete_schedule>\naz ml schedule delete -n simple_cron_job_schedule\n# </delete_schedule>\n\n```\n\n# [Python SDK](#tab/python)\n\n```python\njob_schedule = ml_client.schedules.begin_disable(name=schedule_name).result()\njob_schedule.is_enabled\n```\n\n# [studio UI](#tab/ui)\n\nOn the schedule detail page, you can disable the current schedule. You can also disable schedules from the **All schedules** tab.\n\n\n### Enable a schedule\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```azurecli\n# <create_schedule>\n# This action will create related resources for a schedule. It will take dozens of seconds to complete.\naz ml schedule create --file cron-schedule.yml --no-wait\n# </create_schedule>\n\n# <show_schedule>\naz ml schedule show -n simple_cron_job_schedule\n# </show_schedule>\n\n# <list_schedule>\naz ml schedule list\n# </list_schedule>\n\n# <update_schedule>\naz ml schedule update -n simple_cron_job_schedule  --set description=\"new description\" --no-wait\n# </update_schedule>\n\n# <disable_schedule>\naz ml schedule disable -n simple_cron_job_schedule --no-wait\n# </disable_schedule>\n\n# <enable_schedule>\naz ml schedule enable -n simple_cron_job_schedule --no-wait\n# </enable_schedule>\n\n# <query_triggered_jobs>\n# query triggered jobs from schedule, please replace the simple_cron_job_schedule to your schedule name\naz ml job list --query \"[?contains(display_name,'simple_cron_schedule')]\" \n# </query_triggered_jobs>\n\n# <delete_schedule>\naz ml schedule delete -n simple_cron_job_schedule\n# </delete_schedule>\n\n```\n\n# [Python SDK](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\n# Update trigger expression\njob_schedule.trigger.expression = \"10 10 * * 1\"\njob_schedule = ml_client.schedules.begin_create_or_update(\n    schedule=job_schedule\n).result()\nprint(job_schedule)\n```\n\n# [studio UI](#tab/ui)\n\nOn the schedule detail page, you can enable the current schedule. You can also enable schedules from the **All schedules** tab.\n\n\n## Query triggered jobs from a schedule\n\nAll the display name of jobs triggered by schedule will have the display name as <schedule_name>-YYYYMMDDThhmmssZ. For example, if a schedule with a name of named-schedule is created with a scheduled run every 12 hours starting at 6 AM on Jan 1 2021, then the display names of the jobs created will be as follows:\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-schedule-pipeline-job-628.md"
                }
            },
            {
                "page_content": "List continues below.\n\n# [Python SDK](#tab/python)\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nschedule_name = \"simple_sdk_create_schedule_recurrence\"\n\nschedule_start_time = datetime.utcnow()\nrecurrence_trigger = RecurrenceTrigger(\n    frequency=\"day\",\n    interval=1,\n    schedule=RecurrencePattern(hours=10, minutes=[0, 1]),\n    start_time=schedule_start_time,\n    time_zone=TimeZone.UTC,\n)\n\njob_schedule = JobSchedule(\n    name=schedule_name, trigger=recurrence_trigger, create_job=pipeline_job\n)\n```\n\n`RecurrenceTrigger` contains following properties:\n\n- **(Required)** To provide better coding experience, we use `RecurrenceTrigger` for recurrence schedule.\n\nList continues below.\n\n# [studio UI](#tab/ui)\n\n> [!NOTE]\n> Currently, Azure Machine Learning schedules (v2) only support pipeline job.\n>\n>The UI functions are only for Azure Machine Learning schedules (v2), which means v1 schedules created based on published pipelines or pipeline endpoints are not supported in UI - will NOT be listed or accessed in UI. However, you can create v2 schedules for your v1 pipeline jobs using SDK/CLI v2, or UI.\n\nWhen you have a pipeline job with satisfying performance and outputs, you can set up a schedule to automatically trigger this job on a regular basis.\n\n1. In pipeline job detail page, select **Schedule** -> **Create new schedule** to open the schedule creation wizard.  \n\n    :::image type=\"content\" source=\"./media/how-to-schedule-pipeline-job/schedule-entry-button.png\" alt-text=\"Screenshot of the jobs tab with schedule button selecting showing the create new schedule button.\" lightbox= \"./media/how-to-schedule-pipeline-job/schedule-entry-button.png\":::\n\n2. The *Basic settings* of  the schedule creation wizard contain following properties.\n\n    :::image type=\"content\" source=\"./media/how-to-schedule-pipeline-job/create-schedule-basic-settings.png\" alt-text=\"Screenshot of schedule creation wizard showing the basic settings.\" lightbox= \"./media/how-to-schedule-pipeline-job/create-schedule-basic-settings.png\":::\n\n    - **Name**: the unique identifier of the schedule within the workspace.\n    - **Description**: description of the schedule.\n    - **Trigger**: specifies the recurrence pattern of the schedule, including following properties.\n      - **Time zone**: the time zone based on which to calculate the trigger time, by default is (UTC) Coordinated Universal Time.\n      - **Recurrence** or **Cron expression**: select recurrence to specify the recurring pattern. Under **Recurrence**, you can specify the recurrence frequency as minutely, hourly, daily, weekly and monthly.\n      - **Start**: specifies the date from when the schedule becomes active. By default it's the date you create this schedule.\n      - **End**: specifies the date after when the schedule becomes inactive. By default its NONE, which means the schedule will always be active until you manually disable it.\n      - **Tags**: tags of the schedule.\n\n    After you configure the basic settings, you can directly select **Review + Create**, and the schedule will automatically submit jobs according to the recurrence pattern you specified.\n\n> [!NOTE]\n> The following properties that need to be specified apply for CLI and SDK.\n\n- **(Required)** `frequency` specifies the unit of time that describes how often the schedule fires. Can be `minute`, `hour`, `day`, `week`, `month`.\n  \n- **(Required)** `interval` specifies how often the schedule fires based on the frequency, which is the number of time units to wait until the schedule fires again.\n  \n- (Optional) `schedule` defines the recurrence pattern, containing `hours`, `minutes`, and `weekdays`.\n    - When `frequency` is `day`, pattern can specify `hours` and `minutes`.\n    - When `frequency` is `week` and `month`, pattern can specify `hours`, `minutes` and `weekdays`.\n    - `hours` should be an integer or a list, from 0 to 23.\n    - `minutes` should be an integer or a list, from 0 to 59.\n    - `weekdays` can be a string or list from `monday` to `sunday`.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-schedule-pipeline-job-77.md"
                }
            }
        ]
    },
    {
        "query": "how can I cancel a job using the new v2 cli?",
        "result": "To cancel a job using the new v2 CLI, use the following command: \n\n```\naz ml job cancel --ids <job_id>\n```\n\nReplace `<job_id>` with the ID of the job you want to cancel. You can find the job ID in the Azure Machine Learning studio or by running the `az ml job list` command.",
        "context": [
            {
                "page_content": "\n# Monitor and analyze jobs in studio\n\n\nYou can use [Azure Machine Learning studio](https://ml.azure.com) to monitor, organize, and track your jobs for training and experimentation. Your ML job history is an important part of an explainable and repeatable ML development process.\n\nThis article shows how to do the following tasks:\n\n* Add job display name. \n* Create a custom view. \n* Add a job description. \n* Tag and find jobs.\n* Run search over your job history.\n* Cancel or fail jobs.\n* Monitor the job status by email notification.\n* Monitor your job resources (preview)\n \n\n> [!TIP]\n> * If you're looking for information on using the Azure Machine Learning SDK v1 or CLI v1, see [How to track, monitor, and analyze jobs (v1)](./v1/how-to-track-monitor-analyze-runs.md).\n> * If you're looking for information on monitoring training jobs from the CLI or SDK v2, see [Track experiments with MLflow and CLI v2](how-to-use-mlflow-cli-runs.md).\n> * If you're looking for information on monitoring the Azure Machine Learning service and associated Azure services, see [How to monitor Azure Machine Learning](monitor-azure-machine-learning.md).\n>\n> If you're looking for information on monitoring models deployed to online endpoints, see [Monitor online endpoints](how-to-monitor-online-endpoints.md).\n\n## Prerequisites\n\nYou'll need the following items:\n\n* To use Azure Machine Learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n* You must have an Azure Machine Learning workspace. A workspace is created in [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n\n## Job display name \n\nThe job display name is an optional and customizable name that you can provide for your job. To edit the job display name:\n\n1. Navigate to the **Jobs** list. \n\n1. Select the job to edit.\n\n    :::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/select-job.png\" alt-text=\"Screenshot of Jobs list.\":::\n\n1. Select the **Edit** button to edit the job display name.\n\n    :::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/display-name.gif\" alt-text=\"Screenshot of how to edit the display name.\":::\n\n## Custom View \n    \nTo view your jobs in the studio: \n    \n1. Navigate to the **Jobs** tab.\n    \n1. Select either **All experiments** to view all the jobs in an experiment or select **All jobs** to view all the jobs submitted in the Workspace.\n    \nIn the **All jobs'** page, you can filter the jobs list by tags, experiments, compute target and more to better organize and scope your work.  \n    \n1. Make customizations to the page by selecting jobs to compare, adding charts or applying filters. These changes can be saved as a **Custom View** so you can easily return to your work. Users with workspace permissions can edit, or view the custom view. Also, share the custom view with team members for enhanced collaboration by selecting **Share view**.\n\n1. To view the job logs, select a specific job and in the **Outputs + logs** tab, you can find diagnostic and error logs for your job.\n\n    :::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/custom-views-2.gif\" alt-text=\"Screenshot of how to create a custom view.\":::   \n\n## Job description \n\nA job description can be added to a job to provide more context and information to the job. You can also search on these descriptions from the jobs list and add the job description as a column in the jobs list. \n\nNavigate to the **Job Details** page for your job and select the edit or pencil icon to add, edit, or delete descriptions for your job. To persist the changes to the jobs list, save the changes to your existing Custom View or a new Custom View. Markdown format is supported for job descriptions, which allows images to be embedded and deep linking as shown below.\n\n:::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/run-description-2.gif\" alt-text=\"Screenshot of how to create a job description.\"::: \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-track-monitor-analyze-runs-0.md"
                }
            },
            {
                "page_content": ":::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/run-description-2.gif\" alt-text=\"Screenshot of how to create a job description.\"::: \n\n\n## Tag and find jobs\n\nIn Azure Machine Learning, you can use properties and tags to help organize and query your jobs for important information.\n\n* Edit tags\n\n    You can add, edit, or delete job tags from the studio. Navigate to the **Job Details** page for your job and select the edit, or pencil icon to add, edit, or delete tags for your jobs. You can also search and filter on these tags from the jobs list page.\n    \n    :::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/run-tags.gif\" alt-text=\"Screenshot of how to add, edit, or delete job tags.\":::\n    \n\n* Query properties and tags\n\n    You can query jobs within an experiment to return a list of jobs that match specific properties and tags.\n    \n    To search for specific jobs, navigate to the  **All jobs** list. From there you have two options:\n    \n    1. Use the **Add filter** button and select filter on tags to filter your jobs by tag that was assigned to the job(s). <br><br>\n    OR\n    \n    1. Use the search bar to quickly find jobs by searching on the job metadata like the job status, descriptions, experiment names, and submitter name. \n\n## Cancel or fail jobs\n\nIf you notice a mistake or if your job is taking too long to finish, you can cancel the job.\n\nTo cancel a job in the studio, using the following steps:\n\n1. Go to the running pipeline in either the **Jobs** or **Pipelines** section. \n\n1. Select the pipeline job number you want to cancel.\n\n1. In the toolbar, select **Cancel**.\n\n## Monitor the job status by email notification\n\n1. In the [Azure portal](https://portal.azure.com/), in the left navigation bar, select the **Monitor** tab. \n\n1. Select **Diagnostic settings** and then select **+ Add diagnostic setting**.\n\n    ![Screenshot of diagnostic settings for email notification](./media/how-to-track-monitor-analyze-runs/diagnostic-setting.png)\n\n1. In the Diagnostic Setting, \n    1. under the **Category details**, select the **AmlRunStatusChangedEvent**. \n    1. In the **Destination details**, select the **Send to Log Analytics workspace**  and specify the **Subscription** and **Log Analytics workspace**. \n\n    > [!NOTE]\n    > The **Azure Log Analytics Workspace** is a different type of Azure Resource than the **Azure Machine Learning service Workspace**. If there are no options in that list, you can [create a Log Analytics Workspace](../azure-monitor/logs/quick-create-workspace.md). \n    \n    ![Where to save email notification](./media/how-to-track-monitor-analyze-runs/log-location.png)\n\n1. In the **Logs** tab, add a **New alert rule**. \n\n    ![New alert rule](./media/how-to-track-monitor-analyze-runs/new-alert-rule.png)\n\n1. See [how to create and manage log alerts using Azure Monitor](../azure-monitor/alerts/alerts-log.md).\n\n## Monitor your job resources (preview)\n\nNavigate to your job in the studio and select the Monitoring tab. This view provides insights on your job's resources on a 30 day rolling basis. \n\n:::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/monitoring-tab.png\" alt-text=\"Screenshot of Monitoring tab showing resources the selected job has used.\":::\n\n>[!NOTE] \n>This view supports only compute that is managed by AzureML.\n>Jobs with a runtime of less than 5 minutes will not have enough data to populate this view.\n\n\n## Next steps\n\n* To learn how to log metrics for your experiments, see [Log metrics during training jobs](how-to-log-view-metrics.md).\n* To learn how to monitor resources and logs from Azure Machine Learning, see [Monitoring Azure Machine Learning](monitor-azure-machine-learning.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-track-monitor-analyze-runs-69.md"
                }
            },
            {
                "page_content": "  > If you use `sleep infinity`, you will need to manually [cancel the job](./how-to-interactive-jobs.md#end-job) to let go of the compute resource (and stop billing). \n\n5. Select the training applications you want to use to interact with the job.\n\n  :::image type=\"content\" source=\"./media/interactive-jobs/select-training-apps.png\" alt-text=\"Screenshot of selecting a training application for the user to use for a job.\":::\n\n6. Review and create the job.\n\nIf you don't see the above options, make sure you have enabled the \"Debug & monitor your training jobs\" flight via the [preview panel](./how-to-enable-preview-features.md#how-do-i-enable-preview-features).\n\n# [Python SDK](#tab/python)\n1. Define the interactive services you want to use for your job. Make sure to replace `your compute name` with your own value. If you want to use your own custom environment, follow the examples in [this tutorial](how-to-manage-environments-v2.md) to create a custom environment. \n\n   Note that you have to import the `JobService` class from the `azure.ai.ml.entities` package to configure interactive services via the SDKv2. \n\n    ```python\n    command_job = command(...\n        code=\"./src\",  # local path where the code is stored\n        command=\"python main.py\", # you can add a command like \"sleep 1h\" to reserve the compute resource is reserved after the script finishes running\n        environment=\"AzureML-tensorflow-2.7-ubuntu20.04-py38-cuda11-gpu@latest\",\n        compute=\"<name-of-compute>\",\n        services={\n          \"My_jupyterlab\": JobService(\n            job_service_type=\"jupyter_lab\",\n            nodes=\"all\" # For distributed jobs, use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node. Values are \"all\", or compute node index (for ex. \"0\", \"1\" etc.)\n          ),\n          \"My_vscode\": JobService(\n            job_service_type=\"vs_code\",\n            nodes=\"all\"\n          ),\n          \"My_tensorboard\": JobService(\n            job_service_type=\"tensor_board\",\n            nodes=\"all\",\n            properties={\n                \"logDir\": \"output/tblogs\"  # relative path of Tensorboard logs (same as in your training script)\n            }          \n          ),\n          \"My_ssh\": JobService(\n            job_service_type=\"ssh\",\n            sshPublicKeys=\"<add-public-key>\",\n            nodes=\"all\"\n            properties={\n                \"sshPublicKeys\":\"<add-public-key>\"\n            }    \n          ),\n        }\n    )\n\n    # submit the command\n    returned_job = ml_client.jobs.create_or_update(command_job)\n    ```\n\n    The `services` section specifies the training applications you want to interact with.  \n\n    You can put `sleep <specific time>` at the end of your command to specify the amount of time you want to reserve the compute resource. The format follows: \n    * sleep 1s\n    * sleep 1m\n    * sleep 1h\n    * sleep 1d\n\n    You can also use the `sleep infinity` command that would keep the job alive indefinitely. \n    \n    > [!NOTE]\n    > If you use `sleep infinity`, you will need to manually [cancel the job](./how-to-interactive-jobs.md#end-job) to let go of the compute resource (and stop billing). \n\n2. Submit your training job. For more details on how to train with the Python SDKv2, check out this [article](./how-to-train-model.md).\n\n# [Azure CLI](#tab/azurecli)\n\n1. Create a job yaml `job.yaml` with below sample content. Make sure to replace `your compute name` with your own value. If you want to use custom environment, follow the examples in [this tutorial](how-to-manage-environments-v2.md) to create a custom environment. \n    ```dotnetcli\n    code: src \n    command: \n      python train.py \n      # you can add a command like \"sleep 1h\" to reserve the compute resource is reserved after the script finishes running.\n    environment: azureml:AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11-gpu:41\n    compute: azureml:<your compute name>\n    services:\n        my_vs_code:\n          job_service_type: vs_code\n          nodes: all # For distributed jobs, use the `nodes` property to pick which node you want to enable interactive services on. If `nodes` are not selected, by default, interactive applications are only enabled on the head node. Values are \"all\", or compute node index (for ex. \"0\", \"1\" etc.)\n        my_tensor_board:\n          job_service_type: tensor_board\n          properties:\n            logDir: \"output/tblogs\" # relative path of Tensorboard logs (same as in your training script)\n          nodes: all\n        my_jupyter_lab:\n          job_service_type: jupyter_lab\n          nodes: all\n        my_ssh:\n         job_service_type: ssh\n         properties:\n           sshPublicKeys: <paste the entire pub key content>\n         nodes: all\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-interactive-jobs-50.md"
                }
            }
        ]
    },
    {
        "query": "how can I see which jobs are running on a compute cluster?",
        "result": "You can see which jobs are running on a compute cluster by using the Azure Machine Learning studio. In the studio, navigate to the compute cluster you want to check and click on the \"Jobs\" tab. This will show you a list of all the jobs that are currently running on that cluster. You can also see the status of each job, the time it started, and the duration of the job so far.",
        "context": [
            {
                "page_content": "   * See [Compute Instance release notes](azure-machine-learning-ci-image-release-notes.md) for details on image releases.\n\n1. Alternatively, regularly update OS and Python packages.\n\n   * Use Linux package management tools to update the package list with the latest versions.\n     \n     ```bash\n     sudo apt-get update\n     ```\n     \n   * Use Linux package management tools to upgrade packages to the latest versions. Note that package conflicts might occur using this approach.\n     \n     ```bash\n     sudo apt-get upgrade\n     ```\n     \n   * Use Python package management tools to upgrade packages and check for updates.\n     \n     ```bash\n     pip list --outdated\n     ```\n     \nYou may install and run additional scanning software on compute instance to scan for security issues. \n   \n* [Trivy](https://github.com/aquasecurity/trivy) may be used to discover OS and Python package level vulnerabilities. \n* [ClamAV](https://www.clamav.net/) may be used to discover malware and comes pre-installed on compute instance.\n* Defender for Server agent installation is currently not supported. \n* Consider using [customization scripts](./how-to-customize-compute-instance.md) for automation.\n\n### Compute clusters\n\nCompute clusters automatically upgrade to the latest VM image. If the cluster is configured with min nodes = 0, it automatically upgrades nodes to the latest VM image version when all jobs are completed and the cluster reduces to zero nodes.\n* There are conditions in which cluster nodes do not scale down, and as a result are unable to get the latest VM images. \n\n     * Cluster minimum node count may be set to a value greater than 0.\n     * Jobs may be scheduled continuously on your cluster.\n\n* It is your responsibility to scale non-idle cluster nodes down to get the latest OS VM image updates. Azure Machine Learning does not abort any running workloads on compute nodes to issue VM updates. \n\n     * Temporarily change the minimum nodes to zero and allow the cluster to reduce to zero nodes. \n\n### Managed online endpoints\n\n* Managed Online Endpoints automatically receive OS host image updates that include vulnerability fixes. The update frequency of images is at least once a month. \n* Compute nodes get automatically upgraded to the latest VM image version once released. There\u2019s no action required on you.  \n\n### Customer managed Kubernetes clusters\n\n[Kubernetes compute](how-to-attach-kubernetes-anywhere.md) lets you configure Kubernetes clusters to train, inference, and manage models in Azure Machine Learning. \n* Because you manage the environment with Kubenetes, both OS VM vulnerabilities and container image vulnerability management is your responsibility.  \n* Azure Machine Learning frequently publishes new versions of AzureML extension container images into Microsoft Container Registry. It's Microsoft\u2019s responsibility to ensure new image versions are free from vulnerabilities. Vulnerabilities are fixed with [each release](https://github.com/Azure/AML-Kubernetes/blob/master/docs/release-notes.md).\n* When your clusters run jobs without interruption, running jobs may run outdated container image versions. Once you upgrade the amlarc extension to a running cluster, newly submitted jobs will start to use the latest image version. When upgrading the AMLArc extension to its latest version, clean up the old container image versions from the clusters as required. \n* Observability on whether your Azure Arc cluster is running the latest version of AMLArc, you can find via the Azure portal. Under your Arc resource of the type 'Kubernetes - Azure Arc', see 'Extensions' to find the version of the AMLArc extension. \n\n\n## Automated ML and Designer environments\n\nFor code-based training experiences, you control which Azure Machine Learning environment is used. With AutoML and Designer, the environment is encapsulated as part of the service. These types of jobs can run on computes configured by you, allowing for extra controls such as network isolation.\n\n* Automated ML jobs run on environments that layer on top of Azure ML [base docker images](https://github.com/Azure/AzureML-Containers).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-vulnerability-management-70.md"
                }
            },
            {
                "page_content": "For more information on workspaces, see [how to manage resources in VS Code](how-to-manage-resources-vscode.md).\n\n## Create a GPU cluster for training\n\nA compute target is the computing resource or environment where you run training jobs. For more information, see the [Azure Machine Learning compute targets documentation](./concept-compute-target.md).\n\n1. In the Azure Machine Learning view, expand your workspace node.\n1. Right-click the **Compute clusters** node inside your workspace's **Compute** node and select **Create Compute**\n\n    > [!div class=\"mx-imgBorder\"]\n    > ![Create training compute cluster](./media/tutorial-train-deploy-image-classification-model-vscode/create-compute.png)\n\n1. A specification file appears. Configure the specification file with the following options.\n\n    ```yml\n    $schema: https://azuremlschemas.azureedge.net/latest/compute.schema.json\n    name: gpu-cluster\n    type: amlcompute\n    size: Standard_NC12\n    \n    min_instances: 0\n    max_instances: 3\n    idle_time_before_scale_down: 120\n    ```\n\n    The specification file creates a GPU cluster called `gpu-cluster` with at most 3 Standard_NC12 VM nodes that automatically scales down to 0 nodes after 120 seconds of inactivity.\n\n    For more information on VM sizes, see [sizes for Linux virtual machines in Azure](../virtual-machines/sizes.md).\n\n1. Right-click the specification file and select **Azure ML: Execute YAML**.\n\nAfter a few minutes, the new compute target appears in the *Compute > Compute clusters* node of your workspace.\n\n## <a name=\"train-the-model\"></a> Train image classification model\n\nDuring the training process, a TensorFlow model is trained by processing the training data and learning patterns embedded within it for each of the respective digits being classified.\n\nLike workspaces and compute targets, training jobs are defined using resource templates. For this sample, the specification is defined in the *job.yml* file which looks like the following:\n\n```yml\n$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\ncode: src\ncommand: >\n    python train.py\nenvironment: azureml:AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11-gpu:48\ncompute: azureml:gpu-cluster\nexperiment_name: tensorflow-mnist-example\ndescription: Train a basic neural network with TensorFlow on the MNIST dataset.\n```\n\nThis specification file submits a training job called `tensorflow-mnist-example` to the recently created `gpu-cluster` computer target that runs the code in the *train.py* Python script. The environment used is one of the curated environments provided by Azure Machine Learning which contains TensorFlow and other software dependencies required to run the training script. For more information on curated environments, see [Azure Machine Learning curated environments](resource-curated-environments.md).\n\nTo submit the training job:\n\n1. Open the *job.yml* file.\n1. Right-click the file in the text editor and select **Azure ML: Execute YAML**.\n\nAt this point, a request is sent to Azure to run your experiment on the selected compute target in your workspace. This process takes several minutes. The amount of time to run the training job is impacted by several factors like the compute type and training data size. To track the progress of your experiment, right-click the current run node and select **View Job in Azure portal**.\n\nWhen the dialog requesting to open an external website appears, select **Open**.\n\n> [!div class=\"mx-imgBorder\"]\n> ![Track experiment progress](./media/tutorial-train-deploy-image-classification-model-vscode/track-experiment-progress.png)\n\nWhen the model is done training, the status label next to the run node updates to \"Completed\".\n\n## Next steps\n\nIn this tutorial, you learn the following tasks:\n\n> [!div class=\"checklist\"]\n> * Understand the code\n> * Create a workspace\n> * Create a GPU cluster for training\n> * Train a model\n\nFor next steps, see:\n\n* [Create and manage Azure Machine Learning resources using Visual Studio Code](how-to-set-up-vs-code-remote.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-train-deploy-image-classification-model-vscode-62.md"
                }
            },
            {
                "page_content": "- **component_src**: This is the source code directory for a specific component. It contains the source code that will be executed in the component. You can use your preferred language(Python, R...). The code must be executed by a shell command. The source code can take a few inputs from shell command line to control how this step is going to be executed. For example, a training step may take training data, learning rate, number of epochs to control the training process. The argument of a shell command is used to pass inputs and outputs to the code. \n\n Now let's create a pipeline using the `3b_pipeline_with_data` example. We'll explain the detailed meaning of each file in following sections. \n \n First list your available compute resources with the following command: \n\n```azurecli\naz ml compute list\n```\n\nIf you don't have it, create a cluster called `cpu-cluster` by running:\n\n```azurecli\naz ml compute create -n cpu-cluster --type amlcompute --min-instances 0 --max-instances 10\n```\n\nNow, create a pipeline job defined in the pipeline.yml file with the following command. The compute target will be referenced in the pipeline.yml file as `azureml:cpu-cluster`. If your compute target uses a different name, remember to update it in the pipeline.yml file. \n\n```azurecli\naz ml job create --file pipeline.yml\n```\n\nYou should receive a JSON dictionary with information about the pipeline job, including:\n\n| Key                        | Description                                                           |\n|----------------------------|-----------------------------------------------------------------------|\n| `name`                     | The GUID-based name of the job.                                       |\n| `experiment_name`          | The name under which jobs will be organized in Studio.                |\n| `services.Studio.endpoint` | A URL for monitoring and reviewing the pipeline job.                  |\n| `status`                   | The status of the job. This will likely be `Preparing` at this point. |\n\nOpen the `services.Studio.endpoint` URL you'll see a graph visualization of the pipeline looks like below.\n\n:::image type=\"content\" source=\"./media/how-to-create-component-pipelines-cli/pipeline-graph-dependencies.png\" alt-text=\"Screenshot of a graph visualization of the pipeline.\":::\n\n## Understand the pipeline definition YAML\n\nLet's take a look at the pipeline definition in the *3b_pipeline_with_data/pipeline.yml* file.  \n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\n\ndisplay_name: 3b_pipeline_with_data\ndescription: Pipeline with 3 component jobs with data dependencies\n\nsettings:\n  default_compute: azureml:cpu-cluster\n\noutputs:\n  final_pipeline_output:\n    mode: rw_mount\n\njobs:\n  component_a:\n    type: command\n    component: ./componentA.yml\n    inputs:\n      component_a_input: \n        type: uri_folder\n        path: ./data\n\n    outputs:\n      component_a_output: \n        mode: rw_mount\n  component_b:\n    type: command\n    component: ./componentB.yml\n    inputs:\n      component_b_input: ${{parent.jobs.component_a.outputs.component_a_output}}\n    outputs:\n      component_b_output: \n        mode: rw_mount\n  component_c:\n    type: command\n    component: ./componentC.yml\n    inputs:\n      component_c_input: ${{parent.jobs.component_b.outputs.component_b_output}}\n    outputs:\n      component_c_output: ${{parent.outputs.final_pipeline_output}}\n      #  mode: upload\n\n\n```\n\nBelow table describes the most common used fields of pipeline YAML schema. See [full pipeline YAML schema here](reference-yaml-job-pipeline.md).  \n\n|key|description|\n|------|------|\n|type|**Required**. Job type, must be `pipeline` for pipeline jobs.|\n|display_name|Display name of the pipeline job in Studio UI. Editable in Studio UI. Doesn't have to be unique across all jobs in the workspace.|\n|jobs|**Required**. Dictionary of the set of individual jobs to run as steps within the pipeline. These jobs are considered child jobs of the parent pipeline job. In this release, supported job types in pipeline are `command` and `sweep`\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-component-pipelines-cli-42.md"
                }
            }
        ]
    },
    {
        "query": "what are the constraints on naming a workspace in azureml",
        "result": "When naming a workspace in Azure Machine Learning, the name must be unique within the resource group and region. The name can contain only alphanumeric characters and hyphens, and it must start with a letter. The name must also be between 2 and 64 characters long.",
        "context": [
            {
                "page_content": "\n# Model performance and fairness\n\nThis article describes methods that you can use to understand your model performance and fairness in Azure Machine Learning.\n\n## What is machine learning fairness?\n\nArtificial intelligence and machine learning systems can display unfair behavior. One way to define unfair behavior is by its harm, or its impact on people. AI systems can give rise to many types of harm. To learn more, see the [NeurIPS 2017 keynote by Kate Crawford](https://www.youtube.com/watch?v=fMym_BKWQzk).\n\nTwo common types of AI-caused harms are:\n\n- **Harm of allocation**: An AI system extends or withholds opportunities, resources, or information for certain groups. Examples include hiring, school admissions, and lending, where a model might be better at picking good candidates among a specific group of people than among other groups.\n\n- **Harm of quality-of-service**: An AI system doesn't work as well for\u00a0one group of people as it does for another. For example, a voice recognition system might fail to work as well for women as it does for men.\n\nTo reduce unfair behavior in AI systems, you have to assess and mitigate these harms. The *model overview* component of the [Responsible AI dashboard](concept-responsible-ai-dashboard.md) contributes to the identification stage of the model lifecycle by generating model performance metrics for your entire dataset and your identified cohorts of data. It generates these metrics across subgroups identified in terms of sensitive features or sensitive attributes.\n\n>[!NOTE]\n> Fairness is a socio-technical challenge. Quantitative fairness metrics don't capture many aspects of fairness, such as justice and due process. Also, many quantitative fairness metrics can't all be satisfied simultaneously. \n>\n> The goal of the Fairlearn open-source package is to enable humans to assess the impact and mitigation strategies. Ultimately, it's up to the humans who build AI and machine learning models to make trade-offs that are appropriate for their scenarios.\n\nIn this component of the Responsible AI dashboard, fairness is conceptualized through an approach known as *group fairness*. This approach asks: \"Which groups of individuals are at risk for experiencing harm?\" The term *sensitive features* suggests that the system designer should be sensitive to these features when assessing group fairness. \n\nDuring the assessment phase, fairness is quantified through *disparity metrics*. These metrics can evaluate and compare model behavior across groups either as ratios or as differences. The Responsible AI dashboard supports two classes of disparity metrics:\n\n- **Disparity in model performance**: These sets of metrics calculate the disparity (difference) in the values of the selected performance metric across subgroups of data. Here are a few examples:\n\n  - Disparity in accuracy rate\n  - Disparity in error rate\n  - Disparity in precision\n  - Disparity in recall\n  - Disparity in mean absolute error (MAE)  \n\n- **Disparity in selection rate**: This metric contains the difference in selection rate (favorable prediction) among subgroups. An example of this is disparity in loan approval rate. Selection rate means the fraction of data points in each class classified as 1 (in binary classification) or distribution of prediction values (in regression).\n\nThe fairness assessment capabilities of this component come from the [Fairlearn](https://fairlearn.org/) package. Fairlearn provides a collection of model fairness assessment metrics and unfairness mitigation algorithms.\n\n>[!NOTE]\n> A fairness assessment is not a purely technical exercise. The Fairlearn open-source package can identify quantitative metrics to help you assess the fairness of a model, but it won't perform the assessment for you.  You must perform a qualitative analysis to evaluate the fairness of your own models. The sensitive features noted earlier are an example of this kind of qualitative analysis.\n\n## Parity constraints for mitigating unfairness\n\nAfter you understand your model's fairness issues, you can use the mitigation algorithms in the [Fairlearn](https://fairlearn.org/) open-source package to mitigate those issues. These algorithms support a set of constraints on the predictor's behavior called *parity constraints* or criteria. \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-fairness-ml-0.md"
                }
            },
            {
                "page_content": "> Creation of `MLTable` from data in JSONL format is supported using the SDK and CLI only, for this capability. Creating the `MLTable` via UI is not supported at this time. \n\n\n### JSONL schema samples\n\nThe structure of the TabularDataset depends upon the task at hand. For computer vision task types, it consists of the following fields:\n\nField| Description\n---|---\n`image_url`| Contains filepath as a StreamInfo object\n`image_details`|Image metadata information consists of height, width, and format. This field is optional and hence may or may not exist.\n`label`| A json representation of the image label, based on the task type.\n\nThe following is a sample JSONL file for image classification:\n\n```json\n{\n      \"image_url\": \"azureml://subscriptions/<my-subscription-id>/resourcegroups/<my-resource-group>/workspaces/<my-workspace>/datastores/<my-datastore>/paths/image_data/Image_01.png\",\n      \"image_details\":\n      {\n          \"format\": \"png\",\n          \"width\": \"2230px\",\n          \"height\": \"4356px\"\n      },\n      \"label\": \"cat\"\n  }\n  {\n      \"image_url\": \"azureml://subscriptions/<my-subscription-id>/resourcegroups/<my-resource-group>/workspaces/<my-workspace>/datastores/<my-datastore>/paths/image_data/Image_02.jpeg\",\n      \"image_details\":\n      {\n          \"format\": \"jpeg\",\n          \"width\": \"3456px\",\n          \"height\": \"3467px\"\n      },\n      \"label\": \"dog\"\n  }\n  ```\n\n  The following code is a sample JSONL file for object detection:\n\n  ```json\n  {\n      \"image_url\": \"azureml://subscriptions/<my-subscription-id>/resourcegroups/<my-resource-group>/workspaces/<my-workspace>/datastores/<my-datastore>/paths/image_data/Image_01.png\",\n      \"image_details\":\n      {\n          \"format\": \"png\",\n          \"width\": \"2230px\",\n          \"height\": \"4356px\"\n      },\n      \"label\":\n      {\n          \"label\": \"cat\",\n          \"topX\": \"1\",\n          \"topY\": \"0\",\n          \"bottomX\": \"0\",\n          \"bottomY\": \"1\",\n          \"isCrowd\": \"true\",\n      }\n  }\n  {\n      \"image_url\": \"azureml://subscriptions/<my-subscription-id>/resourcegroups/<my-resource-group>/workspaces/<my-workspace>/datastores/<my-datastore>/paths/image_data/Image_02.png\",\n      \"image_details\":\n      {\n          \"format\": \"jpeg\",\n          \"width\": \"1230px\",\n          \"height\": \"2356px\"\n      },\n      \"label\":\n      {\n          \"label\": \"dog\",\n          \"topX\": \"0\",\n          \"topY\": \"1\",\n          \"bottomX\": \"0\",\n          \"bottomY\": \"1\",\n          \"isCrowd\": \"false\",\n      }\n  }\n  ```\n\n\n### Consume data\n\nOnce your data is in JSONL format, you can create training and validation `MLTable` as shown below.\n\n```yaml\npaths:\n  - file: ./train_annotations.jsonl\ntransformations:\n  - read_json_lines:\n        encoding: utf8\n        invalid_lines: error\n        include_path_column: false\n  - convert_column_types:\n      - columns: image_url\n        column_type: stream_info\n```\n\nAutomated ML doesn't impose any constraints on training or validation data size for computer vision tasks. Maximum dataset size is only limited by the storage layer behind the dataset (i.e. blob store). There's no minimum number of images or labels. However, we recommend starting with a minimum of 10-15 samples per label to ensure the output model is sufficiently trained. The higher the total number of labels/classes, the more samples you need per label.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nTraining data is a required parameter and is passed in using the `training_data` key. You can optionally specify another MLtable as a validation data with the `validation_data` key. If no validation data is specified, 20% of your training data will be used for validation by default, unless you pass `validation_data_size` argument with a different value.\n\nTarget column name is a required parameter and used as target for supervised ML task. It's passed in using the `target_column_name` key. For example,\n\n```yaml\ntarget_column_name: label\ntraining_data:\n  path: data/training-mltable-folder\n  type: mltable\nvalidation_data:\n  path: data/validation-mltable-folder\n  type: mltable\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-image-models-92.md"
                }
            },
            {
                "page_content": "After you understand your model's fairness issues, you can use the mitigation algorithms in the [Fairlearn](https://fairlearn.org/) open-source package to mitigate those issues. These algorithms support a set of constraints on the predictor's behavior called *parity constraints* or criteria. \n\nParity constraints require some aspects of the predictor's behavior to be comparable across the groups that sensitive features define (for example, different races). The mitigation algorithms in the Fairlearn open-source package use such parity constraints to mitigate the observed fairness issues.\n\n>[!NOTE]\n> The unfairness mitigation algorithms in the Fairlearn open-source package can provide suggested mitigation strategies to reduce unfairness in a machine learning model, but those strategies don't eliminate unfairness. Developers might need to consider other parity constraints or criteria for their machine learning models. Developers who use Azure Machine Learning must determine for themselves if the mitigation sufficiently reduces unfairness in their intended use and deployment of machine learning models.  \n\nThe Fairlearn package supports the following types of parity constraints:\n\n|Parity constraint  | Purpose  |Machine learning task  |\n|---------|---------|---------|\n|Demographic parity     |  Mitigate allocation harms | Binary classification, regression |\n|Equalized odds  | Diagnose allocation and quality-of-service harms | Binary classification        |\n|Equal opportunity | Diagnose allocation and quality-of-service harms | Binary classification        |\n|Bounded group loss     |  Mitigate quality-of-service harms | Regression |\n\n## Mitigation algorithms\n\nThe Fairlearn open-source package provides two types of unfairness mitigation algorithms:\n\n- **Reduction**: These algorithms take a standard black-box machine learning estimator (for example, a LightGBM model) and generate a set of retrained models by using a sequence of reweighted training datasets. \n\n  For example, applicants of a certain gender might be upweighted or downweighted to retrain models and reduce disparities across gender groups. Users can then pick a model that provides the best trade-off between accuracy (or another performance metric) and disparity, based on their business rules and cost calculations.  \n- **Post-processing**: These algorithms take an existing classifier and a sensitive feature as input. They then derive a transformation of the classifier's prediction to enforce the specified fairness constraints. The biggest advantage of one post-processing algorithm, threshold optimization, is its simplicity and flexibility because it doesn't need to retrain the model.\n\n| Algorithm | Description | Machine learning task | Sensitive features | Supported parity constraints | Algorithm type |\n| --- | --- | --- | --- | --- | --- |\n| `ExponentiatedGradient` | Black-box approach to fair classification described in [A Reductions Approach to Fair Classification](https://arxiv.org/abs/1803.02453). | Binary classification | Categorical | Demographic parity, equalized odds| Reduction |\n| `GridSearch` | Black-box approach described in [A Reductions Approach to Fair Classification](https://arxiv.org/abs/1803.02453).| Binary classification | Binary | Demographic parity, equalized odds | Reduction |\n| `GridSearch` | Black-box approach that implements a grid-search variant of fair regression with the algorithm for bounded group loss described in [Fair Regression: Quantitative Definitions and Reduction-based Algorithms](https://arxiv.org/abs/1905.12843). | Regression | Binary | Bounded group loss| Reduction |\n| `ThresholdOptimizer` | Postprocessing algorithm based on the paper [Equality of Opportunity in Supervised Learning](https://arxiv.org/abs/1610.02413). This technique takes as input an existing classifier and a sensitive feature. Then, it derives a monotone transformation of the classifier's prediction to enforce the specified parity constraints. | Binary classification | Categorical | Demographic parity, equalized odds| Post-processing |\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-fairness-ml-44.md"
                }
            }
        ]
    },
    {
        "query": "how many metrics names can I use in a run?",
        "result": "There is no limit to the number of metric names you can use in a run. You can use any number of metric names that you want.",
        "context": [
            {
                "page_content": ">  * [Many models and hiearchical time series forecasting training (preview)](how-to-auto-train-forecast.md)\n>  * [Forecasting tasks where deep learning neural networks (DNN) are enabled](how-to-auto-train-forecast.md#enable-deep-learning)\n>  * [Automated ML jobs from local computes or Azure Databricks clusters](how-to-configure-auto-train.md#compute-to-run-experiment)\n\nTo view the test job metrics of the recommended model,\n \n1. Navigate to the **Models** page, select the best model. \n1. Select the **Test results (preview)** tab. \n1. Select the job you want, and view the **Metrics** tab.\n    ![Test results tab of automatically tested, recommended model](./media/how-to-use-automated-ml-for-ml-models/test-best-model-results.png)\n    \nTo view the test predictions used to calculate the test metrics, \n\n1. Navigate to the bottom of the page and select the link under **Outputs dataset** to open the dataset. \n1. On the **Datasets** page, select the **Explore** tab to view the predictions from the test job.\n    1. Alternatively, the prediction file can also be viewed/downloaded from the **Outputs + logs** tab, expand the **Predictions** folder to locate your `predicted.csv` file.\n\nAlternatively, the predictions file can also be viewed/downloaded from the Outputs + logs tab, expand Predictions folder to locate your predictions.csv file.\n\nThe model test job generates the predictions.csv file that's stored in the default datastore created with the workspace. This datastore is visible to all users with the same subscription. Test jobs are not recommended for scenarios if any of the information used for or created by the test job needs to remain private.\n\n## Test an existing automated ML model (preview)\n\n>[!IMPORTANT]\n> Testing your models with a test dataset to evaluate generated models is a preview feature. This capability is an [experimental](/python/api/overview/azure/ml/#stable-vs-experimental) preview feature, and may change at any time.\n\n> [!WARNING]\n> This feature is not available for the following automated ML scenarios\n>  * [Computer vision tasks](how-to-auto-train-image-models.md)\n>  * [Many models and hiearchical time series forecasting training (preview)](how-to-auto-train-forecast.md)\n>  * [Forecasting tasks where deep learning neural networks (DNN) are enabled](how-to-auto-train-forecast.md#enable-deep-learning)\n>  * [Automated ML runs from local computes or Azure Databricks clusters](how-to-configure-auto-train.md#compute-to-run-experiment)\n\nAfter your experiment completes, you can test the model(s) that automated ML generates for you. If you want to test a different automated ML generated model, not the recommended model, you can do so with the following steps. \n\n1. Select an existing automated ML experiment job.  \n1. Navigate to the **Models** tab of the job and select the completed model you want to test.\n1. On the model **Details** page, select the **Test model(preview)** button to open the **Test model** pane.\n1. On the **Test model** pane, select the compute cluster and a test dataset you want to use for your test job. \n1. Select the **Test** button. The schema of the test dataset should match the training dataset, but the **target column** is optional.\n1. Upon successful creation of model test job, the **Details** page displays a success message. Select the **Test results** tab to see the progress of the job.\n\n1. To view the results of the test job, open the **Details** page and follow the steps in the [view results of the remote test job](#view-remote-test-job-results-preview) section. \n\n    ![Test model form](./media/how-to-use-automated-ml-for-ml-models/test-model-form.png)\n    \n\n## Model explanations (preview)\n\nTo better understand your model, you can see which data features (raw or engineered) influenced the model's predictions with the model explanations dashboard. \n\nThe model explanations dashboard provides an overall analysis of the trained model along with its predictions and explanations. It also lets you drill into an individual data point and its individual feature importance. [Learn more about the explanation dashboard visualizations](how-to-machine-learning-interpretability-aml.md#visualizations).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-automated-ml-for-ml-models-204.md"
                }
            },
            {
                "page_content": "\n# Evaluate automated machine learning experiment results\n\nIn this article, learn how to evaluate and compare models trained by your automated machine learning (automated ML) experiment. Over the course of an automated ML experiment, many jobs are created and each job creates a model. For each model, automated ML generates evaluation metrics and charts that help you measure the model's performance. \n\nFor example, automated ML generates the following charts based on experiment type.\n\n| Classification| Regression/forecasting |\n| ----------------------------------------------------------- | --------------------------------------------------------|\n| [Confusion matrix](#confusion-matrix)                       | [Residuals histogram](#residuals)                       |\n| [Receiver operating characteristic (ROC) curve](#roc-curve) | [Predicted vs. true](#predicted-vs-true)                |\n| [Precision-recall (PR) curve](#precision-recall-curve)      | [Forecast horizon (preview)](#forecast-horizon-preview) |\n| [Lift curve](#lift-curve)                                   |                                                         |\n| [Cumulative gains curve](#cumulative-gains-curve)           |                                                         |\n| [Calibration curve](#calibration-curve)                     |                     \n\n\n## Prerequisites\n\n- An Azure subscription. (If you don't have an Azure subscription, [create a free account](https://azure.microsoft.com/free/) before you begin)\n- An Azure Machine Learning experiment created with either:\n  - The [Azure Machine Learning studio](how-to-use-automated-ml-for-ml-models.md) (no code required)\n  - The [Azure Machine Learning Python SDK](how-to-configure-auto-train.md)\n\n## View job results\n\nAfter your automated ML experiment completes, a history of the jobs can be found via:\n  - A browser with [Azure Machine Learning studio](https://ml.azure.com)\n  - A Jupyter notebook using the [JobDetails Jupyter widget](/python/api/azureml-widgets/azureml.widgets.rundetails)\n\nThe following steps and video, show you how to view the run history and model evaluation metrics and charts in the studio:\n\n1. [Sign into the studio](https://ml.azure.com/) and navigate to your workspace.\n1. In the left menu, select **Runs**.\n1. Select your experiment from the list of experiments.\n1. In the table at the bottom of the page, select an automated ML job.\n1. In the **Models** tab, select the **Algorithm name** for the model you want to evaluate.\n1. In the **Metrics** tab, use the checkboxes on the left to view metrics and charts.\n\n## Classification metrics\n\nAutomated ML calculates performance metrics for each classification model generated for your experiment. These metrics are based on the scikit learn implementation. \n\nMany classification metrics are defined for binary classification on two classes, and require averaging over classes to produce one score for multi-class classification. Scikit-learn provides several averaging methods, three of which automated ML exposes: **macro**, **micro**, and **weighted**.\n\n- **Macro** - Calculate the metric for each class and take the unweighted average\n- **Micro** - Calculate the metric globally by counting the total true positives, false negatives, and false positives (independent of classes).\n- **Weighted** - Calculate the metric for each class and take the weighted average based on the number of samples per class.\n\nWhile each averaging method has its benefits, one common consideration when selecting the appropriate method is class imbalance. If classes have different numbers of samples, it might be more informative to use a macro average where minority classes are given equal weighting to majority classes. Learn more about [binary vs multiclass metrics in automated ML](#binary-vs-multiclass-classification-metrics). \n\nThe following table summarizes the model performance metrics that automated ML calculates for each classification model generated for your experiment. For more detail, see the scikit-learn documentation linked in the **Calculation** field of each metric. \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-understand-automated-ml-0.md"
                }
            },
            {
                "page_content": "Repeat the necessary steps to load this future data to a data frame and then run `best_run.forecast_quantiles(test_dataset)` to predict future values.\n\n> [!NOTE]\n> In-sample predictions are not supported for forecasting with automated ML when `target_lags` and/or `target_rolling_window_size` are enabled.\n\n## Forecasting at scale\n\n[!INCLUDE [sdk v1](../../includes/machine-learning-sdk-v1.md)]\n\n> [!IMPORTANT]\n> Many models and hierarchical time series are currently only supported in AzureML v1. Support for AzureML v2 is forthcoming.\n\nThere are scenarios where a single machine learning model is insufficient and multiple machine learning models are needed. For instance, predicting sales for each individual store for a brand, or tailoring an experience to individual users. Building a model for each instance can lead to improved results on many machine learning problems. \n\nGrouping is a concept in time series forecasting that allows time series to be combined to train an individual model per group. This approach can be particularly helpful if you have time series that require smoothing, filling or entities in the group that can benefit from history or trends from other entities. Many models and hierarchical time series forecasting are solutions powered by automated machine learning for these large scale forecasting scenarios. \n\n### Many models\n\nThe Azure Machine Learning many models solution with automated machine learning allows users to train and manage millions of models in parallel. The Many Models Solution Accelerator uses [Azure Machine Learning pipelines](concept-ml-pipelines.md) to train the model. Specifically, a [Pipeline](/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline%28class%29) object and [ParalleRunStep](/python/api/azureml-pipeline-steps/azureml.pipeline.steps.parallelrunstep) are used and require specific configuration parameters set through the [ParallelRunConfig](/python/api/azureml-pipeline-steps/azureml.pipeline.steps.parallelrunconfig). \n\n\nThe following diagram shows the workflow for the many models solution. \n\n![Many models concept diagram](./media/how-to-auto-train-forecast/many-models.svg)\n\nThe following code demonstrates the key parameters users need to set up their many models run. See the [Many Models- Automated ML notebook](https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/forecasting-many-models/auto-ml-forecasting-many-models.ipynb) for a many models forecasting example \n\n```python\nfrom azureml.train.automl.runtime._many_models.many_models_parameters import ManyModelsTrainParameters\n\npartition_column_names = ['Store', 'Brand']\nautoml_settings = {\"task\" : 'forecasting',\n                   \"primary_metric\" : 'normalized_root_mean_squared_error',\n                   \"iteration_timeout_minutes\" : 10, #This needs to be changed based on the dataset. Explore how long training is taking before setting this value \n                   \"iterations\" : 15,\n                   \"experiment_timeout_hours\" : 1,\n                   \"label_column_name\" : 'Quantity',\n                   \"n_cross_validations\" : \"auto\", # Could be customized as an integer\n                   \"cv_step_size\" : \"auto\", # Could be customized as an integer\n                   \"time_column_name\": 'WeekStarting',\n                   \"max_horizon\" : 6,\n                   \"track_child_runs\": False,\n                   \"pipeline_fetch_max_batch_size\": 15,}\n\nmm_paramters = ManyModelsTrainParameters(automl_settings=automl_settings, partition_column_names=partition_column_names)\n\n```\n\n### Hierarchical time series forecasting\n\nIn most applications, customers have a need to understand their forecasts at a macro and micro level of the business; whether that is predicting sales of products at different geographic locations, or understanding the expected workforce demand for different organizations at a company. The ability to train a machine learning model to intelligently forecast on hierarchy data is essential. \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-forecast-439.md"
                }
            }
        ]
    },
    {
        "query": "How can I start an AutoML job from the v2 CLI?",
        "result": "To start an AutoML job from the v2 CLI, you can use the `az ml job create` command with the `--file` parameter to specify the YAML configuration file for the job. The YAML file should include the necessary information such as the task type, training and validation data, compute target, and training parameters. For NLP text NER jobs, the YAML file should follow the schema defined in the Azure Machine Learning documentation.",
        "context": [
            {
                "page_content": "In custom environments, you're responsible for setting up your environment and installing packages or any other dependencies that your training or scoring script needs on the compute. Azure ML allows you to create your own environment using\n\n* A docker image\n* A base docker image with a conda YAML to customize further\n* A docker build context\n\n### Create an Azure ML custom environment\n\n### [Azure CLI](#tab/cli)\n\nTo create an environment using CLI v2, use the following command:\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```bash\naz ml environment create --file my_environment.yml\n```\nFor more information, see [environment YAML schema](reference-yaml-environment.md).\n\n\n\n### [Python SDK](#tab/sdk)\n\nTo create an environment using Python SDK v2, you can use the following code:\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nmy_env = Environment(\n    image=\"pytorch/pytorch:latest\", # base image to use\n    name=\"docker-image-example\", # name of the model\n    description=\"Environment created from a Docker image.\",\n)\n\nml_client.environments.create_or_update(my_env) # use the MLClient to connect to workspace and create/register the environment\n```\n\nThis [Jupyter notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/assets/environment/environment.ipynb) shows more ways to create custom environments using SDK v2.\n\n\n## Data\n\nAzure Machine Learning allows you to work with different types of data:\n\n* URIs (a location in local/cloud storage)\n  * `uri_folder`\n  * `uri_file`\n* Tables (a tabular data abstraction)\n  * `mltable`\n* Primitives\n  * `string`\n  * `boolean`\n  * `number`\n\nFor most scenarios, you'll use URIs (`uri_folder` and `uri_file`) - a location in storage that can be easily mapped to the filesystem of a compute node in a job by either mounting or downloading the storage to the node.\n\n`mltable` is an abstraction for tabular data that is to be used for AutoML Jobs, Parallel Jobs, and some advanced scenarios. If you're just starting to use Azure Machine Learning and aren't using AutoML, we strongly encourage you to begin with URIs.\n\n## Component\n\nAn Azure Machine Learning [component](concept-component.md) is a self-contained piece of code that does one step in a machine learning pipeline. Components are the building blocks of advanced machine learning pipelines. Components can do tasks such as data processing, model training, model scoring, and so on. A component is analogous to a function - it has a name, parameters, expects input, and returns output. \n\n## Next steps\n\n* [How to upgrade from v1 to v2](how-to-migrate-from-v1.md)\n* [Train models with the v2 CLI and SDK](how-to-train-model.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-azure-machine-learning-v2-197.md"
                }
            },
            {
                "page_content": "\n# What is automated machine learning (AutoML)?\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n> [!div class=\"op_single_selector\" title1=\"Select the version of the Azure Machine Learning Python SDK you are using:\"]\n> * [v1](./v1/concept-automated-ml-v1.md)\n> * [v2 (current version)](concept-automated-ml.md)\n\nAutomated machine learning, also referred to as automated ML or AutoML, is the process of automating the time-consuming, iterative tasks of machine learning model development. It allows data scientists, analysts, and developers to build ML models with high scale, efficiency, and productivity all while sustaining model quality. Automated ML in Azure Machine Learning is based on a breakthrough from our [Microsoft Research division](https://www.microsoft.com/research/project/automl/).\n\n* For code-experienced customers, [Azure Machine Learning Python SDK](https://aka.ms/sdk-v2-install).  Get started with [Tutorial: Train an object detection model (preview) with AutoML and Python](tutorial-auto-train-image-models.md).\n\n\n## How does AutoML work?\n\nDuring training, Azure Machine Learning creates a number of pipelines in parallel that try different algorithms and parameters for you. The service iterates through ML algorithms paired with feature selections, where each iteration produces a model with a training score. The better the score for the metric you want to optimize for, the better the model is considered to \"fit\" your data.  It will stop once it hits the exit criteria defined in the experiment. \n\nUsing **Azure Machine Learning**, you can design and run your automated ML training experiments with these steps:\n\n1. **Identify the ML problem** to be solved: classification, forecasting, regression, computer vision or NLP.\n\n1. **Choose whether you want a code-first experience or a no-code studio web experience**: Users who prefer a code-first experience can use the [AzureML SDKv2](how-to-configure-auto-train.md) or the [AzureML CLIv2](how-to-train-cli.md). Get started with [Tutorial: Train an object detection model with AutoML and Python](tutorial-auto-train-image-models.md). Users who prefer a limited/no-code experience can use the [web interface](how-to-use-automated-ml-for-ml-models.md) in Azure Machine Learning studio at [https://ml.azure.com](https://ml.azure.com/).  Get started with [Tutorial: Create a classification model with automated ML in Azure Machine Learning](tutorial-first-experiment-automated-ml.md).\n    \n1. **Specify the source of the labeled training data**: You can bring your data to AzureML in [many different ways](concept-data.md).\n\n1. **Configure the automated machine learning parameters** that determine how many iterations over different models, hyperparameter settings, advanced preprocessing/featurization, and what metrics to look at when determining the best model.  \n1. **Submit the training job.**\n\n1. **Review the results** \n\nThe following diagram illustrates this process. \n![Automated Machine learning](./media/concept-automated-ml/automl-concept-diagram2.png)\n\n\nYou can also inspect the logged job information, which [contains metrics](how-to-understand-automated-ml.md) gathered during the job. The training job produces a Python serialized object (`.pkl` file) that contains the model and data preprocessing.\n\nWhile model building is automated, you can also [learn how important or relevant features are](./v1/how-to-configure-auto-train-v1.md#explain) to the generated models.\n\n## When to use AutoML: classification, regression, forecasting, computer vision & NLP\n\nApply automated ML when you want Azure Machine Learning to train and tune a model for you using the target metric you specify. Automated ML democratizes the machine learning model development process, and empowers its users, no matter their data science expertise, to identify an end-to-end machine learning pipeline for any problem.\n\nML professionals and developers across industries can use automated ML to:\n+ Implement ML solutions without extensive programming knowledge\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-automated-ml-0.md"
                }
            },
            {
                "page_content": "\n# CLI (v2) Automated ML text NER job YAML schema\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n[!INCLUDE [schema note](../../includes/machine-learning-preview-old-json-schema-note.md)]\n\nEvery Azure Machine Learning entity has a schematized YAML representation. You can create a new entity from a YAML configuration file with a `.yml` or `.yaml` extension.\n\nThis article provides a reference for some syntax concepts you will encounter while configuring these YAML files for NLP text NER jobs.\n\nThe source JSON schema can be found at https://azuremlsdk2.blob.core.windows.net/preview/0.0.1/autoMLNLPTextNERJob.schema.json\n\n## YAML syntax\n\n| Key | Type | Description | Allowed values | Default value |\n| --- | ---- | ----------- | -------------- | ------------- |\n| `$schema` | string | Represents the location/url to load the YAML schema. If the user uses the Azure Machine Learning VS Code extension to author the YAML file, including `$schema` at the top of the file enables the user to invoke schema and resource completions. | | |\n| `type` | const | **Required.** The type of job. | `automl` | `automl` |\n| `task` | const | **Required.** The type of AutoML task. <br> Task description for NER: <br> There are multiple possible tags for tokens in sequences. The task is to predict the tags for all the tokens for each sequence. For example, extracting domain-specific entities from unstructured text, such as contracts or financial documents. | `text_ner` |  |\n| `name` | string | Name of the job. Must be unique across all jobs in the workspace. If omitted, Azure ML will autogenerate a GUID for the name. | | |\n| `display_name` | string | Display name of the job in the studio UI. Can be non-unique within the workspace. If omitted, Azure ML will autogenerate a human-readable adjective-noun identifier for the display name. | | |\n| `experiment_name` | string | Experiment name to organize the job under. Each job's run record will be organized under the corresponding experiment in the studio's \"Experiments\" tab. If omitted, Azure ML will default it to the name of the working directory where the job was created. | | |\n| `description` | string | Description of the job. | | |\n| `tags` | object | Dictionary of tags for the job. | | |\n| `compute` | string | Name of the compute target to execute the job on. To reference an existing compute in the workspace, we use syntax: `azureml:<compute_name>` | | |\n| `log_verbosity` | number | Different levels of log verbosity. |`not_set`, `debug`, `info`, `warning`, `error`, `critical` | `info` |\n| `primary_metric` | string |  The metric that AutoML will optimize for model selection. |`accuracy`| `accuracy` |\n| `training_data` | object |  **Required.** The data to be used within the job. Unlike multi-class or multi-label, which takes .csv format datasets, named entity recognition requires CoNLL format. The file must contain exactly two columns and in each row, the token and the label is separated by a single space. See [NER](./how-to-auto-train-nlp-models.md?tabs=cli#named-entity-recognition-ner) section for more detail.| | | \n| `validation_data` | object | **Required.** The validation data to be used within the job. <br> - The file should not start with an empty line <br> - Each line must be an empty line, or follow format `{token}` `{label}`, where there is exactly one space between the token and the label and no white space after the label <br> - All labels must start with I-, B-, or be exactly O. Case sensitive <br> - Exactly one empty line between two samples <br> - Exactly one empty line at the end of the file <br> See [data validation](./how-to-auto-train-nlp-models.md?tabs=cli#data-validation) section for more detail. | | |\n| `limits` | object | Dictionary of limit configurations of the job. Parameters in this section: `max_concurrent_trials`, `max_nodes`, `max_trials`, `timeout_minutes`, `trial_timeout_minutes`. See [limits](#limits) for detail.| | |\n| `training_parameters` | object | Dictionary containing training parameters for the job. Provide an object that has keys as listed in following sections. <br> For more information, see [supported hyperparameters](./how-to-auto-train-nlp-models.md?tabs=cli#supported-hyperparameters) section| | |\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-automl-nlp-cli-ner-0.md"
                }
            }
        ]
    },
    {
        "query": "how do I deploy an MLFLow model using the v2 cli?",
        "result": "To deploy an MLflow model using the v2 CLI, you can use the `az ml online-endpoint create` command. You will need to specify the name of the endpoint, the path to the model, and the instance type and count. You can also specify the deployment configuration in a YAML file. Once the endpoint is created, you can create a deployment for the model using the `az ml online-deployment create` command. You will need to specify the name of the deployment, the endpoint name, and the path to the deployment configuration YAML file.",
        "context": [
            {
                "page_content": "\n    Let's write this configuration into a `JSON` file:\n\n    ```python\n    endpoint_config_path = \"endpoint_config.json\"\n    with open(endpoint_config_path, \"w\") as outfile:\n        outfile.write(json.dumps(endpoint_config))\n    ```\n\n    # [Studio](#tab/studio)\n\n    *You will perform this step in the deployment stage.*\n\n1. Let's create the endpoint:\n    \n    # [Azure CLI](#tab/cli)\n    \n```azurecli\nset -e\n\n# <set_endpoint_name>\nexport ENDPOINT_NAME=\"<YOUR_ENDPOINT_NAME>\"\n# </set_endpoint_name>\n\n#  endpoint name\nexport ENDPOINT_NAME=endpt-ncd-`echo $RANDOM`\nAML_MODEL_NAME=mir-sample-sklearn-ncd-model\necho $AML_MODEL_NAME\n\n# <create_endpoint>\naz ml online-endpoint create --name $ENDPOINT_NAME -f endpoints/online/ncd/create-endpoint.yaml\n# </create_endpoint>\n\n# check if create was successful\nendpoint_status=`az ml online-endpoint show --name $ENDPOINT_NAME --query \"provisioning_state\" -o tsv`\necho $endpoint_status\nif [[ $endpoint_status == \"Succeeded\" ]]\nthen\n  echo \"Endpoint created successfully\"\nelse\n  echo \"Endpoint creation failed\"\n  exit 1\nfi\n\n# cleanup of existing model\nmodel_archive=$(az ml model archive -n $AML_MODEL_NAME --version 1 || true)\n\n# <create_sklearn_deployment>\naz ml online-deployment create --name sklearn-deployment --endpoint $ENDPOINT_NAME -f endpoints/online/ncd/sklearn-deployment.yaml --all-traffic\n# </create_sklearn_deployment>\n\ndeploy_status=`az ml online-deployment show --name sklearn-deployment --endpoint $ENDPOINT_NAME --query \"provisioning_state\" -o tsv`\necho $deploy_status\nif [[ $deploy_status == \"Succeeded\" ]]\nthen\n  echo \"Deployment completed successfully\"\nelse\n  echo \"Deployment failed\"\n  exit 1\nfi\n\n# <test_sklearn_deployment>\naz ml online-endpoint invoke --name $ENDPOINT_NAME --request-file endpoints/online/ncd/sample-request-sklearn.json\n# </test_sklearn_deployment>\n\n# <create_lightgbm_deployment>\naz ml online-deployment create --name lightgbm-deployment --endpoint $ENDPOINT_NAME -f endpoints/online/ncd/lightgbm-deployment.yaml\n# </create_lightgbm_deployment>\n\ndeploy_status=`az ml online-deployment show --name lightgbm-deployment --endpoint $ENDPOINT_NAME --query \"provisioning_state\" -o tsv`\necho $deploy_status\nif [[ $deploy_status == \"Succeeded\" ]]\nthen\n  echo \"Deployment completed successfully\"\nelse\n  echo \"Deployment failed\"\n  exit 1\nfi\n\n# <test_lightgbm_deployment>\naz ml online-endpoint invoke --name $ENDPOINT_NAME --deployment lightgbm-deployment --request-file endpoints/online/ncd/sample-request-lightgbm.json\n# </test_lightgbm_deployment>\n\n# cleanup of model\nmodel_archive=$(az ml model archive -n $AML_MODEL_NAME --version 1 || true)\n\n# <delete_endpoint>\naz ml online-endpoint delete --name $ENDPOINT_NAME --yes \n# </delete_endpoint>\n\n\n```\n\n    # [Python (Azure ML SDK)](#tab/sdk)\n    \n    ```python\n    ml_client.begin_create_or_update(endpoint)\n    ```\n\n    # [Python (MLflow SDK)](#tab/mlflow)\n\n    ```python\n    endpoint = deployment_client.create_endpoint(\n        name=endpoint_name,\n        config={\"endpoint-config-file\": endpoint_config_path},\n    )\n    ```\n\n    # [Studio](#tab/studio)\n\n    *You will perform this step in the deployment stage.*\n\n1. Now, it is time to configure the deployment. A deployment is a set of resources required for hosting the model that does the actual inferencing. \n    \n    # [Azure CLI](#tab/cli)\n\n    __sklearn-deployment.yaml__\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\nname: sklearn-deployment\nendpoint_name: my-endpoint\nmodel:\n  name: mir-sample-sklearn-mlflow-model\n  version: 1\n  path: sklearn-diabetes/model\n  type: mlflow_model\ninstance_type: Standard_DS3_v2\ninstance_count: 1\n```\n\n    # [Python (Azure ML SDK)](#tab/sdk)\n\n    ```python\n    blue_deployment = ManagedOnlineDeployment(\n        name=\"blue\",\n        endpoint_name=endpoint_name,\n        model=model,\n        instance_type=\"Standard_F4s_v2\",\n        instance_count=1\n    )\n    ```\n\n    # [Python (MLflow SDK)](#tab/mlflow)\n\n    ```python\n    blue_deployment_name = \"blue\"\n    ```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-mlflow-models-online-endpoints-266.md"
                }
            },
            {
                "page_content": "> Signatures in MLflow models are optional but they are highly encouraged as they provide a convenient way to early detect data compatibility issues. For more information about how to log models with signatures read [Logging models with a custom signature, environment or samples](how-to-log-mlflow-models.md#logging-models-with-a-custom-signature-environment-or-samples).\n\nYou can inspect the model signature of your model by opening the `MLmodel` file associated with your MLflow model. For more details about how signatures work in MLflow see [Signatures in MLflow](concept-mlflow-models.md#signatures).\n\n### Flavor support\n\nBatch deployments only support deploying MLflow models with a `pyfunc` flavor. If you need to deploy a different flavor, see [Using MLflow models with a scoring script](#customizing-mlflow-models-deployments-with-a-scoring-script).\n\n## Customizing MLflow models deployments with a scoring script\n\nMLflow models can be deployed to batch endpoints without indicating a scoring script in the deployment definition. However, you can opt in to indicate this file (usually referred as the *batch driver*) to customize how inference is executed. \n\nYou will typically select this workflow when: \n> [!div class=\"checklist\"]\n> * You need to process a file type not supported by batch deployments MLflow deployments.\n> * You need to customize the way the model is run, for instance, use an specific flavor to load it with `mlflow.<flavor>.load()`.\n> * You need to do pre/pos processing in your scoring routine when it is not done by the model itself.\n> * The output of the model can't be nicely represented in tabular data. For instance, it is a tensor representing an image.\n> * You model can't process each file at once because of memory constrains and it needs to read it in chunks.\n\n> [!IMPORTANT]\n> If you choose to indicate an scoring script for an MLflow model deployment, you will also have to specify the environment where the deployment will run.\n\n> [!WARNING]\n> Customizing the scoring script for MLflow deployments is only available from the Azure CLI or SDK for Python. If you are creating a deployment using [Azure ML studio UI](https://ml.azure.com), please switch to the CLI or the SDK.\n\n\n### Steps\n\nUse the following steps to deploy an MLflow model with a custom scoring script.\n\n1. Identify the folder where your MLflow model is placed.\n\n    a. Go to [Azure Machine Learning portal](https://ml.azure.com).\n\n    b. Go to the section __Models__.\n\n    c. Select the model you are trying to deploy and click on the tab __Artifacts__.\n\n    d. Take note of the folder that is displayed. This folder was indicated when the model was registered.\n\n    :::image type=\"content\" source=\"media/how-to-deploy-mlflow-models-online-endpoints/mlflow-model-folder-name.png\" lightbox=\"media/how-to-deploy-mlflow-models-online-endpoints/mlflow-model-folder-name.png\" alt-text=\"Screenshot showing the folder where the model artifacts are placed.\":::\n\n1. Create a scoring script. Notice how the folder name `model` you identified before has been included in the `init()` function.\n\n   __batch_driver.py__\n\n   ```python\n   import os\n   import mlflow\n   import pandas as pd\n\n   def init():\n       global model\n\n       # AZUREML_MODEL_DIR is an environment variable created during deployment\n       # It is the path to the model folder\n       model_path = os.path.join(os.environ[\"AZUREML_MODEL_DIR\"], \"model\")\n       model = mlflow.pyfunc.load_model(model_path)\n\n   def run(mini_batch):\n       results = pd.DataFrame(columns=['file', 'predictions'])\n\n       for file_path in mini_batch:        \n           data = pd.read_csv(file_path)\n           pred = model.predict(data)\n\n           df = pd.DataFrame(pred, columns=['predictions'])\n           df['file'] = os.path.basename(file_path)\n           results = pd.concat([results, df])\n\n       return results\n   ```\n\n1. Let's create an environment where the scoring script can be executed. Since our model is MLflow, the conda requirements are also specified in the model package (for more details about MLflow models and the files included on it see [The MLmodel format](concept-mlflow-models.md#the-mlmodel-format)). We are going then to build the environment using the conda dependencies from the file. However, __we need also to include__ the package `azureml-core` which is required for Batch Deployments.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-mlflow-batch-426.md"
                }
            },
            {
                "page_content": "\n# Progressive rollout of MLflow models to Online Endpoints\n\nIn this article, you'll learn how you can progressively update and deploy MLflow models to Online Endpoints without causing service disruption. You'll use blue-green deployment, also known as a safe rollout strategy, to introduce a new version of a web service to production. This strategy will allow you to roll out your new version of the web service to a small subset of users or requests before rolling it out completely.\n\n## About this example\n\nOnline Endpoints have the concept of __Endpoint__ and __Deployment__. An endpoint represents the API that customers use to consume the model, while the deployment indicates the specific implementation of that API. This distinction allows users to decouple the API from the implementation and to change the underlying implementation without affecting the consumer. This example will use such concepts to update the deployed model in endpoints without introducing service disruption. \n\nThe model we will deploy is based on the [UCI Heart Disease Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). The database contains 76 attributes, but we are using a subset of 14 of them. The model tries to predict the presence of heart disease in a patient. It is integer valued from 0 (no presence) to 1 (presence). It has been trained using an `XGBBoost` classifier and all the required preprocessing has been packaged as a `scikit-learn` pipeline, making this model an end-to-end pipeline that goes from raw data to predictions.\n\nThe information in this article is based on code samples contained in the [azureml-examples](https://github.com/azure/azureml-examples) repository. To run the commands locally without having to copy/paste files, clone the repo, and then change directories to `sdk/using-mlflow/deploy`.\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in the following notebooks. In the cloned repository, open the notebook: [mlflow_sdk_online_endpoints_progresive.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/using-mlflow/deploy/mlflow_sdk_online_endpoints_progresive.ipynb).\n\n## Prerequisites\n\nBefore following the steps in this article, make sure you have the following prerequisites:\n\n- An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n- Azure role-based access controls (Azure RBAC) are used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned the owner or contributor role for the Azure Machine Learning workspace, or a custom role allowing Microsoft.MachineLearningServices/workspaces/onlineEndpoints/*. For more information, see [Manage access to an Azure Machine Learning workspace](how-to-assign-roles.md).\n\nAdditionally, you will need to:\n\n# [Azure CLI](#tab/cli)\n\n- Install the Azure CLI and the ml extension to the Azure CLI. For more information, see [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n\n# [Python (Azure ML SDK)](#tab/sdk)\n\n- Install the Azure Machine Learning SDK for Python\n    \n    ```bash\n    pip install azure-ai-ml\n    ```\n    \n# [Python (MLflow SDK)](#tab/mlflow)\n\n- Install the Mlflow SDK package `mlflow` and the Azure Machine Learning plug-in for MLflow `azureml-mlflow`.\n\n    ```bash\n    pip install mlflow azureml-mlflow\n    ```\n\n- If you are not running in Azure Machine Learning compute, configure the MLflow tracking URI or MLflow's registry URI to point to the workspace you are working on. See [Configure MLflow for Azure Machine Learning](how-to-use-mlflow-configure-tracking.md) for more details.\n\n\n### Connect to your workspace\n\nFirst, let's connect to Azure Machine Learning workspace where we are going to work on.\n\n# [Azure CLI](#tab/cli)\n\n```azurecli\naz account set --subscription <subscription>\naz configure --defaults workspace=<workspace> group=<resource-group> location=<location>\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-deploy-mlflow-models-online-progressive-0.md"
                }
            }
        ]
    },
    {
        "query": "how can I view the logs of a deployment using the v2 cli?",
        "result": "You can use the Azure CLI v2 to view the logs of a deployment. First, use the command `az ml endpoint deployment list` to get the deployment ID. Then, use the command `az ml endpoint logs -i <deployment_id>` to view the logs. You can also use the `--since` and `--tail` parameters to specify the time range and number of lines to display, respectively.",
        "context": [
            {
                "page_content": "To get logs from the storage initializer container, use the Azure CLI or Python SDK. These logs contain information on whether code and model data were successfully downloaded to the container. See the [get container logs section in troubleshooting online endpoints deployment](how-to-troubleshoot-online-endpoints.md#get-container-logs).\n\n## Add a deployment to a managed online endpoint\n\nYou can add a deployment to your existing managed online endpoint.\n\nFrom the **Endpoint details page**\n\n1. Select **+ Add Deployment** button in the [endpoint details page](#view-managed-online-endpoints).\n2. Follow the instructions to complete the deployment.\n\n:::image type=\"content\" source=\"media/how-to-create-managed-online-endpoint-studio/add-deploy-option-from-endpoint-page.png\" lightbox=\"media/how-to-create-managed-online-endpoint-studio/add-deploy-option-from-endpoint-page.png\" alt-text=\"A screenshot of Add deployment option from Endpoint details page.\":::\n\nAlternatively, you can use the **Models** page to add a deployment:\n\n1. In the left navigation bar, select the **Models** page.\n1. Select a model by checking the circle next to the model name.\n1. Select **Deploy** > **Deploy to real-time endpoint**.\n1. Choose to deploy to an existing managed online endpoint.\n\n:::image type=\"content\" source=\"media/how-to-create-managed-online-endpoint-studio/select-existing-managed-endpoints.png\" lightbox=\"media/how-to-create-managed-online-endpoint-studio/select-existing-managed-endpoints.png\" alt-text=\"A screenshot of Add deployment option from Models page.\":::\n\n> [!NOTE]\n> You can adjust the traffic balance between deployments in an endpoint when adding a new deployment.\n>\n> :::image type=\"content\" source=\"media/how-to-create-managed-online-endpoint-studio/adjust-deployment-traffic.png\" lightbox=\"media/how-to-create-managed-online-endpoint-studio/adjust-deployment-traffic.png\" alt-text=\"A screenshot of how to use sliders to control traffic distribution across multiple deployments.\":::\n\n## Update managed online endpoints\n\nYou can update deployment traffic percentage and instance count from Azure Machine Learning studio.\n\n### Update deployment traffic allocation\n\nUse **deployment traffic allocation** to control the percentage of incoming of requests going to each deployment in an endpoint.\n\n1. In the endpoint details page, Select  **Update traffic**.\n2. Adjust your traffic and select **Update**.\n\n> [!TIP]\n> The **Total traffic percentage** must sum to either 0% (to disable traffic) or 100% (to enable traffic).\n\n### Update deployment instance count\n\nUse the following instructions to scale an individual deployment up or down by adjusting the number of instances:\n\n1. In the endpoint details page. Find the card for the deployment you want to update.\n1. Select the **edit icon** in the deployment detail card.\n1. Update the instance count.\n1. Select **Update**.\n\n## Delete managed online endpoints and deployments\n\nLearn how to delete an entire managed online endpoint and it's associated deployments. Or, delete an individual deployment from a managed online endpoint.\n\n### Delete a managed online endpoint\n\nDeleting a managed online endpoint also deletes any deployments associated with it.\n\n1. Go to the [Azure Machine Learning studio](https://ml.azure.com).\n1. In the left navigation bar, select the **Endpoints** page.\n1. Select an endpoint by checking the circle next to the model name.\n1. Select **Delete**.\n\nAlternatively, you can delete a managed online endpoint directly in the [endpoint details page](#view-managed-online-endpoints). \n\n### Delete an individual deployment\n\nUse the following steps to delete an individual deployment from a managed online endpoint. This does affect the other deployments in the managed online endpoint:\n\n> [!NOTE]\n> You cannot delete a deployment that has allocated traffic. You must first [set traffic allocation](#update-deployment-traffic-allocation) for the deployment to 0% before deleting it.\n\n1. Go to the [Azure Machine Learning studio](https://ml.azure.com).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-managed-online-endpoint-studio-113.md"
                }
            },
            {
                "page_content": "To configure the experiment you want to work on use MLflow command [`mlflow.set_experiment()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_experiment).\n    \n```Python\nexperiment_name = 'experiment_with_mlflow'\nmlflow.set_experiment(experiment_name)\n```\n\n# [Using an environment variable](#tab/environ)\n\nYou can also set one of the MLflow environment variables [MLFLOW_EXPERIMENT_NAME or MLFLOW_EXPERIMENT_ID](https://mlflow.org/docs/latest/cli.html#cmdoption-mlflow-run-arg-uri) with the experiment name. \n\n```bash\nexport MLFLOW_EXPERIMENT_NAME=\"experiment_with_mlflow\"\n```\n\n\n### Start training job\n\nAfter you set the MLflow experiment name, you can start your training job with `start_run()`. Then use `log_metric()` to activate the MLflow logging API and begin logging your training job metrics.\n\n```Python\nimport os\nfrom random import random\n\nwith mlflow.start_run() as mlflow_run:\n    mlflow.log_param(\"hello_param\", \"world\")\n    mlflow.log_metric(\"hello_metric\", random())\n    os.system(f\"echo 'hello world' > helloworld.txt\")\n    mlflow.log_artifact(\"helloworld.txt\")\n```\n\n\nFor details about how to log metrics, parameters and artifacts in a run using MLflow view [How to log and view metrics](how-to-log-view-metrics.md).\n\n## Track jobs running on Azure Machine Learning\n\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nRemote runs (jobs) let you train your models in a more robust and repetitive way. They can also leverage more powerful computes, such as Machine Learning Compute clusters. See [What are compute targets in Azure Machine Learning?](concept-compute-target.md) to learn about different compute options.\n\nWhen submitting runs using jobs, Azure Machine Learning automatically configures MLflow to work with the workspace the job is running in. This means that there is no need to configure the MLflow tracking URI. On top of that, experiments are automatically named based on the details of the job.\n\n> [!IMPORTANT]\n> When submitting training jobs to Azure Machine Learning, you don't have to configure the MLflow tracking URI on your training logic as it is already configured for you.\n\n### Creating a training routine\n\nFirst, you should create a `src` subdirectory and create a file with your training code in a `hello_world.py` file in the `src` subdirectory. All your training code will go into the `src` subdirectory, including `train.py`.\n\nThe training code is taken from this [MLfLow example](https://github.com/Azure/azureml-examples/blob/main/cli/jobs/basics/src/hello-mlflow.py) in the Azure Machine Learning example repo. \n\nCopy this code into the file:\n\n```python\n# imports\nimport os\nimport mlflow\n\nfrom random import random\n\n# define functions\ndef main():\n    mlflow.log_param(\"hello_param\", \"world\")\n    mlflow.log_metric(\"hello_metric\", random())\n    os.system(f\"echo 'hello world' > helloworld.txt\")\n    mlflow.log_artifact(\"helloworld.txt\")\n\n\n# run functions\nif __name__ == \"__main__\":\n    # run main function\n    main()\n\n```\n\n\n> [!NOTE]\n> Note how this sample don't contains the instructions `mlflow.start_run` nor `mlflow.set_experiment`. This is automatically done by Azure Machine Learning.\n\n### Submitting the job\n\nUse the [Azure Machine Learning](how-to-train-model.md) to submit a remote run. When using the Azure Machine Learning CLI (v2), the MLflow tracking URI and experiment name are set automatically and directs the logging from MLflow to your workspace. Learn more about [logging Azure Machine Learning experiments with MLflow](how-to-use-mlflow-cli-runs.md) \n\n\nCreate a YAML file with your job definition in a `job.yml` file. This file should be created outside the `src` directory. Copy this code into the file:\n\n```azurecli\n$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\ncommand: python hello-mlflow.py\ncode: src\nenvironment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\ncompute: azureml:cpu-cluster\n\n```\n\nOpen your terminal and use the following to submit the job.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-mlflow-cli-runs-58.md"
                }
            },
            {
                "page_content": "\n# Monitor and analyze jobs in studio\n\n\nYou can use [Azure Machine Learning studio](https://ml.azure.com) to monitor, organize, and track your jobs for training and experimentation. Your ML job history is an important part of an explainable and repeatable ML development process.\n\nThis article shows how to do the following tasks:\n\n* Add job display name. \n* Create a custom view. \n* Add a job description. \n* Tag and find jobs.\n* Run search over your job history.\n* Cancel or fail jobs.\n* Monitor the job status by email notification.\n* Monitor your job resources (preview)\n \n\n> [!TIP]\n> * If you're looking for information on using the Azure Machine Learning SDK v1 or CLI v1, see [How to track, monitor, and analyze jobs (v1)](./v1/how-to-track-monitor-analyze-runs.md).\n> * If you're looking for information on monitoring training jobs from the CLI or SDK v2, see [Track experiments with MLflow and CLI v2](how-to-use-mlflow-cli-runs.md).\n> * If you're looking for information on monitoring the Azure Machine Learning service and associated Azure services, see [How to monitor Azure Machine Learning](monitor-azure-machine-learning.md).\n>\n> If you're looking for information on monitoring models deployed to online endpoints, see [Monitor online endpoints](how-to-monitor-online-endpoints.md).\n\n## Prerequisites\n\nYou'll need the following items:\n\n* To use Azure Machine Learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n* You must have an Azure Machine Learning workspace. A workspace is created in [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n\n## Job display name \n\nThe job display name is an optional and customizable name that you can provide for your job. To edit the job display name:\n\n1. Navigate to the **Jobs** list. \n\n1. Select the job to edit.\n\n    :::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/select-job.png\" alt-text=\"Screenshot of Jobs list.\":::\n\n1. Select the **Edit** button to edit the job display name.\n\n    :::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/display-name.gif\" alt-text=\"Screenshot of how to edit the display name.\":::\n\n## Custom View \n    \nTo view your jobs in the studio: \n    \n1. Navigate to the **Jobs** tab.\n    \n1. Select either **All experiments** to view all the jobs in an experiment or select **All jobs** to view all the jobs submitted in the Workspace.\n    \nIn the **All jobs'** page, you can filter the jobs list by tags, experiments, compute target and more to better organize and scope your work.  \n    \n1. Make customizations to the page by selecting jobs to compare, adding charts or applying filters. These changes can be saved as a **Custom View** so you can easily return to your work. Users with workspace permissions can edit, or view the custom view. Also, share the custom view with team members for enhanced collaboration by selecting **Share view**.\n\n1. To view the job logs, select a specific job and in the **Outputs + logs** tab, you can find diagnostic and error logs for your job.\n\n    :::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/custom-views-2.gif\" alt-text=\"Screenshot of how to create a custom view.\":::   \n\n## Job description \n\nA job description can be added to a job to provide more context and information to the job. You can also search on these descriptions from the jobs list and add the job description as a column in the jobs list. \n\nNavigate to the **Job Details** page for your job and select the edit or pencil icon to add, edit, or delete descriptions for your job. To persist the changes to the jobs list, save the changes to your existing Custom View or a new Custom View. Markdown format is supported for job descriptions, which allows images to be embedded and deep linking as shown below.\n\n:::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/run-description-2.gif\" alt-text=\"Screenshot of how to create a job description.\"::: \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-track-monitor-analyze-runs-0.md"
                }
            }
        ]
    },
    {
        "query": "How can I tune the hyperparameters of a training job?",
        "result": "You can tune the hyperparameters of a training job by sweeping the hyperparameters to find the optimal settings for your model. This feature applies the hyperparameter tuning capabilities in Azure Machine Learning. You can sweep hyperparameters by defining a search space in the configuration file and submitting the job. The search space is a dictionary of the hyperparameter search space. The key is the name of the hyperparameter and the value is the parameter expression. You can also use the Azure Machine Learning studio to visualize all of your hyperparameter tuning jobs.",
        "context": [
            {
                "page_content": "| `training_parameters` | object | Dictionary containing training parameters for the job. <br> See [supported hyperparameters](#supported-hyperparameters) for detail. <br> *Note*: Hyperparameters set in the `training_parameters` are fixed across all sweeping runs and thus don't need to be included in the search space. | | |\n| `sweep` | object | Dictionary containing sweep parameters for the job. It has two keys - `sampling_algorithm` (**required**) and `early_termination`. For more information, see [model sweeping and hyperparameter tuning](./how-to-auto-train-nlp-models.md?tabs=cli#model-sweeping-and-hyperparameter-tuning-preview) sections. | | |\n| `search_space` | object | Dictionary of the hyperparameter search space. The key is the name of the hyperparameter and the value is the parameter expression. All parameters that are fixable via `training_parameters` are supported here (to be instead swept over). See  [supported hyperparameters](#supported-hyperparameters) for more detail. <br> There are two types of hyperparameters: <br> - **Discrete Hyperparameters**: Discrete hyperparameters are specified as a [`choice`](./reference-yaml-job-sweep.md#choice) among discrete values. `choice` can be one or more comma-separated values, a `range` object, or any arbitrary `list` object. Advanced discrete hyperparameters can also be specified using a distribution - [`randint`](./reference-yaml-job-sweep.md#randint), [`qlognormal`, `qnormal`](./reference-yaml-job-sweep.md#qlognormal-qnormal), [`qloguniform`, `quniform`](./reference-yaml-job-sweep.md#qloguniform-quniform). For more information, see this [section](./how-to-tune-hyperparameters.md#discrete-hyperparameters). <br> - **Continuous hyperparameters**: Continuous hyperparameters are specified as a distribution over a continuous range of values. Currently supported distributions are - [`lognormal`, `normal`](./reference-yaml-job-sweep.md#lognormal-normal), [`loguniform`](./reference-yaml-job-sweep.md#loguniform), [`uniform`](./reference-yaml-job-sweep.md#uniform). For more information, see this [section](./how-to-tune-hyperparameters.md#continuous-hyperparameters). <br> <br> See [parameter expressions](./reference-yaml-job-sweep.md#parameter-expressions) section for the set of possible expressions to use.  | | |\n| `outputs` | object | Dictionary of output configurations of the job. The key is a name for the output within the context of the job and the value is the output configuration. | | |\n| `outputs.best_model` | object | Dictionary of output configurations for best model. For more information, see [Best model output configuration](#best-model-output-configuration). | | |\n\nOther syntax used in configurations:\n\n### Limits\n\n| Key | Type | Description | Allowed values | Default value |\n| --- | ---- | ----------- | -------------- | ------------- |\n| `max_concurrent_trials` | integer | Represents the maximum number of trials (children jobs) that would be executed in parallel. | | `1` |\n| `max_trials` | integer | Represents the maximum number of trials an AutoML nlp job can try to run a training algorithm with different combination of hyperparameters.  | | `1` |\n| `timeout_minutes ` | integer | Represents the maximum amount of time in minutes that the submitted AutoML job can take to run . After this, the job will get terminated. The default timeout in AutoML NLP jobs is 7 days.  | | `10080`|\n| `trial_timeout_minutes ` | integer | Represents the maximum amount of time in minutes that each trial (child job) in the submitted AutoML job can take run. After this, the child job will get terminated.  | | |\n|`max_nodes`| integer | The maximum number of nodes from the backing compute cluster to leverage for the job.| | `1` |\n\n### Supported hyperparameters\n\nThe following table describes the hyperparameters that AutoML NLP supports. \n\n| Parameter name | Description | Syntax |\n|-------|---------|---------| \n| gradient_accumulation_steps | The number of backward operations whose gradients are to be summed up before performing one step of gradient descent by calling the optimizer\u2019s step function. <br><br> This is leveraged to use an effective batch size which is gradient_accumulation_steps times larger than the maximum size that fits the GPU. | Must be a positive integer.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-automl-nlp-cli-multilabel-classification-34.md"
                }
            },
            {
                "page_content": "The `command_job` is called as a function so we can apply the parameter expressions to the sweep inputs. The `sweep` function is then configured with `trial`, `sampling-algorithm`, `objective`, `limits`, and `compute`. The above code snippet is taken from the sample notebook [Run hyperparameter sweep on a Command or CommandComponent](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/single-step/lightgbm/iris/lightgbm-iris-sweep.ipynb). In this sample, the `learning_rate` and `boosting` parameters will be tuned. Early stopping of jobs will be determined by a `MedianStoppingPolicy`, which stops a job whose primary metric value is worse than the median of the averages across all training jobs.(see [MedianStoppingPolicy class reference](/python/api/azure-ai-ml/azure.ai.ml.sweep.medianstoppingpolicy)).\n\nTo see how the parameter values are received, parsed, and passed to the training script to be tuned, refer to this [code sample](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/single-step/lightgbm/iris/src/main.py)\n\n> [!Important]\n> Every hyperparameter sweep job restarts the training from scratch, including rebuilding the model and _all the data loaders_. You can minimize \n> this cost by using an Azure Machine Learning pipeline or manual process to do as much data preparation as possible prior to your training jobs. \n\n## Submit hyperparameter tuning experiment\n\nAfter you define your hyperparameter tuning configuration, [submit the job](/python/api/azure-ai-ml/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update):\n\n```Python\n# submit the sweep\nreturned_sweep_job = ml_client.create_or_update(sweep_job)\n# get a URL for the status of the job\nreturned_sweep_job.services[\"Studio\"].endpoint\n```\n\n## Visualize hyperparameter tuning jobs\n\nYou can visualize all of your hyperparameter tuning jobs in the [Azure Machine Learning studio](https://ml.azure.com). For more information on how to view an experiment in the portal, see [View job records in the studio](how-to-log-view-metrics.md#view-the-experiment-in-the-web-portal).\n\n- **Metrics chart**: This visualization tracks the metrics logged for each hyperdrive child job over the duration of hyperparameter tuning. Each line represents a child job, and each point measures the primary metric value at that iteration of runtime.  \n\n    :::image type=\"content\" source=\"media/how-to-tune-hyperparameters/hyperparameter-tuning-metrics.png\" alt-text=\"Hyperparameter tuning metrics chart\":::\n\n- **Parallel Coordinates Chart**: This visualization shows the correlation between primary metric performance and individual hyperparameter values. The chart is interactive via movement of axes (click and drag by the axis label), and by highlighting values across a single axis (click and drag vertically along a single axis to highlight a range of desired values). The parallel coordinates chart includes an axis on the rightmost portion of the chart that plots the best metric value corresponding to the hyperparameters set for that job instance. This axis is provided in order to project the chart gradient legend onto the data in a more readable fashion.\n\n    :::image type=\"content\" source=\"media/how-to-tune-hyperparameters/hyperparameter-tuning-parallel-coordinates.png\" alt-text=\"Hyperparameter tuning parallel coordinates chart\":::\n\n- **2-Dimensional Scatter Chart**: This visualization shows the correlation between any two individual hyperparameters along with their associated primary metric value.\n\n    :::image type=\"content\" source=\"media/how-to-tune-hyperparameters/hyperparameter-tuning-2-dimensional-scatter.png\" alt-text=\"Hyparameter tuning 2-dimensional scatter chart\":::\n\n- **3-Dimensional Scatter Chart**: This visualization is the same as 2D but allows for three hyperparameter dimensions of correlation with the primary metric value. You can also click and drag to reorient the chart to view different correlations in 3D space.\n\n    :::image type=\"content\" source=\"media/how-to-tune-hyperparameters/hyperparameter-tuning-3-dimensional-scatter.png\" alt-text=\"Hyparameter tuning 3-dimensional scatter chart\":::\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-tune-hyperparameters-403.md"
                }
            },
            {
                "page_content": "A number of trials between 10 and 20 will likely work well on many datasets. The [time budget](#job-limits) for the AutoML job can still be set, but we recommend doing this only if each trial may take a long time.\n\n> [!Warning]\n> Launching automatic sweeps via the UI is not supported at this time.\n\n\n### Individual trials\n\nIn individual trials, you directly control the model architecture and hyperparameters. The model architecture is passed via the `model_name` parameter.\n\n#### Supported model architectures\n\nThe following table summarizes the supported models for each computer vision task.\n\nTask |  model architectures | String literal syntax<br> ***`default_model`\\**** denoted with \\*\n---|----------|----------\nImage classification<br> (multi-class and multi-label)| **MobileNet**: Light-weighted models for mobile applications <br> **ResNet**: Residual networks<br> **ResNeSt**: Split attention networks<br> **SE-ResNeXt50**: Squeeze-and-Excitation networks<br> **ViT**: Vision transformer networks| `mobilenetv2`   <br>`resnet18` <br>`resnet34` <br> `resnet50`  <br> `resnet101` <br> `resnet152`    <br> `resnest50` <br> `resnest101`  <br> `seresnext`  <br> `vits16r224` (small) <br> ***`vitb16r224`\\**** (base) <br>`vitl16r224` (large)|\nObject detection | **YOLOv5**: One stage object detection model   <br>  **Faster RCNN ResNet FPN**: Two stage object detection models  <br> **RetinaNet ResNet FPN**: address class imbalance with Focal Loss <br> <br>*Note: Refer to [`model_size` hyperparameter](reference-automl-images-hyperparameters.md#model-specific-hyperparameters) for YOLOv5 model sizes.*| ***`yolov5`\\**** <br> `fasterrcnn_resnet18_fpn` <br> `fasterrcnn_resnet34_fpn` <br> `fasterrcnn_resnet50_fpn` <br> `fasterrcnn_resnet101_fpn` <br> `fasterrcnn_resnet152_fpn` <br> `retinanet_resnet50_fpn` \nInstance segmentation | **MaskRCNN ResNet FPN**| `maskrcnn_resnet18_fpn` <br> `maskrcnn_resnet34_fpn` <br> ***`maskrcnn_resnet50_fpn`\\****  <br> `maskrcnn_resnet101_fpn` <br> `maskrcnn_resnet152_fpn`\n\n\nIn addition to controlling the model architecture, you can also tune hyperparameters used for model training. While many of the hyperparameters exposed are model-agnostic, there are instances where hyperparameters are task-specific or model-specific. [Learn more about the available hyperparameters for these instances](reference-automl-images-hyperparameters.md). \n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nIf you wish to use the default hyperparameter values for a given architecture (say yolov5), you can specify it using the model_name key in the training_parameters section. For example,\n\n```yaml\ntraining_parameters:\n    model_name: yolov5\n```\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nIf you wish to use the default hyperparameter values for a given architecture (say yolov5), you can specify it using the model_name parameter in the set_training_parameters method of the task specific `automl` job. For example,\n\n```python\nimage_object_detection_job.set_training_parameters(model_name=\"yolov5\")\n```\n\n### Manually sweeping model hyperparameters\n\nWhen training computer vision models, model performance depends heavily on the hyperparameter values selected. Often, you might want to tune the hyperparameters to get optimal performance. For computer vision tasks, you can sweep hyperparameters to find the optimal settings for your model. This feature applies the hyperparameter tuning capabilities in Azure Machine Learning. [Learn how to tune hyperparameters](how-to-tune-hyperparameters.md).\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-image-models-334.md"
                }
            }
        ]
    },
    {
        "query": "please provide an example of a sweep job yaml to be used with the v2 CLI",
        "result": "Here is an example of a sweep job YAML to be used with the v2 CLI:\n\n```yaml\nsweep:\n  sampling_algorithm: random\n  early_termination:\n    type: bandit\n    evaluation_interval: 2\n    slack_factor: 0.2\n    delay_evaluation: 6\n\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\n```\n\nYou can use this YAML file to submit an AutoML job using the Azure CLI v2.",
        "context": [
            {
                "page_content": "The Bandit early termination policy is also used. This policy terminates poor performing trials; that is, those trials that are not within 20% slack of the best performing trial, which significantly saves compute resources.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nsweep:\n  sampling_algorithm: random\n  early_termination:\n    type: bandit\n    evaluation_interval: 2\n    slack_factor: 0.2\n    delay_evaluation: 6\n```\n\n```yaml\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\n```\n\n# [Python SDK](#tab/python)\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=sweep-settings)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=search-space-settings)]\n\n\nOnce the search space and sweep settings are defined, you can then submit the job to train an image model using your training dataset.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nTo submit your AutoML job, you run the following CLI v2 command with the path to your .yml file, workspace name, resource group and subscription ID.\n\n```azurecli\naz ml job create --file ./hello-automl-job-basic.yml --workspace-name [YOUR_AZURE_WORKSPACE] --resource-group [YOUR_AZURE_RESOURCE_GROUP] --subscription [YOUR_AZURE_SUBSCRIPTION]\n```\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nWhen you've configured your AutoML Job to the desired settings, you can submit the job.\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=submit-run)]\n\n\nWhen doing a hyperparameter sweep, it can be useful to visualize the different trials that were tried using the HyperDrive UI. You can navigate to this UI by going to the 'Child jobs' tab in the UI of the main automl_image_job from above, which is the HyperDrive parent job. Then you can go into the 'Child jobs' tab of this one.\n\nAlternatively, here below you can see directly the HyperDrive parent job and navigate to its 'Child jobs' tab:\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n```yaml\nCLI example not available, please use Python SDK.\n```\n\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nhd_job = ml_client.jobs.get(returned_job.name + '_HD')\nhd_job\n```\n\n\n## Register and deploy model\n\nOnce the job completes, you can register the model that was created from the best trial (configuration that resulted in the best primary metric). You can either register the model after downloading or by specifying the azureml path with corresponding jobid.  \n\n### Get the best trial\n\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n```yaml\nCLI example not available, please use Python SDK.\n```\n\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=best_run)] \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-auto-train-image-models-387.md"
                }
            },
            {
                "page_content": "\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nYou can create data inputs from training and validation MLTable from your local directory or cloud storage with the following code:\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=data-load)]\n\nTraining data is a required parameter and is passed in using the `training_data` parameter of the task specific `automl` type function. You can optionally specify another MLTable as a validation data with the `validation_data` parameter. If no validation data is specified, 20% of your training data will be used for validation by default, unless you pass `validation_data_size` argument with a different value.\n\nTarget column name is a required parameter and used as target for supervised ML task. It's passed in using the `target_column_name` parameter of the task specific `automl` function. For example,\n\n```python\nfrom azure.ai.ml import automl\nimage_object_detection_job = automl.image_object_detection(\n    training_data=my_training_data_input,\n    validation_data=my_validation_data_input,\n    target_column_name=\"label\"\n)\n```\n\n## Compute to run experiment\n\nProvide a [compute target](concept-azure-machine-learning-architecture.md#compute-targets) for automated ML to conduct model training. Automated ML models for computer vision tasks require GPU SKUs and support NC and ND families. We recommend the NCsv3-series (with v100 GPUs) for faster training. A compute target with a multi-GPU VM SKU leverages multiple GPUs to also speed up training. Additionally, when you set up a compute target with multiple nodes you can conduct faster model training through parallelism when tuning hyperparameters for your model.\n\n> [!NOTE]\n> If you are using a [compute instance](concept-compute-instance.md) as your compute target, please make sure that multiple AutoML jobs are not run at the same time. Also, please make sure that `max_concurrent_trials` is set to 1 in your [job limits](#job-limits).\n\nThe compute target is passed in using the `compute` parameter. For example:\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\ncompute: azureml:gpu-cluster\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nfrom azure.ai.ml import automl\n\ncompute_name = \"gpu-cluster\"\nimage_object_detection_job = automl.image_object_detection(\n    compute=compute_name,\n)\n```\n\n## Configure experiments\n\nFor computer vision tasks, you can launch either [individual trials](#individual-trials), [manual sweeps](#manually-sweeping-model-hyperparameters) or [automatic sweeps](#automatically-sweeping-model-hyperparameters-automode). We recommend starting with an automatic sweep to get a first baseline model. Then, you can try out individual trials with certain models and hyperparameter configurations. Finally, with manual sweeps you can explore multiple hyperparameter values near the more promising models and hyperparameter configurations. This three step workflow (automatic sweep, individual trials, manual sweeps) avoids searching the entirety of the hyperparameter space, which grows exponentially in the number of hyperparameters.\n\nAutomatic sweeps can yield competitive results for many datasets. Additionally, they do not require advanced knowledge of model architectures, they take into account hyperparameter correlations and they work seamlessly across different hardware setups. All these reasons make them a strong option for the early stage of your experimentation process.\n\n### Primary metric\n\nAn AutoML training job uses a primary metric for model optimization and hyperparameter tuning. The primary metric depends on the task type as shown below; other primary metric values are currently not supported. \n\n* [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) for image classification\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-image-models-207.md"
                }
            },
            {
                "page_content": "* [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) for image classification\n* [Intersection over union](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) for image classification multilabel\n* [Mean average precision](how-to-understand-automated-ml.md#object-detection-and-instance-segmentation-metrics) for image object detection\n* [Mean average precision](how-to-understand-automated-ml.md#object-detection-and-instance-segmentation-metrics) for image instance segmentation\n    \n### Job limits\n\nYou can control the resources spent on your AutoML Image training job by specifying the `timeout_minutes`, `max_trials` and the `max_concurrent_trials` for the job in limit settings as described in the below example.\n\nParameter | Detail\n-----|----\n`max_trials` |  Parameter for maximum number of trials to sweep. Must be an integer between 1 and 1000. When exploring just the default hyperparameters for a given model architecture, set this parameter to 1. The default value is 1.\n`max_concurrent_trials`| Maximum number of trials that can run concurrently. If specified, must be an integer between 1 and 100.  The default value is 1. <br><br> **NOTE:** <li> The number of concurrent trials is gated on the resources available in the specified compute target. Ensure that the compute target has the available resources for the desired concurrency.  <li> `max_concurrent_trials` is capped at `max_trials` internally. For example, if user sets `max_concurrent_trials=4`, `max_trials=2`, values would be internally updated as `max_concurrent_trials=2`, `max_trials=2`.\n`timeout_minutes`| The amount of time in minutes before the experiment terminates. If none specified, default experiment timeout_minutes is seven days (maximum 60 days)\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nlimits:\n  timeout_minutes: 60\n  max_trials: 10\n  max_concurrent_trials: 2\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=limit-settings)]\n\n\n### Automatically sweeping model hyperparameters (AutoMode)\n\n> [!IMPORTANT]\n> This feature is currently in public preview. This preview version is provided without a service-level agreement. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\n\nIt is generally hard to predict the best model architecture and hyperparameters for a dataset. Also, in some cases the human time allocated to tuning hyperparameters may be limited. For computer vision tasks, you can specify a number of trials and the system will automatically determine the region of the hyperparameter space to sweep. You do not have to define a hyperparameter search space, a sampling method or an early termination policy.\n\n#### Triggering AutoMode\n\nYou can run automatic sweeps by setting `max_trials` to a value greater than 1 in `limits` and by not specifying the search space, sampling method and termination policy. We call this functionality AutoMode; please see an example below.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nlimits:\n  max_trials: 10\n  max_concurrent_trials: 2\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nimage_object_detection_job.set_limits(max_trials=10, max_concurrent_trials=2)\n```\n\nA number of trials between 10 and 20 will likely work well on many datasets. The [time budget](#job-limits) for the AutoML job can still be set, but we recommend doing this only if each trial may take a long time.\n\n> [!Warning]\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-image-models-270.md"
                }
            }
        ]
    },
    {
        "query": "how can I schedule a job to run every day at 9am using the new CLI v2?",
        "result": "You can create a schedule to run a job every day at 9am using the new CLI v2 by defining a recurrence trigger in the YAML file. Here's an example YAML file that creates a schedule to run a job every day at 9am:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/schedule.schema.json\nname: daily_job_schedule\ndisplay_name: Daily job schedule\ndescription: a daily job schedule\n\ntrigger:\n  type: recurrence\n  frequency: day\n  interval: 1\n  schedule:\n    hours: [9]\n  start_time: \"2022-07-10T09:00:00\"\n  time_zone: \"Pacific Standard Time\"\n\ncreate_job: ./my_job.yml\n```\n\nReplace `./my_job.yml` with the path to your job YAML file.",
        "context": [
            {
                "page_content": "| `mode` | string | Mode of how the data should be delivered to the compute target. <br><br> For read-only mount (`ro_mount`), the data will be consumed as a mount path. A folder will be mounted as a folder and a file will be mounted as a file. Azure ML will resolve the input to the mount path. <br><br> For `download` mode the data will be downloaded to the compute target. Azure ML will resolve the input to the downloaded path. <br><br> If you only want the URL of the storage location of the data artifact(s) rather than mounting or downloading the data itself, you can use the `direct` mode. This will pass in the URL of the storage location as the job input. In this case, you're fully responsible for handling credentials to access the storage. | `ro_mount`, `download`, `direct` | `ro_mount` |\n\n### Job outputs\n\n| Key | Type | Description | Allowed values | Default value |\n| --- | ---- | ----------- | -------------- | ------------- |\n| `type` | string | The type of job output. For the default `uri_folder` type, the output will correspond to a folder. | `uri_folder` | `uri_folder` |\n| `path` | string | The path to the data to use as input. This can be specified in a few ways: <br><br> - A local path to the data source file or folder, for example, `path: ./iris.csv`. The data will get uploaded during job submission. <br><br> - A URI of a cloud path to the file or folder to use as the input. Supported URI types are `azureml`, `https`, `wasbs`, `abfss`, `adl`. For more information on how to use the `azureml://` URI format, see [Core yaml syntax](reference-yaml-core-syntax.md). <br><br> - An existing registered Azure ML data asset to use as the input. To reference a registered data asset, use the `azureml:<data_name>:<data_version>` syntax or `azureml:<data_name>@latest` (to reference the latest version of that data asset), for example, `path: azureml:cifar10-data:1` or `path: azureml:cifar10-data@latest`. | | |\n| `mode` | string | Mode of how output file(s) will get delivered to the destination storage. For read-write mount mode (`rw_mount`) the output directory will be a mounted directory. For upload mode the file(s) written will get uploaded at the end of the job. | `rw_mount`, `upload` | `rw_mount` |\n\n## Remarks\n\nThe `az ml schedule` command can be used for managing Azure Machine Learning models.\n\n## Examples\n\nExamples are available in the [examples GitHub repository](https://github.com/Azure/azureml-examples/tree/main/cli/schedules). A couple are shown below.\n\n## YAML: Schedule with recurrence pattern\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/schedule.schema.json\nname: simple_recurrence_job_schedule\ndisplay_name: Simple recurrence job schedule\ndescription: a simple hourly recurrence job schedule\n\ntrigger:\n  type: recurrence\n  frequency: day #can be minute, hour, day, week, month\n  interval: 1 #every day\n  schedule:\n    hours: [4,5,10,11,12]\n    minutes: [0,30]\n  start_time: \"2022-07-10T10:00:00\" # optional - default will be schedule creation time\n  time_zone: \"Pacific Standard Time\" # optional - default will be UTC\n\ncreate_job: ./simple-pipeline-job.yml\n# create_job: azureml:simple-pipeline-job\n\n```\n\n## YAML: Schedule with cron expression\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/schedule.schema.json\nname: simple_cron_job_schedule\ndisplay_name: Simple cron job schedule\ndescription: a simple hourly cron job schedule\n\ntrigger:\n  type: cron\n  expression: \"0 * * * *\"\n  start_time: \"2022-07-10T10:00:00\" # optional - default will be schedule creation time\n  time_zone: \"Pacific Standard Time\" # optional - default will be UTC\n\n# create_job: azureml:simple-pipeline-job\ncreate_job: ./simple-pipeline-job.yml\n```\n\n## Appendix\n\n### Timezone\n\nCurrent schedule supports the following timezones. The key can be used directly in the Python SDK, while the value can be used in the YAML job. The table is organized by UTC(Coordinated Universal Time).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-yaml-schedule-87.md"
                }
            },
            {
                "page_content": "Currently there are three action rules related to schedules and you can configure in Azure portal. You can learn more details about [how to manage access to an Azure Machine Learning workspace.](how-to-assign-roles.md#create-custom-role)\n\n| Action | Description                                                                | Rule                                                          |\n|--------|----------------------------------------------------------------------------|---------------------------------------------------------------|\n| Read   | Get and list schedules in Machine Learning workspace                        | Microsoft.MachineLearningServices/workspaces/schedules/read   |\n| Write  | Create, update, disable and enable schedules in Machine Learning workspace | Microsoft.MachineLearningServices/workspaces/schedules/write  |\n| Delete | Delete a schedule in Machine Learning workspace                            | Microsoft.MachineLearningServices/workspaces/schedules/delete |\n\n## Frequently asked questions\n\n- Why my schedules created by SDK aren't listed in UI?\n\n    The schedules UI is for v2 schedules. Hence, your v1 schedules won't be listed or accessed via UI.\n\n    However, v2 schedules also support v1 pipeline jobs. You don't have to publish pipeline first, and you can directly set up schedules for a pipeline job.\n\n- Why my schedules don't trigger job at the time I set before?\n  - By default schedules will use UTC timezone to calculate trigger time. You can specify timezone in the creation wizard, or update timezone in schedule detail page.\n  - If you set the recurrence as the 31st day of every month, in months with less than 31 days, the schedule won't trigger jobs.\n  - If you're using cron expressions, MONTH isn't supported. If you pass a value, it will be ignored and treated as *. This is a known limitation.\n- Are event-based schedules supported?\n  - No, V2 schedule does not support event-based schedules.\n\n## Next steps\n\n* Learn more about the [CLI (v2) schedule YAML schema](./reference-yaml-schedule.md).\n* Learn how to [create pipeline job in CLI v2](how-to-create-component-pipelines-cli.md).\n* Learn how to [create pipeline job in SDK v2](how-to-create-component-pipeline-python.md).\n* Learn more about [CLI (v2) core YAML syntax](reference-yaml-core-syntax.md).\n* Learn more about [Pipelines](concept-ml-pipelines.md).\n* Learn more about [Component](concept-component.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-schedule-pipeline-job-871.md"
                }
            },
            {
                "page_content": "Administrators can use a built-in [Azure Policy](./../governance/policy/overview.md) definition to enforce auto-stop on all compute instances in a given subscription/resource-group. \n\n1. Navigate to Azure Policy in the Azure portal.\n2. Under \"Definitions\", look for the idle shutdown policy.\n\n      :::image type=\"content\" source=\"media/how-to-create-attach-studio/idle-shutdown-policy.png\" alt-text=\"Screenshot for the idle shutdown policy in Azure portal.\" lightbox=\"media/how-to-create-attach-studio/idle-shutdown-policy.png\":::\n\n3. Assign policy to the necessary scope.\n\nYou can also create your own custom Azure policy. For example, if the below policy is assigned, all new compute instances will have auto-stop configured with a 60-minute inactivity period. \n\n```json\n{\n  \"mode\": \"All\",\n  \"policyRule\": {\n    \"if\": {\n      \"allOf\": [\n        {\n          \"field\": \"type\",\n          \"equals\": \"Microsoft.MachineLearningServices/workspaces/computes\"\n        },\n        {\n          \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/computeType\",\n          \"equals\": \"ComputeInstance\"\n        },\n        {\n          \"anyOf\": [\n            {\n              \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/idleTimeBeforeShutdown\",\n              \"exists\": false\n            },\n            {\n              \"value\": \"[empty(field('Microsoft.MachineLearningServices/workspaces/computes/idleTimeBeforeShutdown'))]\",\n              \"equals\": true\n            }\n          ]\n        }\n      ]\n    },\n    \"then\": {\n      \"effect\": \"append\",\n      \"details\": [\n        {\n          \"field\": \"Microsoft.MachineLearningServices/workspaces/computes/idleTimeBeforeShutdown\",\n          \"value\": \"PT60M\"\n        }\n      ]\n    }\n  },\n  \"parameters\": {}\n}\n```\n\n## Schedule automatic start and stop\n\nDefine multiple schedules for auto-shutdown and auto-start. For instance, create a schedule to start at 9 AM and stop at 6 PM from Monday-Thursday, and a second schedule to start at 9 AM and stop at 4 PM for Friday.  You can create a total of four schedules per compute instance.\n\nSchedules can also be defined for [create on behalf of](#create-on-behalf-of-preview) compute instances. You can create a schedule that creates the compute instance in a stopped state. Stopped compute instances are useful when you create a compute instance on behalf of another user.\n\nPrior to a scheduled shutdown, users will see a notification alerting them that the Compute Instance is about to shut down. At that point, the user can choose to dismiss the upcoming shutdown event, if for example they are in the middle of using their Compute Instance.\n\n### Create a schedule in studio\n\n1. [Fill out the form](?tabs=azure-studio#create).\n1. On the second page of the form, open **Show advanced settings**.\n1. Select **Add schedule** to add a new schedule.\n\n    :::image type=\"content\" source=\"media/how-to-create-attach-studio/create-schedule.png\" alt-text=\"Screenshot: Add schedule in advanced settings.\":::\n\n1. Select **Start compute instance** or **Stop compute instance**.\n1. Select the **Time zone**.\n1. Select the **Startup time** or **Shutdown time**.\n1. Select the days when this schedule is active.\n\n    :::image type=\"content\" source=\"media/how-to-create-attach-studio/stop-compute-schedule.png\" alt-text=\"Screenshot: schedule a compute instance to shut down.\":::\n\n1. Select **Add schedule** again if you want to create another schedule.\n\nOnce the compute instance is created, you can view, edit, or add new schedules from the compute instance details section.\n\n\n> [!NOTE]\n> Timezone labels don't account for day light savings. For instance,  (UTC+01:00) Amsterdam, Berlin, Bern, Rome, Stockholm, Vienna is actually UTC+02:00 during day light savings.\n\n### Create a schedule with CLI\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```azurecli\naz ml compute create -f create-instance.yml\n```\n\nWhere the file *create-instance.yml* is:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/computeInstance.schema.json \nname: schedule-example-i\ntype: computeinstance\nsize: STANDARD_DS3_v2\nschedules:\n   compute_start_stop:\n      - action: stop\n        trigger:\n         type: cron\n         start_time: \"2021-03-10T21:21:07\"\n         time_zone: Pacific Standard Time\n         expression: 0 18 * * *\n         \n\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-273.md"
                }
            }
        ]
    },
    {
        "query": "how can I see the quota assigned to my workspace in the v2 cli?",
        "result": "You can use the Azure CLI to view the quota assigned to your workspace. Run the following command: \n\n```azurecli-interactive\naz ml workspace show-quota --workspace-name <workspace-name> --resource-group <resource-group-name>\n```\n\nThis command will show the quota assigned to your workspace.",
        "context": [
            {
                "page_content": "\nAfter deployment, this role becomes available in the specified workspace. Now you can add and assign this role in the Azure portal.\n\nFor more information on custom roles, see [Azure custom roles](../role-based-access-control/custom-roles.md). \n\n### Azure Machine Learning operations\n\nFor more information on the operations (actions and not actions) usable with custom roles, see [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices). You can also use the following Azure CLI command to list operations:\n\n```azurecli-interactive\naz provider operation show \u2013n Microsoft.MachineLearningServices\n```\n\n## List custom roles\n\nIn the Azure CLI, run the following command:\n\n```azurecli-interactive\naz role definition list --subscription <sub-id> --custom-role-only true\n```\n\nTo view the role definition for a specific custom role, use the following Azure CLI command. The `<role-name>` should be in the same format returned by the command above:\n\n```azurecli-interactive\naz role definition list -n <role-name> --subscription <sub-id>\n```\n\n## Update a custom role\n\nIn the Azure CLI, run the following command:\n\n```azurecli-interactive\naz role definition update --role-definition update_def.json --subscription <sub-id>\n```\n\nYou need to have permissions on the entire scope of your new role definition. For example if this new role has a scope across three subscriptions, you need to have permissions on all three subscriptions. \n\n> [!NOTE]\n> Role updates can take 15 minutes to an hour to apply across all role assignments in that scope.\n\n## Use Azure Resource Manager templates for repeatability\n\nIf you anticipate that you'll need to recreate complex role assignments, an Azure Resource Manager template can be a significant help. The [machine-learning-dependencies-role-assignment template](https://github.com/Azure/azure-quickstart-templates/tree/master//quickstarts/microsoft.machinelearningservices/machine-learning-dependencies-role-assignment) shows how role assignments can be specified in source code for reuse. \n\n## Common scenarios\n\nThe following table is a summary of Azure Machine Learning activities and the permissions required to perform them at the least scope. For example, if an activity can be performed with a workspace scope (Column 4), then all higher scope with that permission will also work automatically. Note that for certain activities the permissions differ between V1 and V2 APIs.\n\n> [!IMPORTANT]\n> All paths in this table that start with `/` are **relative paths** to `Microsoft.MachineLearningServices/` :\n\n| Activity | Subscription-level scope | Resource group-level scope | Workspace-level scope |\n| ----- | ----- | ----- | ----- |\n| Create new workspace <sub>1</sub> | Not required | Owner or contributor | N/A (becomes Owner or inherits higher scope role after creation) |\n| Request subscription level Amlcompute quota or set workspace level quota | Owner, or contributor, or custom role </br>allowing `/locations/updateQuotas/action`</br> at subscription scope | Not Authorized | Not Authorized |\n| Create new compute cluster | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\n| Create new compute instance | Not required | Not required | Owner, contributor, or custom role allowing: `/workspaces/computes/write` |\n| Submitting any type of run (V1) | Not required | Not required | Owner, contributor, or custom role allowing: `\"/workspaces/*/read\", \"/workspaces/environments/write\", \"/workspaces/experiments/runs/write\", \"/workspaces/metadata/artifacts/write\", \"/workspaces/metadata/snapshots/write\", \"/workspaces/environments/build/action\", \"/workspaces/experiments/runs/submit/action\", \"/workspaces/environments/readSecrets/action\"` |\n| Submitting any type of run (V2) | Not required | Not required | Owner, contributor, or custom role allowing: `\"/workspaces/*/read\", \"/workspaces/environments/write\", \"/workspaces/jobs/*\", \"/workspaces/metadata/artifacts/write\", \"/workspaces/metadata/codes/*/write\", \"/workspaces/environments/build/action\", \"/workspaces/environments/readSecrets/action\"` |\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-108.md"
                }
            },
            {
                "page_content": "\n## Troubleshooting\n\nHere are a few things to be aware of while you use Azure role-based access control (Azure RBAC):\n\n- When you create a resource in Azure, such as a workspace, you're not directly the owner of the resource. Your role is inherited from the highest scope role that you're authorized against in that subscription. As an example if you're a Network Administrator, and have the permissions to create a Machine Learning workspace, you would be assigned the Network Administrator role against that workspace, and not the Owner role.\n\n- To perform quota operations in a workspace, you need subscription level permissions. This means setting either subscription level quota or workspace level quota for your managed compute resources can only happen if you have write permissions at the subscription scope.\n\n- When there are two role assignments to the same Azure Active Directory user with conflicting sections of Actions/NotActions, your operations listed in NotActions from one role might not take effect if they are also listed as Actions in another role. To learn more about how Azure parses role assignments, read [How Azure RBAC determines if a user has access to a resource](../role-based-access-control/overview.md#how-azure-rbac-determines-if-a-user-has-access-to-a-resource)\n\n- To deploy your compute resources inside a VNet, you need to explicitly have permissions for the following actions:\n    - `Microsoft.Network/virtualNetworks/*/read` on the VNet resources.\n    - `Microsoft.Network/virtualNetworks/subnets/join/action` on the subnet resource.\n    \n    For more information on Azure RBAC with networking, see the [Networking built-in roles](../role-based-access-control/built-in-roles.md#networking).\n\n- It can sometimes take up to 1 hour for your new role assignments to take effect over cached permissions across the stack.\n\n## Next steps\n\n- [Enterprise security overview](concept-enterprise-security.md)\n- [Virtual network isolation and privacy overview](how-to-network-security-overview.md)\n- [Tutorial: Train and deploy a model](tutorial-train-deploy-notebook.md)\n- [Resource provider operations](../role-based-access-control/resource-provider-operations.md#microsoftmachinelearningservices)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-assign-roles-573.md"
                }
            },
            {
                "page_content": "\n## Assign managed identity\n\nYou can assign a system- or user-assigned [managed identity](../active-directory/managed-identities-azure-resources/overview.md) to a compute instance, to authenticate against other Azure resources such as storage. Using managed identities for authentication helps improve workspace security and management. For example, you can allow users to access training data only when logged in to a compute instance. Or use a common user-assigned managed identity to permit access to a specific storage account. \n\nYou can create compute instance with managed identity from Azure ML Studio:\n\n1.\tFill out the form to [create a new compute instance](?tabs=azure-studio#create).\n1.\tSelect **Next: Advanced Settings**.\n1.\tEnable **Assign a managed identity**.\n1.  Select **System-assigned** or **User-assigned** under **Identity type**.\n1.  If you selected **User-assigned**, select subscription and name of the identity.\n\nYou can use V2 CLI to create compute instance with assign system-assigned managed identity:\n\n```azurecli\naz ml compute create --name myinstance --identity-type SystemAssigned --type ComputeInstance --resource-group my-resource-group --workspace-name my-workspace\n```\n\nYou can also use V2 CLI with yaml file, for example to create a compute instance with user-assigned managed identity:\n\n```azurecli\nazure ml compute create --file compute.yaml --resource-group my-resource-group --workspace-name my-workspace\n```\n\nThe identity definition is contained in compute.yaml file:\n\n```yaml\nhttps://azuremlschemas.azureedge.net/latest/computeInstance.schema.json\nname: myinstance\ntype: computeinstance\nidentity:\n  type: user_assigned\n  user_assigned_identities: \n    - resource_id: identity_resource_id\n```\n\nOnce the managed identity is created, grant the managed identity at least Storage Blob Data Reader role on the storage account of the datastore, see [Accessing storage services](how-to-identity-based-service-authentication.md?tabs=cli#accessing-storage-services). Then, when you work on the compute instance, the managed identity is used automatically to authenticate against datastores.\n\n> [!NOTE]\n> The name of the created system managed identity will be in the format /workspace-name/computes/compute-instance-name in your Azure Active Directory. \n\nYou can also use the managed identity manually to authenticate against other Azure resources. The following example shows how to use it to get an Azure Resource Manager access token:\n\n```python\nimport requests\n\ndef get_access_token_msi(resource):\n    client_id = os.environ.get(\"DEFAULT_IDENTITY_CLIENT_ID\", None)\n    resp = requests.get(f\"{os.environ['MSI_ENDPOINT']}?resource={resource}&clientid={client_id}&api-version=2017-09-01\", headers={'Secret': os.environ[\"MSI_SECRET\"]})\n    resp.raise_for_status()\n    return resp.json()[\"access_token\"]\n\narm_access_token = get_access_token_msi(\"https://management.azure.com\")\n```\n\nTo use Azure CLI with the managed identity for authentication, specify the identity client ID as the username when logging in: \n```azurecli\naz login --identity --username $DEFAULT_IDENTITY_CLIENT_ID\n```\n\n> [!NOTE]\n> You cannot use ```azcopy``` when trying to use managed identity. ```azcopy login --identity``` will not work.\n\n## Add custom applications such as RStudio or Posit Workbench (preview)\n\n> [!IMPORTANT]\n> Items marked (preview) below are currently in public preview.\n> The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.\n> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).\n\nYou can set up other applications, such as RStudio, or Posit Workbench (formerly RStudio Workbench), when creating a compute instance. Follow these steps in studio to set up a custom application on your compute instance\n\n1.\tFill out the form to [create a new compute instance](?tabs=azure-studio#create)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-manage-compute-instance-534.md"
                }
            }
        ]
    },
    {
        "query": "How can I tune the hyperparameters of a training job using the v2 CLI?",
        "result": "You can tune the hyperparameters of a training job using the v2 CLI by defining a search space for the hyperparameters and specifying the sweep procedure via sampling algorithm early termination. You can define the search space with customized settings and configure the sweep procedure via sampling algorithm early termination. You can also specify the experiment budget for your AutoML NLP training job using the `timeout_minutes` parameter in the `limits`. Finally, you can automatically end poorly performing runs with an early termination policy.",
        "context": [
            {
                "page_content": "\nYou can define a search space with customized settings:\n\n```python \ntext_ner_job.extend_search_space( \n    [ \n        SearchSpace( \n            model_name=Choice([NlpModels.BERT_BASE_CASED, NlpModels.ROBERTA_BASE]) \n        ), \n        SearchSpace( \n            model_name=Choice([NlpModels.DISTILROBERTA_BASE]), \n            learning_rate_scheduler=Choice([NlpLearningRateScheduler.LINEAR,  \n                                            NlpLearningRateScheduler.COSINE]), \n            learning_rate=Uniform(5e-6, 5e-5) \n        ) \n    ] \n) \n ```\n\nYou can configure the sweep procedure via sampling algorithm early termination: \n```python\ntext_ner_job.set_sweep( \n    sampling_algorithm=\"Random\", \n    early_termination=BanditPolicy( \n        evaluation_interval=2, slack_factor=0.05, delay_evaluation=6 \n    ) \n) \n```\n\n\n### Sampling methods for the sweep \n\nWhen sweeping hyperparameters, you need to specify the sampling method to use for sweeping over the defined parameter space. Currently, the following sampling methods are supported with the `sampling_algorithm` parameter:\n\n| Sampling type | AutoML Job syntax |\n|-------|---------|\n|[Random Sampling](how-to-tune-hyperparameters.md#random-sampling)| `random` |\n|[Grid Sampling](how-to-tune-hyperparameters.md#grid-sampling)| `grid` |\n|[Bayesian Sampling](how-to-tune-hyperparameters.md#bayesian-sampling)| `bayesian` |\n\n### Experiment budget \n\nYou can optionally specify the experiment budget for your AutoML NLP training job using the `timeout_minutes` parameter in the `limits` - the amount of time in minutes before the experiment terminates. If none specified, the default experiment timeout is seven days (maximum 60 days).  \n\nAutoML NLP also supports `trial_timeout_minutes`, the maximum amount of time in minutes an individual trial can run before being terminated, and `max_nodes`, the maximum number of nodes from the backing compute cluster to leverage for the job. These parameters also belong to the `limits` section.  \n\n\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nlimits: \n  timeout_minutes: 60 \n  trial_timeout_minutes: 20 \n  max_nodes: 2 \n```\n\n\n### Early termination policies  \n\nYou can automatically end poorly performing runs with an early termination policy. Early termination improves computational efficiency, saving compute resources that would have been otherwise spent on less promising configurations. AutoML NLP supports early termination policies using the `early_termination` parameter. If no termination policy is specified, all configurations are run to completion. \n\nLearn more about [how to configure the early termination policy for your hyperparameter sweep.](how-to-tune-hyperparameters.md#early-termination) \n\n### Resources for the sweep\n\nYou can control the resources spent on your hyperparameter sweep by specifying the `max_trials` and the `max_concurrent_trials` for the sweep.\n\nParameter | Detail\n-----|----\n`max_trials` |  Parameter for maximum number of configurations to sweep. Must be an integer between 1 and 1000. When exploring just the default hyperparameters for a given model algorithm, set this parameter to 1. The default value is 1.\n`max_concurrent_trials`| Maximum number of runs that can run concurrently. If specified, must be an integer between 1 and 100.  The default value is 1. <br><br> **NOTE:** <li> The number of concurrent runs is gated on the resources available in the specified compute target. Ensure that the compute target has the available resources for the desired concurrency.  <li> `max_concurrent_trials` is capped at `max_trials` internally. For example, if user sets `max_concurrent_trials=4`, `max_trials=2`, values would be internally updated as `max_concurrent_trials=2`, `max_trials=2`.\n\nYou can configure all the sweep related parameters as shown in the example below.\n\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nsweep:\n  limits:\n    max_trials: 10\n    max_concurrent_trials: 2\n  sampling_algorithm: random\n  early_termination:\n    type: bandit\n    evaluation_interval: 2\n    slack_factor: 0.2\n    delay_evaluation: 6\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-nlp-models-416.md"
                }
            },
            {
                "page_content": "\n# How to do hyperparameter tuning in pipeline (v2)\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\nIn this article, you'll learn how to do hyperparameter tuning in Azure Machine Learning pipeline.\n\n## Prerequisite\n\n1. Understand what is [hyperparameter tuning](how-to-tune-hyperparameters.md) and how to do hyperparameter tuning in Azure Machine Learning use SweepJob.\n2. Understand what is a [Azure Machine Learning pipeline](concept-ml-pipelines.md)\n3. Build a command component that takes hyperparameter as input.\n\n## How to do hyperparameter tuning in Azure Machine Learning pipeline\n\nThis section explains how to do hyperparameter tuning in Azure Machine Learning pipeline using CLI v2 and Python SDK. Both approaches share the same prerequisite: you already have a command component created and the command component takes hyperparameters as inputs. If you don't have a command component yet. Follow below links to create a command component first.\n\n- [AzureML CLI v2](how-to-create-component-pipelines-cli.md)\n- [AzureML Python SDK v2](how-to-create-component-pipeline-python.md)\n\n### CLI v2\n\nThe example used in this article can be found in [azureml-example repo](https://github.com/Azure/azureml-examples). Navigate to *[azureml-examples/cli/jobs/pipelines-with-components/pipeline_with_hyperparameter_sweep* to check the example.\n\nAssume you already have a command component defined in `train.yaml`. A two-step pipeline job (train and predict) YAML file looks like below.\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\ndisplay_name: pipeline_with_hyperparameter_sweep\ndescription: Tune hyperparameters using TF component\nsettings:\n    default_compute: azureml:cpu-cluster\njobs:\n  sweep_step:\n    type: sweep\n    inputs:\n      data: \n        type: uri_file\n        path: wasbs://datasets@azuremlexamples.blob.core.windows.net/iris.csv\n      degree: 3\n      gamma: \"scale\"\n      shrinking: False\n      probability: False\n      tol: 0.001\n      cache_size: 1024\n      verbose: False\n      max_iter: -1\n      decision_function_shape: \"ovr\"\n      break_ties: False\n      random_state: 42\n    outputs:\n      model_output:\n      test_data:\n    sampling_algorithm: random\n    trial: ./train.yml\n    search_space:\n      c_value:\n        type: uniform\n        min_value: 0.5\n        max_value: 0.9\n      kernel:\n        type: choice\n        values: [\"rbf\", \"linear\", \"poly\"]\n      coef0:\n        type: uniform\n        min_value: 0.1\n        max_value: 1\n    objective:\n      goal: minimize\n      primary_metric: training_f1_score\n    limits:\n      max_total_trials: 5\n      max_concurrent_trials: 3\n      timeout: 7200\n\n  predict_step:\n    type: command\n    inputs:\n      model: ${{parent.jobs.sweep_step.outputs.model_output}}\n      test_data: ${{parent.jobs.sweep_step.outputs.test_data}}\n    outputs:\n      predict_result:\n    component: ./predict.yml\n\n    \n```\n\nThe `sweep_step` is the step for hyperparameter tuning. Its type needs to be `sweep`.  And `trial` refers to the command component defined in `train.yaml`. From the `search space` field we can see three hyparmeters (`c_value`, `kernel`, and `coef`) are added to the search space. After you submit this pipeline job, Azure Machine Learning will run the trial component multiple times to sweep over hyperparameters based on the search space and terminate policy you defined in `sweep_step`. Check [sweep job YAML schema](reference-yaml-job-sweep.md) for full schema of sweep job.\n\nBelow is the trial component definition (train.yml file). \n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\ntype: command\n\nname: train_model\ndisplay_name: train_model\nversion: 1\n\ninputs: \n  data:\n    type: uri_folder\n  c_value:\n    type: number\n    default: 1.0\n  kernel:\n    type: string\n    default: rbf\n  degree:\n    type: integer\n    default: 3\n  gamma:\n    type: string\n    default: scale\n  coef0: \n    type: number\n    default: 0\n  shrinking:\n    type: boolean\n    default: false\n  probability:\n    type: boolean\n    default: false\n  tol:\n    type: number\n    default: 1e-3\n  cache_size:\n    type: number\n    default: 1024\n  verbose:\n    type: boolean\n    default: false\n  max_iter:\n    type: integer\n    default: -1\n  decision_function_shape:\n    type: string\n    default: ovr\n  break_ties:\n    type: boolean\n    default: false\n  random_state:\n    type: integer\n    default: 42\n\noutputs:\n  model_output:\n    type: mlflow_model\n  test_data:\n    type: uri_folder\n  \ncode: ./train-src\n\nenvironment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\n\ncommand: >-\n  python train.py \n  --data ${{inputs.data}}\n  --C ${{inputs.c_value}}\n  --kernel ${{inputs.kernel}}\n  --degree ${{inputs.degree}}\n  --gamma ${{inputs.gamma}}\n  --coef0 ${{inputs.coef0}}\n  --shrinking ${{inputs.shrinking}}\n  --probability ${{inputs.probability}}\n  --tol ${{inputs.tol}}\n  --cache_size ${{inputs.cache_size}}\n  --verbose ${{inputs.verbose}}\n  --max_iter ${{inputs.max_iter}}\n  --decision_function_shape ${{inputs.decision_function_shape}}\n  --break_ties ${{inputs.break_ties}}\n  --random_state ${{inputs.random_state}}\n  --model_output ${{outputs.model_output}}\n  --test_data ${{outputs.test_data}}\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-sweep-in-pipeline-0.md"
                }
            },
            {
                "page_content": "A number of trials between 10 and 20 will likely work well on many datasets. The [time budget](#job-limits) for the AutoML job can still be set, but we recommend doing this only if each trial may take a long time.\n\n> [!Warning]\n> Launching automatic sweeps via the UI is not supported at this time.\n\n\n### Individual trials\n\nIn individual trials, you directly control the model architecture and hyperparameters. The model architecture is passed via the `model_name` parameter.\n\n#### Supported model architectures\n\nThe following table summarizes the supported models for each computer vision task.\n\nTask |  model architectures | String literal syntax<br> ***`default_model`\\**** denoted with \\*\n---|----------|----------\nImage classification<br> (multi-class and multi-label)| **MobileNet**: Light-weighted models for mobile applications <br> **ResNet**: Residual networks<br> **ResNeSt**: Split attention networks<br> **SE-ResNeXt50**: Squeeze-and-Excitation networks<br> **ViT**: Vision transformer networks| `mobilenetv2`   <br>`resnet18` <br>`resnet34` <br> `resnet50`  <br> `resnet101` <br> `resnet152`    <br> `resnest50` <br> `resnest101`  <br> `seresnext`  <br> `vits16r224` (small) <br> ***`vitb16r224`\\**** (base) <br>`vitl16r224` (large)|\nObject detection | **YOLOv5**: One stage object detection model   <br>  **Faster RCNN ResNet FPN**: Two stage object detection models  <br> **RetinaNet ResNet FPN**: address class imbalance with Focal Loss <br> <br>*Note: Refer to [`model_size` hyperparameter](reference-automl-images-hyperparameters.md#model-specific-hyperparameters) for YOLOv5 model sizes.*| ***`yolov5`\\**** <br> `fasterrcnn_resnet18_fpn` <br> `fasterrcnn_resnet34_fpn` <br> `fasterrcnn_resnet50_fpn` <br> `fasterrcnn_resnet101_fpn` <br> `fasterrcnn_resnet152_fpn` <br> `retinanet_resnet50_fpn` \nInstance segmentation | **MaskRCNN ResNet FPN**| `maskrcnn_resnet18_fpn` <br> `maskrcnn_resnet34_fpn` <br> ***`maskrcnn_resnet50_fpn`\\****  <br> `maskrcnn_resnet101_fpn` <br> `maskrcnn_resnet152_fpn`\n\n\nIn addition to controlling the model architecture, you can also tune hyperparameters used for model training. While many of the hyperparameters exposed are model-agnostic, there are instances where hyperparameters are task-specific or model-specific. [Learn more about the available hyperparameters for these instances](reference-automl-images-hyperparameters.md). \n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nIf you wish to use the default hyperparameter values for a given architecture (say yolov5), you can specify it using the model_name key in the training_parameters section. For example,\n\n```yaml\ntraining_parameters:\n    model_name: yolov5\n```\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nIf you wish to use the default hyperparameter values for a given architecture (say yolov5), you can specify it using the model_name parameter in the set_training_parameters method of the task specific `automl` job. For example,\n\n```python\nimage_object_detection_job.set_training_parameters(model_name=\"yolov5\")\n```\n\n### Manually sweeping model hyperparameters\n\nWhen training computer vision models, model performance depends heavily on the hyperparameter values selected. Often, you might want to tune the hyperparameters to get optimal performance. For computer vision tasks, you can sweep hyperparameters to find the optimal settings for your model. This feature applies the hyperparameter tuning capabilities in Azure Machine Learning. [Learn how to tune hyperparameters](how-to-tune-hyperparameters.md).\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-image-models-334.md"
                }
            }
        ]
    },
    {
        "query": "How can I use components with pipelines using the new CLI v2? Please provide a concrete example.",
        "result": "To use components with pipelines using the new CLI v2, you can create a pipeline YAML file that defines the machine learning pipeline and a component YAML file that defines the component. The component YAML file should include metadata, interface, command, code, and environment. You can then use the Azure CLI to register the component and create a pipeline. For a concrete example, you can navigate to the `3b_pipeline_with_data` subdirectory of the `cli/jobs/pipelines-with-components/basics` directory in the `azureml-examples` repository and follow the instructions provided in the README file.",
        "context": [
            {
                "page_content": "\n# Create and run machine learning pipelines using components with the Azure Machine Learning studio\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nIn this article, you'll learn how to create and run [machine learning pipelines](concept-ml-pipelines.md) by using the Azure Machine Learning studio and [Components](concept-component.md). You can create pipelines without using components, but components offer better amount of flexibility and reuse. Azure ML Pipelines may be defined in YAML and [run from the CLI](how-to-create-component-pipelines-cli.md), [authored in Python](how-to-create-component-pipeline-python.md), or composed in Azure ML Studio Designer with a drag-and-drop UI. This document focuses on the AzureML studio designer UI.\n\n## Prerequisites\n\n* If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n\n* An Azure Machine Learning workspace[Create workspace resources](quickstart-create-resources.md).\n\n* [Install and set up the Azure CLI extension for Machine Learning](how-to-configure-cli.md).\n\n* Clone the examples repository:\n\n    ```azurecli-interactive\n    git clone https://github.com/Azure/azureml-examples --depth 1\n    cd azureml-examples/cli/jobs/pipelines-with-components/\n    ```\n\n## Register component in your workspace\n\n>[!Note]\n> Designer supports two type of components, classic prebuilt components and custom components. These two types of components are not compatible.  \n>\n>Classic prebuilt components provides prebuilt components majorly for data processing and traditional machine learning tasks like regression and classification. This type of component continues to be supported but will not have any new components added.\n>\n>\n>Custom components allow you to provide your own code as a component. It supports sharing across workspaces and seamless authoring across Studio, CLI, and SDK interfaces.\n>\n>This article applies to custom components. \n\nTo build pipeline using components in UI, you need to register components to your workspace first. You can use CLI or SDK to register components to your workspace, so that you can share and reuse the component within the workspace. Registered components support automatic versioning so you can update the component but assure that pipelines that require an older version will continue to work.  \n\nIn the example below take using CLI for example. If you want to learn more about how to build a component, see [Create and run pipelines using components with  CLI](how-to-create-component-pipelines-cli.md).\n\n1. From the `cli/jobs/pipelines-with-components/basics` directory of the [`azureml-examples` repository](https://github.com/Azure/azureml-examples), navigate to the `1b_e2e_registered_components` subdirectory.\n\n1. Register the components to AzureML workspace using following commands. Learn more about [ML components](concept-component.md).\n\n    ```CLI\n    az ml component create --file train.yml\n    az ml component create --file score.yml\n    az ml component create --file eval.yml\n    ```\n\n1. After register component successfully, you can see your component in the studio UI.\n\n:::image type=\"content\" source=\"./media/how-to-create-component-pipelines-ui/component-page.png\" alt-text=\"Screenshot showing registered component in component page.\" lightbox =\"./media/how-to-create-component-pipelines-ui/component-page.png\":::\n\n## Create pipeline using registered component\n\n1. Create a new pipeline in the designer.\n\n    :::image type=\"content\" source=\"./media/how-to-create-component-pipelines-ui/new-pipeline.png\" alt-text=\"Screenshot showing creating new pipeline in designer homepage.\" lightbox =\"./media/how-to-create-component-pipelines-ui/new-pipeline.png\":::\n\n1. Set the default compute target of the pipeline. \n\n    Select the **Gear icon** ![Screenshot of the gear icon that is in the UI.](./media/tutorial-designer-automobile-price-train-score/gear-icon.png) at the top right of the canvas to open the **Settings** pane. Select the default compute target for your pipeline.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-component-pipelines-ui-0.md"
                }
            },
            {
                "page_content": "\n# Upgrade to v2\n\nAzure Machine Learning's v2 REST APIs, Azure CLI extension, and Python SDK introduce consistency and a set of new features to accelerate the production machine learning lifecycle. This article provides an overview of upgrading to v2 with recommendations to help you decide on v1, v2, or both.\n\n## Prerequisites\n\n- General familiarity with Azure ML and the v1 Python SDK.\n- Understand [what is v2?](concept-v2.md)\n\n## Should I use v2?\n\nYou should use v2 if you're starting a new machine learning project or workflow. You should use v2 if you want to use the new features offered in v2. The features include:\n* Managed Inferencing\n* Reusable components in pipelines\n* Improved scheduling of pipelines\n* Responsible AI dashboard\n* Registry of assets\n\nA new v2 project can reuse existing resources like workspaces and compute and existing assets like models and environments created using v1. \n\nSome feature gaps in v2 include:\n\n- Spark support in jobs - this is currently in preview in v2.\n- Publishing jobs (pipelines in v1) as endpoints. You can however, schedule pipelines without publishing.\n- Support for SQL/database datastores.\n- Ability to use classic prebuilt components in the designer with v2.\n\nYou should then ensure the features you need in v2 meet your organization's requirements, such as being generally available. \n\n> [!IMPORTANT]\n> New features in Azure ML will only be launched in v2.\n\n## Should I upgrade existing code to v2\n\nYou can reuse your existing assets in your v2 workflows. For instance a model created in v1 can be used to perform Managed Inferencing in v2.\n\nOptionally, if you want to upgrade specific parts of your existing code to v2, please refer to the comparison links provided in the details of each resource or asset in the rest of this document.\n\n## Which v2 API should I use?\n\nIn v2 interfaces via REST API, CLI, and Python SDK are available. The interface you should use depends on your scenario and preferences.\n\n|API|Notes|\n|-|-|\n|REST|Fewest dependencies and overhead. Use for building applications on Azure ML as a platform, directly in programming languages without an SDK provided, or per personal preference.|\n|CLI|Recommended for automation with CI/CD or per personal preference. Allows quick iteration with YAML files and straightforward separation between Azure ML and ML model code.|\n|Python SDK|Recommended for complicated scripting (for example, programmatically generating large pipeline jobs) or per personal preference. Allows quick iteration with YAML files or development solely in Python.|\n\n## Can I use v1 and v2 together?\n\nv1 and v2 can co-exist in a workspace. You can reuse your existing assets in your v2 workflows. For instance a model created in v1 can be used to perform Managed Inferencing in v2. Resources like workspace, compute, and datastore work across v1 and v2, with exceptions. A user can call the v1 Python SDK to change a workspace's description, then using the v2 CLI extension change it again. Jobs (experiments/runs/pipelines in v1) can be submitted to the same workspace from the v1 or v2 Python SDK. A workspace can have both v1 and v2 model deployment endpoints. \n\n### Using v1 and v2 code together\nWe do not recommend using the v1 and v2 SDKs together in the same code. It is technically possible to use v1 and v2 in the same code because they use different Azure namespaces. However, there are many classes with the same name across these namespaces (like Workspace, Model) which can cause confusion and make code readability and debuggability challenging. \n\n> [!IMPORTANT]\n> If your workspace uses a private endpoint, it will automatically have the `v1_legacy_mode` flag enabled, preventing usage of v2 APIs. See [how to configure network isolation with v2](how-to-configure-network-isolation-with-v2.md) for details.\n\n## Resources and assets in v1 and v2\n\nThis section gives an overview of specific resources and assets in Azure ML. See the concept article for each entity for details on their usage in v2.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-migrate-from-v1-0.md"
                }
            },
            {
                "page_content": "\n# Create and run machine learning pipelines using components with the Azure Machine Learning CLI\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n\nIn this article, you learn how to create and run [machine learning pipelines](concept-ml-pipelines.md) by using the Azure CLI and components (for more, see [What is an Azure Machine Learning component?](concept-component.md)). You can create pipelines without using components, but components offer the greatest amount of flexibility and reuse. AzureML Pipelines may be defined in YAML and run from the CLI, authored in Python, or composed in AzureML Studio Designer with a drag-and-drop UI. This document focuses on the CLI.\n\n## Prerequisites\n\n- If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n\n- An Azure Machine Learning workspace. [Create workspace resources](quickstart-create-resources.md).\n\n- [Install and set up the Azure CLI extension for Machine Learning](how-to-configure-cli.md).\n\n- Clone the examples repository:\n\n    ```azurecli-interactive\n    git clone https://github.com/Azure/azureml-examples --depth 1\n    cd azureml-examples/cli/jobs/pipelines-with-components/basics\n    ```\n\n### Suggested pre-reading\n\n- [What is Azure Machine Learning pipeline](./concept-ml-pipelines.md)\n- [What is Azure Machine Learning component](./concept-component.md)\n\n## Create your first pipeline with component\n\nLet's create your first pipeline with component using an example. This section aims to give you an initial impression of what pipeline and component look like in AzureML with a concrete example.\n\nFrom the `cli/jobs/pipelines-with-components/basics` directory of the [`azureml-examples` repository](https://github.com/Azure/azureml-examples), navigate to the `3b_pipeline_with_data` subdirector. There are three types of files in this directory. Those are the files you'll need to create when building your own pipeline.\n\n- **pipeline.yml**: This YAML file defines the machine learning pipeline. This YAML file describes how to break a full machine learning task into a multistep workflow. For example, considering a simple machine learning task of using historical data to train a sales forecasting model, you may want to build a sequential workflow with data processing, model training, and model evaluation steps.  Each step is a component that has well defined interface and can be developed, tested, and optimized independently. The pipeline YAML also defines how the child steps connect to other steps in the pipeline, for example the model training step generate a model file and the model file will pass to a model evaluation step.\n\n- **component.yml**:  This YAML file defines the component. It packages following information:\n  - Metadata: name, display name, version, description, type etc. The metadata helps to describe and manage the component.\n  - Interface: inputs and outputs. For example, a model training component will take training data and number of epochs as input, and generate a trained model file as output. Once the interface is defined, different teams can develop and test the component independently.\n  - Command, code & environment: the command, code and environment to run the component.    Command is the shell command to execute the component. Code usually refers to a source  code directory. Environment could be an AzureML environment(curated or customer created), docker image or conda environment.  \n\n- **component_src**: This is the source code directory for a specific component. It contains the source code that will be executed in the component. You can use your preferred language(Python, R...). The code must be executed by a shell command. The source code can take a few inputs from shell command line to control how this step is going to be executed. For example, a training step may take training data, learning rate, number of epochs to control the training process. The argument of a shell command is used to pass inputs and outputs to the code. \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-component-pipelines-cli-0.md"
                }
            }
        ]
    },
    {
        "query": "can you give me an example of a yaml file to train a model with the CLI v2?",
        "result": "Here is an example of a YAML file to train a model with the Azure Machine Learning CLI v2:\n\n```yaml\nname: my_training_job\nenvironment: my_environment\nentry_script: my_training_script.py\ncompute:\n  target: my_compute\n  node_count: 2\n  process_count_per_node: 1\ninputs:\n  training_data: $AZUREML_DATAREFERENCE_training_data\n  validation_data: $AZUREML_DATAREFERENCE_validation_data\noutputs:\n  model: $AZUREML_MODEL_DIR/model.pkl\n```\n\nThis YAML file defines a training job named `my_training_job` that uses an environment named `my_environment` and a script named `my_training_script.py`. The job runs on a compute target named `my_compute` with two nodes and one process per node. The job takes two inputs, `training_data` and `validation_data`, and produces an output named `model`.",
        "context": [
            {
                "page_content": "\n# Create and run machine learning pipelines using components with the Azure Machine Learning CLI\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n\nIn this article, you learn how to create and run [machine learning pipelines](concept-ml-pipelines.md) by using the Azure CLI and components (for more, see [What is an Azure Machine Learning component?](concept-component.md)). You can create pipelines without using components, but components offer the greatest amount of flexibility and reuse. AzureML Pipelines may be defined in YAML and run from the CLI, authored in Python, or composed in AzureML Studio Designer with a drag-and-drop UI. This document focuses on the CLI.\n\n## Prerequisites\n\n- If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n\n- An Azure Machine Learning workspace. [Create workspace resources](quickstart-create-resources.md).\n\n- [Install and set up the Azure CLI extension for Machine Learning](how-to-configure-cli.md).\n\n- Clone the examples repository:\n\n    ```azurecli-interactive\n    git clone https://github.com/Azure/azureml-examples --depth 1\n    cd azureml-examples/cli/jobs/pipelines-with-components/basics\n    ```\n\n### Suggested pre-reading\n\n- [What is Azure Machine Learning pipeline](./concept-ml-pipelines.md)\n- [What is Azure Machine Learning component](./concept-component.md)\n\n## Create your first pipeline with component\n\nLet's create your first pipeline with component using an example. This section aims to give you an initial impression of what pipeline and component look like in AzureML with a concrete example.\n\nFrom the `cli/jobs/pipelines-with-components/basics` directory of the [`azureml-examples` repository](https://github.com/Azure/azureml-examples), navigate to the `3b_pipeline_with_data` subdirector. There are three types of files in this directory. Those are the files you'll need to create when building your own pipeline.\n\n- **pipeline.yml**: This YAML file defines the machine learning pipeline. This YAML file describes how to break a full machine learning task into a multistep workflow. For example, considering a simple machine learning task of using historical data to train a sales forecasting model, you may want to build a sequential workflow with data processing, model training, and model evaluation steps.  Each step is a component that has well defined interface and can be developed, tested, and optimized independently. The pipeline YAML also defines how the child steps connect to other steps in the pipeline, for example the model training step generate a model file and the model file will pass to a model evaluation step.\n\n- **component.yml**:  This YAML file defines the component. It packages following information:\n  - Metadata: name, display name, version, description, type etc. The metadata helps to describe and manage the component.\n  - Interface: inputs and outputs. For example, a model training component will take training data and number of epochs as input, and generate a trained model file as output. Once the interface is defined, different teams can develop and test the component independently.\n  - Command, code & environment: the command, code and environment to run the component.    Command is the shell command to execute the component. Code usually refers to a source  code directory. Environment could be an AzureML environment(curated or customer created), docker image or conda environment.  \n\n- **component_src**: This is the source code directory for a specific component. It contains the source code that will be executed in the component. You can use your preferred language(Python, R...). The code must be executed by a shell command. The source code can take a few inputs from shell command line to control how this step is going to be executed. For example, a training step may take training data, learning rate, number of epochs to control the training process. The argument of a shell command is used to pass inputs and outputs to the code. \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-create-component-pipelines-cli-0.md"
                }
            },
            {
                "page_content": "\nYou can define a search space with customized settings:\n\n```python \ntext_ner_job.extend_search_space( \n    [ \n        SearchSpace( \n            model_name=Choice([NlpModels.BERT_BASE_CASED, NlpModels.ROBERTA_BASE]) \n        ), \n        SearchSpace( \n            model_name=Choice([NlpModels.DISTILROBERTA_BASE]), \n            learning_rate_scheduler=Choice([NlpLearningRateScheduler.LINEAR,  \n                                            NlpLearningRateScheduler.COSINE]), \n            learning_rate=Uniform(5e-6, 5e-5) \n        ) \n    ] \n) \n ```\n\nYou can configure the sweep procedure via sampling algorithm early termination: \n```python\ntext_ner_job.set_sweep( \n    sampling_algorithm=\"Random\", \n    early_termination=BanditPolicy( \n        evaluation_interval=2, slack_factor=0.05, delay_evaluation=6 \n    ) \n) \n```\n\n\n### Sampling methods for the sweep \n\nWhen sweeping hyperparameters, you need to specify the sampling method to use for sweeping over the defined parameter space. Currently, the following sampling methods are supported with the `sampling_algorithm` parameter:\n\n| Sampling type | AutoML Job syntax |\n|-------|---------|\n|[Random Sampling](how-to-tune-hyperparameters.md#random-sampling)| `random` |\n|[Grid Sampling](how-to-tune-hyperparameters.md#grid-sampling)| `grid` |\n|[Bayesian Sampling](how-to-tune-hyperparameters.md#bayesian-sampling)| `bayesian` |\n\n### Experiment budget \n\nYou can optionally specify the experiment budget for your AutoML NLP training job using the `timeout_minutes` parameter in the `limits` - the amount of time in minutes before the experiment terminates. If none specified, the default experiment timeout is seven days (maximum 60 days).  \n\nAutoML NLP also supports `trial_timeout_minutes`, the maximum amount of time in minutes an individual trial can run before being terminated, and `max_nodes`, the maximum number of nodes from the backing compute cluster to leverage for the job. These parameters also belong to the `limits` section.  \n\n\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nlimits: \n  timeout_minutes: 60 \n  trial_timeout_minutes: 20 \n  max_nodes: 2 \n```\n\n\n### Early termination policies  \n\nYou can automatically end poorly performing runs with an early termination policy. Early termination improves computational efficiency, saving compute resources that would have been otherwise spent on less promising configurations. AutoML NLP supports early termination policies using the `early_termination` parameter. If no termination policy is specified, all configurations are run to completion. \n\nLearn more about [how to configure the early termination policy for your hyperparameter sweep.](how-to-tune-hyperparameters.md#early-termination) \n\n### Resources for the sweep\n\nYou can control the resources spent on your hyperparameter sweep by specifying the `max_trials` and the `max_concurrent_trials` for the sweep.\n\nParameter | Detail\n-----|----\n`max_trials` |  Parameter for maximum number of configurations to sweep. Must be an integer between 1 and 1000. When exploring just the default hyperparameters for a given model algorithm, set this parameter to 1. The default value is 1.\n`max_concurrent_trials`| Maximum number of runs that can run concurrently. If specified, must be an integer between 1 and 100.  The default value is 1. <br><br> **NOTE:** <li> The number of concurrent runs is gated on the resources available in the specified compute target. Ensure that the compute target has the available resources for the desired concurrency.  <li> `max_concurrent_trials` is capped at `max_trials` internally. For example, if user sets `max_concurrent_trials=4`, `max_trials=2`, values would be internally updated as `max_concurrent_trials=2`, `max_trials=2`.\n\nYou can configure all the sweep related parameters as shown in the example below.\n\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nsweep:\n  limits:\n    max_trials: 10\n    max_concurrent_trials: 2\n  sampling_algorithm: random\n  early_termination:\n    type: bandit\n    evaluation_interval: 2\n    slack_factor: 0.2\n    delay_evaluation: 6\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-auto-train-nlp-models-416.md"
                }
            },
            {
                "page_content": "\n# What is Azure Machine Learning CLI & Python SDK v2?\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\nAzure Machine Learning CLI v2 and Azure Machine Learning Python SDK v2 introduce a consistency of features and terminology across the interfaces.  In order to create this consistency, the syntax of commands differs, in some cases significantly, from the first versions (v1).\n\n## Azure Machine Learning CLI v2\n\nThe Azure Machine Learning CLI v2 (CLI v2) is the latest extension for the [Azure CLI](/cli/azure/what-is-azure-cli). The CLI v2 provides commands in the format *az ml __\\<noun\\> \\<verb\\> \\<options\\>__* to create and maintain Azure ML assets and workflows. The assets or workflows themselves are defined using a YAML file. The YAML file defines the configuration of the asset or workflow \u2013 what is it, where should it run, and so on.\n\nA few examples of CLI v2 commands:\n\n* `az ml job create --file my_job_definition.yaml`\n* `az ml environment update --name my-env --file my_updated_env_definition.yaml`\n* `az ml model list`\n* `az ml compute show --name my_compute`\n\n### Use cases for CLI v2\n\nThe CLI v2 is useful in the following scenarios:\n\n* On board to Azure ML without the need to learn a specific programming language\n\n    The YAML file defines the configuration of the asset or workflow \u2013 what is it, where should it run, and so on. Any custom logic/IP used, say data preparation, model training, model scoring can remain in script files, which are referred to in the YAML, but not part of the YAML itself. Azure ML supports script files in python, R, Java, Julia or C#. All you need to learn is YAML format and command lines to use Azure ML. You can stick with script files of your choice.\n\n* Ease of deployment and automation\n\n    The use of command-line for execution makes deployment and automation simpler, since workflows can be invoked from any offering/platform, which allows users to call the command line.\n\n* Managed inference deployments\n\n    Azure ML offers [endpoints](concept-endpoints.md) to streamline model deployments for both real-time and batch inference deployments. This functionality is available only via CLI v2 and SDK v2.\n\n* Reusable components in pipelines\n\n    Azure ML introduces [components](concept-component.md) for managing and reusing common logic across pipelines. This functionality is available only via CLI v2 and SDK v2.\n\n\n## Azure Machine Learning Python SDK v2\n\nAzure ML Python SDK v2 is an updated Python SDK package, which allows users to:\n\n* Submit training jobs\n* Manage data, models, environments\n* Perform managed inferencing (real time and batch)\n* Stitch together multiple tasks and production workflows using Azure ML pipelines\n\nThe SDK v2 is on par with CLI v2 functionality and is consistent in how assets (nouns) and actions (verbs) are used between SDK and CLI.  For example, to list an asset, the `list` action can be used in both CLI and SDK. The same `list` action can be used to list a compute, model, environment, and so on.\n\n### Use cases for SDK v2\n\nThe SDK v2 is useful in the following scenarios:\n\n* Use Python functions to build a single step or a complex workflow\n\n    SDK v2 allows you to build a single command or a chain of commands like Python functions - the command has a name, parameters, expects input, and returns output.\n\n* Move from simple to complex concepts incrementally\n\n    SDK v2 allows you to: \n    * Construct a single command.\n    * Add a hyperparameter sweep on top of that command, \n    * Add the command with various others into a pipeline one after the other. \n    \n    This construction is useful, given the iterative nature of machine learning.\n\n* Reusable components in pipelines\n\n    Azure ML introduces [components](concept-component.md) for managing and reusing common logic across pipelines. This functionality is available only via CLI v2 and SDK v2.\n\n* Managed inferencing\n\n    Azure ML offers [endpoints](concept-endpoints.md) to streamline model deployments for both real-time and batch inference deployments. This functionality is available only via CLI v2 and SDK v2.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-v2-0.md"
                }
            }
        ]
    },
    {
        "query": "how can I run a job under the user's identity using CLI v2? Please provide a concrete example of a yaml file.",
        "result": "To run a job under the user's identity using CLI v2, you can use the Azure CLI to log in and then run a batch deployment job. When working on Azure ML studio, batch endpoints/deployments are always executed using the identity of the current user logged in. Here is an example YAML file:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/computeInstance.schema.json\nname: minimal-example-i\ntype: computeinstance\n```\n\nThis YAML file creates a minimal compute instance named `minimal-example-i`.",
        "context": [
            {
                "page_content": "\n# Authorization on batch endpoints\n\nBatch endpoints support Azure Active Directory authentication, or `aad_token`. That means that in order to invoke a batch endpoint, the user must present a valid Azure Active Directory authentication token to the batch endpoint URI. Authorization is enforced at the endpoint level. The following article explains how to correctly interact with batch endpoints and the security requirements for it. \n\n## Prerequisites\n\n* This example assumes that you have a model correctly deployed as a batch endpoint. Particularly, we are using the *heart condition classifier* created in the tutorial [Using MLflow models in batch deployments](how-to-mlflow-batch.md).\n\n## How authorization works\n\nTo invoke a batch endpoint, the user must present a valid Azure Active Directory token representing a __security principal__. This principal can be a __user principal__ or a __service principal__. In any case, once an endpoint is invoked, a batch deployment job is created under the identity associated with the token. The identity needs the following permissions in order to successfully create a job:\n\n> [!div class=\"checklist\"]\n> * Read batch endpoints/deployments.\n> * Create jobs in batch inference endpoints/deployment.\n> * Create experiments/runs.\n> * Read and write from/to data stores.\n> * Lists datastore secrets.\n\nYou can either use one of the [built-in security roles](../role-based-access-control/built-in-roles.md) or create a new one. In any case, the identity used to invoke the endpoints requires to be granted the permissions explicitly. See [Steps to assign an Azure role](../role-based-access-control/role-assignments-steps.md) for instructions to assign them.\n\n> [!IMPORTANT]\n> The identity used for invoking a batch endpoint may not be used to read the underlying data depending on how the data store is configured. Please see [Security considerations when reading data](how-to-access-data-batch-endpoints-jobs.md#security-considerations-when-reading-data) for more details.\n\n## How to run jobs using different types of credentials\n\nThe following examples show different ways to start batch deployment jobs using different types of credentials:\n\n> [!IMPORTANT] \n> When working on a private link-enabled workspaces, batch endpoints can't be invoked from the UI in Azure ML studio. Please use the Azure ML CLI v2 instead for job creation.\n\n### Running jobs using user's credentials\n\nIn this case, we want to execute a batch endpoint using the identity of the user currently logged in. Follow these steps:\n\n> [!NOTE]\n> When working on Azure ML studio, batch endpoints/deployments are always executed using the identity of the current user logged in.\n\n# [Azure CLI](#tab/cli)\n\n1. Use the Azure CLI to log in using either interactive or device code authentication:\n\n    ```azurecli\n    az login\n    ```\n\n1. Once authenticated, use the following command to run a batch deployment job:\n\n    ```azurecli\n    az ml batch-endpoint invoke --name $ENDPOINT_NAME --input https://azuremlexampledata.blob.core.windows.net/data/heart-disease-uci\n    ```\n\n# [Python](#tab/sdk)\n\n1. Use the Azure ML SDK for Python to log in using either interactive or device authentication:\n\n    ```python\n    from azure.ai.ml import MLClient\n    from azure.identity import InteractiveAzureCredentials\n\n    subscription_id = \"<subscription>\"\n    resource_group = \"<resource-group>\"\n    workspace = \"<workspace>\"\n\n    ml_client = MLClient(InteractiveAzureCredentials(), subscription_id, resource_group, workspace)\n    ```\n\n1. Once authenticated, use the following command to run a batch deployment job:\n\n    ```python\n    job = ml_client.batch_endpoints.invoke(\n            endpoint_name, \n            input=Input(path=\"https://azuremlexampledata.blob.core.windows.net/data/heart-disease-uci\")\n        )\n    ```\n\n# [REST](#tab/rest)\n\nWhen working with REST APIs, we recommend to using either a [service principal](#running-jobs-using-a-service-principal) or a [managed identity](#running-jobs-using-a-managed-identity) to interact with the API.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-authenticate-batch-endpoint-0.md"
                }
            },
            {
                "page_content": "\n# CLI (v2) compute instance YAML schema\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nThe source JSON schema can be found at https://azuremlschemas.azureedge.net/latest/computeInstance.schema.json.\n\n\n\n[!INCLUDE [schema note](../../includes/machine-learning-preview-old-json-schema-note.md)]\n\n## YAML syntax\n\n| Key | Type | Description | Allowed values | Default value |\n| --- | ---- | ----------- | -------------- | ------- |\n| `$schema` | string | The YAML schema. If you use the Azure Machine Learning VS Code extension to author the YAML file, including `$schema` at the top of your file enables you to invoke schema and resource completions. | | |\n| `type` | string | **Required.** The type of compute. | `computeinstance` | |\n| `name` | string | **Required.** Name of the compute. | | |\n| `description` | string | Description of the compute. | | |\n| `size` | string | The VM size to use for the compute instance. For more information, see [Supported VM series and sizes](concept-compute-target.md#supported-vm-series-and-sizes). Note that not all sizes are available in all regions. | For the list of supported sizes in a given region, please use the `az ml compute list-sizes` command.  | `Standard_DS3_v2` |\n| `create_on_behalf_of` | object | Settings for creating the compute instance on behalf of another user. Please ensure that the assigned user has correct RBAC permissions. |  |  |\n| `create_on_behalf_of.user_tenant_id` | string | The AAD Tenant ID of the assigned user. |  |  |\n| `create_on_behalf_of.user_object_id` | string | The AAD Object ID of the assigned user. |  |  |\n| `ssh_public_access_enabled` | boolean | Whether to enable public SSH access on the compute instance. | | `false` |\n| `ssh_settings` | object | SSH settings for connecting to the compute instance. | | |\n| `ssh_settings.ssh_key_value` | string | The SSH public key of the administrator user account. | | |\n| `network_settings` | object | Network security settings. | | |\n| `network_settings.vnet_name` | string | Name of the virtual network (VNet) when creating a new one or referencing an existing one. | | |\n| `network_settings.subnet` | string | Either the name of the subnet when creating a new VNet or referencing an existing one, or the fully qualified resource ID of a subnet in an existing VNet. Do not specify `network_settings.vnet_name` if the subnet ID is specified. The subnet ID can refer to a VNet/subnet in another resource group. | | |\n| `identity` | object | The managed identity configuration to assign to the compute. AmlCompute clusters support only one system-assigned identity or multiple user-assigned identities, not both concurrently. | | |\n| `identity.type` | string | The type of managed identity to assign to the compute. If the type is `user_assigned`, the `identity.user_assigned_identities` property must also be specified. | `system_assigned`, `user_assigned` | |\n| `identity.user_assigned_identities` | array | List of fully qualified resource IDs of the user-assigned identities. | | |\n\n## Remarks\n\nThe `az ml compute` command can be used for managing Azure Machine Learning compute instances.\n\n## YAML: minimal\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/computeInstance.schema.json\nname: minimal-example-i\ntype: computeinstance\n\n```\n\n## YAML: basic\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/computeInstance.schema.json \nname: basic-example-i\ntype: computeinstance\nsize: STANDARD_DS3_v2\n\n```\n\n## Next steps\n\n- [Install and use the CLI (v2)](how-to-configure-cli.md)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-yaml-compute-instance-0.md"
                }
            },
            {
                "page_content": "\n# CLI (v2) Automated ML image classification job YAML schema\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nThe source JSON schema can be found at https://azuremlsdk2.blob.core.windows.net/preview/0.0.1/autoMLImageClassificationJob.schema.json.\n\n\n\n[!INCLUDE [schema note](../../includes/machine-learning-preview-old-json-schema-note.md)]\n\n## YAML syntax\n\n| Key | Type | Description | Allowed values | Default value |\n| --- | ---- | ----------- | -------------- | ------------- |\n| `$schema` | string | The YAML schema. If the user uses the Azure Machine Learning VS Code extension to author the YAML file, including `$schema` at the top of the file enables the user to invoke schema and resource completions. | | |\n| `type` | const | **Required.** The type of job. | `automl` | `automl` |\n| `task` | const | **Required.** The type of AutoML task. | `image_classification` | `image_classification` |\n| `name` | string | Name of the job. Must be unique across all jobs in the workspace. If omitted, Azure ML will autogenerate a GUID for the name. | | |\n| `display_name` | string | Display name of the job in the studio UI. Can be non-unique within the workspace. If omitted, Azure ML will autogenerate a human-readable adjective-noun identifier for the display name. | | |\n| `experiment_name` | string | Experiment name to organize the job under. Each job's run record will be organized under the corresponding experiment in the studio's \"Experiments\" tab. If omitted, Azure ML will default it to the name of the working directory where the job was created. | | |\n| `description` | string | Description of the job. | | |\n| `tags` | object | Dictionary of tags for the job. | | |\n| `compute` | string | Name of the compute target to execute the job on. This compute can be either a reference to an existing compute in the workspace (using the `azureml:<compute_name>` syntax) or `local` to designate local execution. For more information on compute for AutoML image jobs, see [Compute to run experiment](./how-to-auto-train-image-models.md?tabs=cli#compute-to-run-experiment) section.<br> <br>  *Note:* jobs in pipeline don't support `local` as `compute`. * | | `local` |\n| `log_verbosity` | number | Different levels of log verbosity. |`not_set`, `debug`, `info`, `warning`, `error`, `critical` | `info` |\n| `primary_metric` | string |  The metric that AutoML will optimize for model selection. |`accuracy` | `accuracy` |\n| `target_column_name` | string |  **Required.** The name of the column to target for predictions. It must always be specified. This parameter is applicable to `training_data` and `validation_data`. | |  |\n| `training_data` | object |  **Required.** The data to be used within the job. It should contain both training feature columns and a target column. The parameter training_data must always be provided. For more information on keys and their descriptions, see [Training or validation data](#training-or-validation-data) section. For an example, see [Consume data](./how-to-auto-train-image-models.md?tabs=cli#consume-data) section. | |  |\n| `validation_data` | object |  The validation data to be used within the job. It should contain both training features and label column (optionally a sample weights column). If `validation_data` is specified, then `training_data` and `target_column_name` parameters must be specified. For more information on keys and their descriptions, see [Training or validation data](#training-or-validation-data) section. For an example, see [Consume data](./how-to-auto-train-image-models.md?tabs=cli#consume-data) section| |  |\n| `validation_data_size` | float |  What fraction of the data to hold out for validation when user validation data isn't specified. | A value in range (0.0, 1.0) |  |\n| `limits` | object | Dictionary of limit configurations of the job. The key is name for the limit within the context of the job and the value is limit value. For more information, see [Configure your experiment settings](./how-to-auto-train-image-models.md?tabs=cli#job-limits) section. | | |\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-automl-images-cli-classification-0.md"
                }
            }
        ]
    },
    {
        "query": "how can I use my csv file in a job using the v2 cli?",
        "result": "To use a CSV file in a job using the Azure Machine Learning CLI v2, you can define the input in the YAML file for the job. For example, you can define an input named `my_csv` with the type `uri_file` and the path to the CSV file. Then, you can reference this input in the command for the job using the `${{inputs.my_csv}}` syntax. When you run the job, the CSV file will be available in the job's working directory.",
        "context": [
            {
                "page_content": "\n\n### 2. Create a compute resource for training\n\nAn AzureML compute cluster is a fully managed compute resource that can be used to run the training job. In the following examples, a compute cluster named `cpu-compute` is created.\n\n# [Python SDK](#tab/python)\n\n```python\nfrom azure.ai.ml.entities import AmlCompute\n\n# specify aml compute name.\ncpu_compute_target = \"cpu-cluster\"\n\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(\"Creating a new cpu compute target...\")\n    compute = AmlCompute(\n        name=cpu_compute_target, size=\"STANDARD_D2_V2\", min_instances=0, max_instances=4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n```\n\n# [Azure CLI](#tab/azurecli)\n\n```azurecli\naz ml compute create -n cpu-cluster --type amlcompute --min-instances 0 --max-instances 4\n```\n\n# [REST API](#tab/restapi)\n\n```bash\ncurl -X PUT \\\n  \"https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.MachineLearningServices/workspaces/$WORKSPACE/computes/$COMPUTE_NAME?api-version=$API_VERSION\" \\\n  -H \"Authorization:Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"location\": \"'$LOCATION'\",\n    \"properties\": {\n        \"computeType\": \"AmlCompute\",\n        \"properties\": {\n            \"vmSize\": \"Standard_D2_V2\",\n            \"vmPriority\": \"Dedicated\",\n            \"scaleSettings\": {\n                \"maxNodeCount\": 4,\n                \"minNodeCount\": 0,\n                \"nodeIdleTimeBeforeScaleDown\": \"PT30M\"\n            }\n        }\n    }\n}'\n```\n\n> [!TIP]\n> While a response is returned after a few seconds, this only indicates that the creation request has been accepted. It can take several minutes for the cluster creation to finish.\n\n\n### 4. Submit the training job\n\n# [Python SDK](#tab/python)\n\nTo run this script, you'll use a `command`. The command will be run by submitting it as a `job` to Azure ML. \n\n```python\nfrom azure.ai.ml import command, Input\n\n# define the command\ncommand_job = command(\n    code=\"./src\",\n    command=\"python main.py --iris-csv ${{inputs.iris_csv}} --learning-rate ${{inputs.learning_rate}} --boosting ${{inputs.boosting}}\",\n    environment=\"AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu@latest\",\n    inputs={\n        \"iris_csv\": Input(\n            type=\"uri_file\",\n            path=\"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\",\n        ),\n        \"learning_rate\": 0.9,\n        \"boosting\": \"gbdt\",\n    },\n    compute=\"cpu-cluster\",\n)\n```\n\n```python\n# submit the command\nreturned_job = ml_client.jobs.create_or_update(command_job)\n# get a URL for the status of the job\nreturned_job.studio_url\n```\n\nIn the above examples, you configured:\n- `code` - path where the code to run the command is located\n- `command` -  command that needs to be run\n- `environment` - the environment needed to run the training script. In this example, we use a curated or ready-made environment provided by AzureML called `AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu`. We use the latest version of this environment by using the `@latest` directive. You can also use custom environments by specifying a base docker image and specifying a conda yaml on top of it.\n- `inputs` - dictionary of inputs using name value pairs to the command. The key is a name for the input within the context of the job and the value is the input value. Inputs are referenced in the `command` using the `${{inputs.<input_name>}}` expression. To use files or folders as inputs, you can use the `Input` class.\n\nFor more information, see the [reference documentation](/python/api/azure-ai-ml/azure.ai.ml#azure-ai-ml-command).\n\nWhen you submit the job, a URL is returned to the job status in the AzureML studio. Use the studio UI to view the job progress. You can also use `returned_job.status` to check the current status of the job.\n\n# [Azure CLI](#tab/azurecli)\n\nThe `az ml job create` command used in this example requires a YAML job definition file. The contents of the file used in this example are:\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-train-model-660.md"
                }
            },
            {
                "page_content": "If the CLI or SDK code defines an option to use managed identity, Azure Machine Learning Managed (Automatic) Spark compute uses user-assigned managed identity attached to the workspace. You can attach a user-assigned managed identity to an existing Azure Machine Learning workspace using Azure Machine Learning CLI v2, or with `ARMClient`.\n\n### Attach user assigned managed identity using CLI v2\n1. Create a YAML file that defines the user-assigned managed identity that should be attached to the workspace:\n    ```yaml\n    identity:\n      type: system_assigned,user_assigned\n      tenant_id: <TENANT_ID>\n      user_assigned_identities:\n        '/subscriptions/<SUBSCRIPTION_ID/resourceGroups/<RESOURCE_GROUP>/providers/Microsoft.ManagedIdentity/userAssignedIdentities/<AML_USER_MANAGED_ID>':\n          {}\n    ```\n1. With the `--file` parameter, use the YAML file in the `az ml workspace update` command to attach the user assigned managed identity:\n    ```azurecli\n    az ml workspace update --subscription <SUBSCRIPTION_ID> --resource-group <RESOURCE_GROUP> --name <AML_WORKSPACE_NAME> --file <YAML_FILE_NAME>.yaml\n    ```\n\n### Attach user assigned managed identity using `ARMClient`\n1. Install [ARMClient](https://github.com/projectkudu/ARMClient), a simple command line tool that invokes the Azure Resource Manager API.\n1. Create a JSON file that defines the user-assigned managed identity that should be attached to the workspace:\n    ```json\n    {\n        \"properties\":{\n        },\n        \"location\": \"<AZURE_REGION>\",\n        \"identity\":{\n            \"type\":\"SystemAssigned,UserAssigned\",\n            \"userAssignedIdentities\":{\n                \"/subscriptions/<SUBSCRIPTION_ID/resourceGroups/<RESOURCE_GROUP>/providers/Microsoft.ManagedIdentity/userAssignedIdentities/<AML_USER_MANAGED_ID>\": { }\n            }\n        }\n    }\n    ```\n1. Execute the following command in the PowerShell prompt or the command prompt, to attach the user-assigned managed identity to the workspace.\n    ```cmd\n    armclient PATCH https://management.azure.com/subscriptions/<SUBSCRIPTION_ID>/resourceGroups/<RESOURCE_GROUP>/providers/Microsoft.MachineLearningServices/workspaces/<AML_WORKSPACE_NAME>?api-version=2022-05-01 '@<JSON_FILE_NAME>.json'\n    ```\n\n> [!NOTE]\n> - To ensure successful execution of the Spark job, assign the **Contributor** and **Storage Blob Data Contributor** roles, on the Azure storage account used for data input and output, to the identity that the Spark job uses\n> - If an [attached Synapse Spark pool](./how-to-manage-synapse-spark-pool.md) points to a Synapse Spark pool, in an Azure Synapse workspace that has a managed virtual network associated with it, [a managed private endpoint to storage account should be configured](../synapse-analytics/security/connect-to-a-secure-storage-account.md) to ensure data access.\n\n## Submit a standalone Spark job\nA Python script developed by [interactive data wrangling](./interactive-data-wrangling-with-apache-spark-azure-ml.md) can be used to submit a batch job to process a larger volume of data, after making necessary changes for Python script parameterization. A simple data wrangling batch job can be submitted as a standalone Spark job.\n\nA Spark job requires a Python script that takes arguments, which can be developed with modification of the Python code developed from [interactive data wrangling](./interactive-data-wrangling-with-apache-spark-azure-ml.md). A sample Python script is shown here.\n\n```python\n# titanic.py\nimport argparse\nfrom operator import add\nimport pyspark.pandas as pd\nfrom pyspark.ml.feature import Imputer\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--titanic_data\")\nparser.add_argument(\"--wrangled_data\")\n\nargs = parser.parse_args()\nprint(args.wrangled_data)\nprint(args.titanic_data)\n\ndf = pd.read_csv(args.titanic_data, index_col=\"PassengerId\")\nimputer = Imputer(inputCols=[\"Age\"], outputCol=\"Age\").setStrategy(\n    \"mean\"\n)  # Replace missing values in Age column with the mean value\ndf.fillna(\n    value={\"Cabin\": \"None\"}, inplace=True\n)  # Fill Cabin column with value \"None\" if missing\ndf.dropna(inplace=True)  # Drop the rows which still have any missing value\ndf.to_csv(args.wrangled_data, index_col=\"PassengerId\")\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-submit-spark-jobs-51.md"
                }
            },
            {
                "page_content": "\n\n## Write data in a job\n\nIn your job, you can write data to your cloud-based storage with *outputs*. The [Supported modes](#supported-modes) section showed that only job *outputs* can write data, because the mode can be either `rw_mount` or `upload`.\n\n# [Azure CLI](#tab/cli)\n\nCreate a job specification YAML file (`<file-name>.yml`), with the `outputs` section populated with the type and path where you'd like to write your data:\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/CommandJob.schema.json\n\n# Possible Paths for Data:\n# Blob: https://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>\n# Datastore: azureml://datastores/paths/<folder>/<file>\n# Data Asset: azureml:<my_data>:<version>\n\ncode: src\ncommand: >-\n  python prep.py \n  --raw_data ${{inputs.raw_data}} \n  --prep_data ${{outputs.prep_data}}\ninputs:\n  raw_data: \n    type: <type> # uri_file, uri_folder, mltable\n    path: <path>\noutputs:\n  prep_data: \n    type: <type> # uri_file, uri_folder, mltable\n    path: <path>\nenvironment: azureml:<environment_name>@latest\ncompute: azureml:cpu-cluster\n```\n\nNext, create a job with the CLI:\n\n```azurecli\naz ml job create --file <file-name>.yml\n```\n\n# [Python SDK](#tab/python)\n\n```python\nfrom azure.ai.ml import command\nfrom azure.ai.ml.entities import Data\nfrom azure.ai.ml import Input, Output\nfrom azure.ai.ml.constants import AssetTypes\n\n# Possible Asset Types for Data:\n# AssetTypes.URI_FILE\n# AssetTypes.URI_FOLDER\n# AssetTypes.MLTABLE\n\n# Possible Paths for Data:\n# Blob: https://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>\n# Datastore: azureml://datastores/paths/<folder>/<file>\n# Data Asset: azureml:<my_data>:<version>\n\nmy_job_inputs = {\n    \"raw_data\": Input(type=AssetTypes.URI_FOLDER, path=\"<path>\")\n}\n\nmy_job_outputs = {\n    \"prep_data\": Output(type=AssetTypes.URI_FOLDER, path=\"<path>\")\n}\n\njob = command(\n    code=\"./src\",  # local path where the code is stored\n    command=\"python process_data.py --raw_data ${{inputs.raw_data}} --prep_data ${{outputs.prep_data}}\",\n    inputs=my_job_inputs,\n    outputs=my_job_outputs,\n    environment=\"<environment_name>:<version>\",\n    compute=\"cpu-cluster\",\n)\n\n# submit the command\nreturned_job = ml_client.create_or_update(job)\n# get a URL for the status of the job\nreturned_job.services[\"Studio\"].endpoint\n\n```\n\n\n## Data in pipelines\n\nIf you work with Azure Machine Learning pipelines, you can read data into and move data between pipeline components with the Azure Machine Learning CLI v2 extension, or the Python SDK v2.\n\n### Azure Machine Learning CLI v2\nThis YAML file shows how to use the output data from one component as the input for another component of the pipeline, with the Azure Machine Learning CLI v2 extension:\n\n[!INCLUDE [CLI v2](../../includes/machine-learning-CLI-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\n\ndisplay_name: 3b_pipeline_with_data\ndescription: Pipeline with 3 component jobs with data dependencies\n\nsettings:\n  default_compute: azureml:cpu-cluster\n\noutputs:\n  final_pipeline_output:\n    mode: rw_mount\n\njobs:\n  component_a:\n    type: command\n    component: ./componentA.yml\n    inputs:\n      component_a_input: \n        type: uri_folder\n        path: ./data\n\n    outputs:\n      component_a_output: \n        mode: rw_mount\n  component_b:\n    type: command\n    component: ./componentB.yml\n    inputs:\n      component_b_input: ${{parent.jobs.component_a.outputs.component_a_output}}\n    outputs:\n      component_b_output: \n        mode: rw_mount\n  component_c:\n    type: command\n    component: ./componentC.yml\n    inputs:\n      component_c_input: ${{parent.jobs.component_b.outputs.component_b_output}}\n    outputs:\n      component_c_output: ${{parent.outputs.final_pipeline_output}}\n      #  mode: upload\n\n\n```\n\n### Python SDK v2\n\nThis example defines a pipeline that contains three nodes, and moves data between each node.\n\n* `prepare_data_node` loads the image and labels from Fashion MNIST data set into `mnist_train.csv` and `mnist_test.csv`.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-read-write-data-v2-259.md"
                }
            }
        ]
    },
    {
        "query": "how can I register a model from a job using the v2 cli?",
        "result": "You can register a model from a job using the v2 CLI by running the following command: \n\n```azurecli\naz ml model register --experiment-name <experiment-name> --run-id <run-id> --model-name <model-name> --model-path <model-path>\n```\n\nReplace `<experiment-name>` with the name of the experiment, `<run-id>` with the ID of the run, `<model-name>` with the name you want to give the model, and `<model-path>` with the path to the model file.",
        "context": [
            {
                "page_content": "\n# Work with models in Azure Machine Learning\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\nAzure Machine Learning allows you to work with different types of models. In this article, you learn about using Azure Machine Learning to work with different model types, such as custom, MLflow, and Triton. You also learn how to register a model from different locations, and how to use the Azure Machine Learning SDK, the user interface (UI), and the Azure Machine Learning CLI to manage your models.\n\n> [!TIP]\n> If you have model assets created that use the SDK/CLI v1, you can still use those with SDK/CLI v2. Full backward compatibility is provided. All models registered with the V1 SDK are assigned the type `custom`.\n\n## Prerequisites\n\n* An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n* An Azure Machine Learning workspace.\n* The Azure Machine Learning [SDK v2 for Python](https://aka.ms/sdk-v2-install).\n* The Azure Machine Learning [CLI v2](how-to-configure-cli.md).\n\nAdditionally, you will need to:\n\n# [Azure CLI](#tab/cli)\n\n- Install the Azure CLI and the ml extension to the Azure CLI. For more information, see [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n\n# [Python SDK](#tab/python)\n\n- Install the Azure Machine Learning SDK for Python\n    \n    ```bash\n    pip install azure-ai-ml\n    ```\n\n## Supported paths\n\nWhen you provide a model you want to register, you'll need to specify a `path` parameter that points to the data or job location. Below is a table that shows the different data locations supported in Azure Machine Learning and examples for the `path` parameter:\n\n\n|Location  | Examples  |\n|---------|---------|\n|A path on your local computer     | `mlflow-model/model.pkl`         |\n|A path on an AzureML Datastore   |   `azureml://datastores/<datastore-name>/paths/<path_on_datastore>`      |\n|A path from an AzureML job   |   `azureml://jobs/<job-name>/outputs/<output-name>/paths/<path-to-model-relative-to-the-named-output-location>`      |\n|A path from an MLflow job   |   `runs:/<run-id>/<path-to-model-relative-to-the-root-of-the-artifact-location>`      |\n|A path from a Model Asset in AzureML Workspace  | `azureml:<model-name>:<version>`|\n|A path from a Model Asset in  AzureML Registry  | `azureml://registries/<registry-name>/models/<model-name>/versions/<version>`|\n\n## Supported modes\n\nWhen you run a job with model inputs/outputs, you can specify the *mode* - for example, whether you would like the model to be read-only mounted or downloaded to the compute target. The table below shows the possible modes for different type/mode/input/output combinations:\n\nType | Input/Output | `upload` | `download` | `ro_mount` | `rw_mount` | `direct` \n------ | ------ | :---: | :---: | :---: | :---: | :---: \n`custom` file  | Input  |   |  |   |  |    \n`custom` folder    | Input |   | \u2713 | \u2713  | |\u2713  \n`mlflow`    | Input |   | \u2713 |  \u2713 |   |   \n`custom` file | Output  | \u2713  |   |    | \u2713  | \u2713   \n`custom` folder    | Output | \u2713  |   |   | \u2713 | \u2713  \n`mlflow`   | Output | \u2713  |   |    | \u2713  | \u2713 \n\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in a Jupyter Notebook. In the [azureml-examples](https://github.com/azure/azureml-examples) repository, open the notebook: [model.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/assets/model/model.ipynb).\n\n## Create a model in the model registry\n\n[Model registration](concept-model-management-and-deployment.md) allows you to store and version your models in the Azure cloud, in your workspace. The model registry helps you organize and keep track of your trained models.\n\nThe code snippets in this section cover how to:\n\n* Register your model as an asset in Machine Learning by using the CLI.\n* Register your model as an asset in Machine Learning by using the SDK.\n* Register your model as an asset in Machine Learning by using the UI.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-models-0.md"
                }
            },
            {
                "page_content": "The Bandit early termination policy is also used. This policy terminates poor performing trials; that is, those trials that are not within 20% slack of the best performing trial, which significantly saves compute resources.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nsweep:\n  sampling_algorithm: random\n  early_termination:\n    type: bandit\n    evaluation_interval: 2\n    slack_factor: 0.2\n    delay_evaluation: 6\n```\n\n```yaml\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\n```\n\n# [Python SDK](#tab/python)\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=sweep-settings)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=search-space-settings)]\n\n\nOnce the search space and sweep settings are defined, you can then submit the job to train an image model using your training dataset.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nTo submit your AutoML job, you run the following CLI v2 command with the path to your .yml file, workspace name, resource group and subscription ID.\n\n```azurecli\naz ml job create --file ./hello-automl-job-basic.yml --workspace-name [YOUR_AZURE_WORKSPACE] --resource-group [YOUR_AZURE_RESOURCE_GROUP] --subscription [YOUR_AZURE_SUBSCRIPTION]\n```\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nWhen you've configured your AutoML Job to the desired settings, you can submit the job.\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=submit-run)]\n\n\nWhen doing a hyperparameter sweep, it can be useful to visualize the different trials that were tried using the HyperDrive UI. You can navigate to this UI by going to the 'Child jobs' tab in the UI of the main automl_image_job from above, which is the HyperDrive parent job. Then you can go into the 'Child jobs' tab of this one.\n\nAlternatively, here below you can see directly the HyperDrive parent job and navigate to its 'Child jobs' tab:\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n```yaml\nCLI example not available, please use Python SDK.\n```\n\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nhd_job = ml_client.jobs.get(returned_job.name + '_HD')\nhd_job\n```\n\n\n## Register and deploy model\n\nOnce the job completes, you can register the model that was created from the best trial (configuration that resulted in the best primary metric). You can either register the model after downloading or by specifying the azureml path with corresponding jobid.  \n\n### Get the best trial\n\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n```yaml\nCLI example not available, please use Python SDK.\n```\n\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=best_run)] \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-auto-train-image-models-387.md"
                }
            },
            {
                "page_content": "\n\n# What is an Azure Machine Learning workspace?\n\nThe workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.  The workspace keeps a history of all training runs, including logs, metrics, output, and a snapshot of your scripts. You use this information to determine which training run produces the best model.  \n\nOnce you have a model you like, you register it with the workspace. You then use the registered model and scoring scripts to deploy to an [online endpoint](concept-endpoints.md) as a REST-based HTTP endpoint.\n\n## Taxonomy \n\n+ A workspace can contain [Azure Machine Learning compute instances](concept-compute-instance.md), cloud resources configured with the Python environment necessary to run Azure Machine Learning.\n\n+ [User roles](how-to-assign-roles.md) enable you to share your workspace with other users, teams, or projects.\n+ [Compute targets](concept-compute-target.md) are used to run your experiments.\n+ When you create the workspace, [associated resources](#associated-resources) are also created for you.\n+ Jobs are training runs you use to build your models.  You can organize your jobs into Experiments.\n+ [Pipelines](concept-ml-pipelines.md) are reusable workflows for training and retraining your model.\n+ [Data assets](concept-data.md) aid in management of the data you use for model training and pipeline creation.\n+ Once you have a model you want to deploy, you create a registered model.\n+ Use the registered model and a scoring script to create an [online endpoint](concept-endpoints.md).\n\n## Tools for workspace interaction\n\nYou can interact with your workspace in the following ways:\n\n+ On the web:\n    + [Azure Machine Learning studio ](https://ml.azure.com) \n    + [Azure Machine Learning designer](concept-designer.md) \n+ In any Python environment with the [Azure Machine Learning SDK for Python](https://aka.ms/sdk-v2-install).\n+ On the command line using the Azure Machine Learning [CLI extension](how-to-configure-cli.md)\n+ [Azure Machine Learning VS Code Extension](how-to-manage-resources-vscode.md#workspaces)\n\n## Machine learning with a workspace\n\nMachine learning tasks read and/or write artifacts to your workspace.\n\n+ Run an experiment to train a model - writes job run results to the workspace.\n+ Use automated ML to train a model - writes training results to the workspace.\n+ Register a model in the workspace.\n+ Deploy a model - uses the registered model to create a deployment.\n+ Create and run reusable workflows.\n+ View machine learning artifacts such as jobs, pipelines, models, deployments.\n+ Track and monitor models.\n\n## Workspace management\n\nYou can also perform the following workspace management tasks:\n\n| Workspace management task           | Portal      | Studio      | Python SDK  | Azure CLI   | VS Code     |\n|-------------------------------------|-------------|-------------|-------------|-------------|-------------|\n| Create a workspace                  | **&check;** | **&check;** | **&check;** | **&check;** | **&check;** |\n| Manage workspace access             | **&check;** |             |             | **&check;** |             |\n| Create and manage compute resources | **&check;** | **&check;** | **&check;** | **&check;** | **&check;** |\n| Create a compute instance           |             | **&check;** | **&check;** | **&check;** | **&check;** |\n\n> [!WARNING]\n> Moving your Azure Machine Learning workspace to a different subscription, or moving the owning subscription to a new tenant, is not supported. Doing so may cause errors.\n\n## Create a workspace\n\nThere are multiple ways to create a workspace:  \n\n* Use [Azure Machine Learning studio](quickstart-create-resources.md) to quickly create a workspace with default settings.\n* Use the [Azure portal](how-to-manage-workspace.md?tabs=azure-portal#create-a-workspace) for a point-and-click interface with more options. \n* Use the [Azure Machine Learning SDK for Python](how-to-manage-workspace.md?tabs=python#create-a-workspace) to create a workspace on the fly from Python scripts or Jupyter notebooks.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-workspace-0.md"
                }
            }
        ]
    },
    {
        "query": "I'm trying to access the files/artifacts located in the code tab under an AzureML job. How can I download the code of a job with the v2 CLI?",
        "result": "You can download the code of an AzureML job with the v2 CLI by using the `az ml job download` command. You need to specify the job ID and the target directory where you want to download the code. For example, to download the code of a job with ID `my_job_id` to the directory `./my_job_code`, you can run the following command: `az ml job download --job-id my_job_id --target-dir ./my_job_code`.",
        "context": [
            {
                "page_content": "\n# Access data in a job\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\n> [!div class=\"op_single_selector\" title1=\"Select the version of Azure Machine Learning CLI extension you use:\"]\n> * [v1](v1/how-to-train-with-datasets.md)\n> * [v2 (current version)](how-to-read-write-data-v2.md)\n\nLearn how to read and write data for your jobs with the Azure Machine Learning Python SDK v2 and the Azure Machine Learning CLI extension v2.\n\n## Prerequisites\n\n- An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n\n- The [Azure Machine Learning SDK for Python v2](https://aka.ms/sdk-v2-install).\n\n- An Azure Machine Learning workspace\n\n## Supported paths\n\nWhen you provide a data input/output to a Job, you must specify a `path` parameter that points to the data location. This table shows both the different data locations that Azure Machine Learning supports, and examples for the `path` parameter:\n\n\n|Location  | Examples  |\n|---------|---------|\n|A path on your local computer     | `./home/username/data/my_data`         |\n|A path on a public http(s) server    |  `https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv`    |\n|A path on Azure Storage     |   `https://<account_name>.blob.core.windows.net/<container_name>/<path>` <br> `abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>`    |\n|A path on a Datastore   |   `azureml://datastores/<data_store_name>/paths/<path>`      |\n|A path to a Data Asset  |  `azureml:<my_data>:<version>`  |\n\n## Supported modes\n\nWhen you run a job with data inputs/outputs, you can specify the *mode* - for example, whether the data should be read-only mounted, or downloaded to the compute target. This table shows the possible modes for different type/mode/input/output combinations:\n\nType | Input/Output | `upload` | `download` | `ro_mount` | `rw_mount` | `direct` | `eval_download` | `eval_mount` \n------ | ------ | :---: | :---: | :---: | :---: | :---: | :---: | :---:\n`uri_folder` | Input  |   | \u2713  |  \u2713  |   | \u2713  |  | \n`uri_file`   | Input |   | \u2713  |  \u2713  |   | \u2713  |  | \n`mltable`   | Input |   | \u2713  |  \u2713  |   | \u2713  | \u2713 | \u2713\n`uri_folder` | Output  | \u2713  |   |    | \u2713  |   |  | \n`uri_file`   | Output | \u2713  |   |    | \u2713  |   |  | \n`mltable`   | Output | \u2713  |   |    | \u2713  | \u2713  |  | \n\n> [!NOTE]\n> `eval_download` and `eval_mount` are unique to `mltable`. The `ro_mount` is the default mode for MLTable. In some scenarios, however, an MLTable can yield files that are not necessarily co-located with the MLTable file in storage. Alternately, an `mltable` can subset or shuffle the data located in the storage resource. That view becomes visible only if the engine actually evaluates the MLTable file. These modes provide that view of the files.\n\n\n## Read data in a job\n\n# [Azure CLI](#tab/cli)\n\nCreate a job specification YAML file (`<file-name>.yml`). In the `inputs` section of the job, specify:\n\n1. The `type`; whether the data is a specific file (`uri_file`), a folder location (`uri_folder`), or an `mltable`.\n1. The `path` of your data location; any of the paths outlined in the [Supported Paths](#supported-paths) section will work.\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n\n# Possible Paths for Data:\n# Blob: https://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>\n# Datastore: azureml://datastores/paths/<folder>/<file>\n# Data Asset: azureml:<my_data>:<version>\n\ncommand: |\n  ls ${{inputs.my_data}}\ncode: <folder where code is located>\ninputs:\n  my_data:\n    type: <type> # uri_file, uri_folder, mltable\n    path: <path>\nenvironment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\ncompute: azureml:cpu-cluster\n```\n\nNext, run in the CLI\n\n```azurecli\naz ml job create -f <file-name>.yml\n```\n\n# [Python SDK](#tab/python)\n\nUse the `Input` class to define:\n\n1. The `type`; whether the data is a specific file (`uri_file`), a folder location (`uri_folder`), or an `mltable`.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-read-write-data-v2-0.md"
                }
            },
            {
                "page_content": "\n# Work with models in Azure Machine Learning\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\nAzure Machine Learning allows you to work with different types of models. In this article, you learn about using Azure Machine Learning to work with different model types, such as custom, MLflow, and Triton. You also learn how to register a model from different locations, and how to use the Azure Machine Learning SDK, the user interface (UI), and the Azure Machine Learning CLI to manage your models.\n\n> [!TIP]\n> If you have model assets created that use the SDK/CLI v1, you can still use those with SDK/CLI v2. Full backward compatibility is provided. All models registered with the V1 SDK are assigned the type `custom`.\n\n## Prerequisites\n\n* An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n* An Azure Machine Learning workspace.\n* The Azure Machine Learning [SDK v2 for Python](https://aka.ms/sdk-v2-install).\n* The Azure Machine Learning [CLI v2](how-to-configure-cli.md).\n\nAdditionally, you will need to:\n\n# [Azure CLI](#tab/cli)\n\n- Install the Azure CLI and the ml extension to the Azure CLI. For more information, see [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n\n# [Python SDK](#tab/python)\n\n- Install the Azure Machine Learning SDK for Python\n    \n    ```bash\n    pip install azure-ai-ml\n    ```\n\n## Supported paths\n\nWhen you provide a model you want to register, you'll need to specify a `path` parameter that points to the data or job location. Below is a table that shows the different data locations supported in Azure Machine Learning and examples for the `path` parameter:\n\n\n|Location  | Examples  |\n|---------|---------|\n|A path on your local computer     | `mlflow-model/model.pkl`         |\n|A path on an AzureML Datastore   |   `azureml://datastores/<datastore-name>/paths/<path_on_datastore>`      |\n|A path from an AzureML job   |   `azureml://jobs/<job-name>/outputs/<output-name>/paths/<path-to-model-relative-to-the-named-output-location>`      |\n|A path from an MLflow job   |   `runs:/<run-id>/<path-to-model-relative-to-the-root-of-the-artifact-location>`      |\n|A path from a Model Asset in AzureML Workspace  | `azureml:<model-name>:<version>`|\n|A path from a Model Asset in  AzureML Registry  | `azureml://registries/<registry-name>/models/<model-name>/versions/<version>`|\n\n## Supported modes\n\nWhen you run a job with model inputs/outputs, you can specify the *mode* - for example, whether you would like the model to be read-only mounted or downloaded to the compute target. The table below shows the possible modes for different type/mode/input/output combinations:\n\nType | Input/Output | `upload` | `download` | `ro_mount` | `rw_mount` | `direct` \n------ | ------ | :---: | :---: | :---: | :---: | :---: \n`custom` file  | Input  |   |  |   |  |    \n`custom` folder    | Input |   | \u2713 | \u2713  | |\u2713  \n`mlflow`    | Input |   | \u2713 |  \u2713 |   |   \n`custom` file | Output  | \u2713  |   |    | \u2713  | \u2713   \n`custom` folder    | Output | \u2713  |   |   | \u2713 | \u2713  \n`mlflow`   | Output | \u2713  |   |    | \u2713  | \u2713 \n\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in a Jupyter Notebook. In the [azureml-examples](https://github.com/azure/azureml-examples) repository, open the notebook: [model.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/assets/model/model.ipynb).\n\n## Create a model in the model registry\n\n[Model registration](concept-model-management-and-deployment.md) allows you to store and version your models in the Azure cloud, in your workspace. The model registry helps you organize and keep track of your trained models.\n\nThe code snippets in this section cover how to:\n\n* Register your model as an asset in Machine Learning by using the CLI.\n* Register your model as an asset in Machine Learning by using the SDK.\n* Register your model as an asset in Machine Learning by using the UI.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-manage-models-0.md"
                }
            },
            {
                "page_content": "> Any jobs that are running when a service outage occurs will not automatically transition to the secondary workspace. It is also unlikely that the jobs will resume and finish successfully in the primary workspace once the outage is resolved. Instead, these jobs must be resubmitted, either in the secondary workspace or in the primary (once the outage is resolved).\n\n### Moving artifacts between workspaces\n\nDepending on your recovery approach, you may need to copy artifacts such as dataset and model objects between the workspaces to continue your work. Currently, the portability of artifacts between workspaces is limited. We recommend managing artifacts as code where possible so that they can be recreated in the failover instance.\n\nThe following artifacts can be exported and imported between workspaces by using the [Azure CLI extension for machine learning](v1/reference-azure-machine-learning-cli.md):\n\n| Artifact | Export | Import |\n| ----- | ----- | ----- |\n| Models | [az ml model download --model-id {ID} --target-dir {PATH}](/cli/azure/ml/model#az-ml-model-download) | [az ml model register \u2013name {NAME} --path {PATH}](/cli/azure/ml/model) |\n| Environments | [az ml environment download -n {NAME} -d {PATH}](/cli/azure/ml/environment#ml-az-ml-environment-download) | [az ml environment register -d {PATH}](/cli/azure/ml/environment#ml-az-ml-environment-register) |\n| Azure ML pipelines (code-generated) | [az ml pipeline get --path {PATH}](/cli/azure/ml(v1)/pipeline#az-ml(v1)-pipeline-get) | [az ml pipeline create --name {NAME} -y {PATH}](/cli/azure/ml(v1)/pipeline#az-ml(v1)-pipeline-create)\n\n> [!TIP]\n> * __Registered datasets__ cannot be downloaded or moved. This includes datasets generated by Azure ML, such as intermediate pipeline datasets. However datasets that refer to a shared file location that both workspaces can access, or where the underlying data storage is replicated, can be registered on both workspaces. Use the [az ml dataset register](/cli/azure/ml(v1)/dataset#ml-az-ml-dataset-register) to register a dataset.\n> * __Job outputs__ are stored in the default storage account associated with a workspace. While job outputs might become inaccessible from the studio UI in the case of a service outage, you can directly access the data through the storage account. For more information on working with data stored in blobs, see [Create, download, and list blobs with Azure CLI](../storage/blobs/storage-quickstart-blobs-cli.md).\n\n## Recovery options\n\n### Workspace deletion\n\nIf you accidentally deleted your workspace it is currently not possible to recover it. However you are able to retrieve your existing notebooks from the corresponding storage if you follow these steps:\n* In the [Azure portal](https://portal.azure.com) navigate to the storage account that was linked to the deleted Azure Machine Learning workspace.\n* In the Data storage section on the left, click on **File shares**.\n* Your notebooks are located on the file share with the name that contains your workspace ID. \n\n![Reference notebook file share](./media/how-to-high-availability-machine-learning/notebook-file-share.png)\n\n## Next steps\n\nTo learn about repeatable infrastructure deployments with Azure Machine Learning, use an [Azure Resource Manager template](./tutorial-create-secure-workspace-template.md).",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-high-availability-machine-learning-160.md"
                }
            }
        ]
    },
    {
        "query": "how can I cancel a run using the new v2 cli?",
        "result": "To cancel a run using the new v2 CLI, use the `az ml run cancel` command followed by the run ID. For example, to cancel a run with the ID `run-id-123`, run the command `az ml run cancel --run-id run-id-123`.",
        "context": [
            {
                "page_content": "    1. A configuration file containing your run configuration settings opens in the editor. If you're satisfied with the settings, select **Submit experiment**. Alternatively, you open the command palette (**View > Command Palette**) from the menu bar and enter the `Azure ML: Submit experiment` command into the text box.\n1. Once your experiment is submitted, a Docker image containing your script and the configurations specified in your run configuration is created.\n\n    When the Docker image build process begins, the contents of the `60_control_log.txt` file stream to the output console in VS Code.\n\n    > [!NOTE]\n    > The first time your Docker image is created can take several minutes.\n\n1. Once your image is built, a prompt appears to start the debugger. Set your breakpoints in your script and select **Start debugger** when you're ready to start debugging. Doing so attaches the VS Code debugger to the container running your experiment. Alternatively, in the Azure Machine Learning extension, hover over the node for your current run and select the play icon to start the debugger.\n\n    > [!IMPORTANT]\n    > You cannot have multiple debug sessions for a single experiment. You can however debug two or more experiments using multiple VS Code instances.\n\nAt this point, you should be able to step-through and debug your code using VS Code.\n\nIf at any point you want to cancel your run, right-click your run node and select **Cancel run**.\n\nSimilar to remote experiment runs, you can expand your run node to inspect the logs and outputs.\n\n> [!TIP]\n> Docker images that use the same dependencies defined in your environment are reused between runs. However, if you run an experiment using a new or different environment, a new image is created. Since these images are saved to your local storage, it's recommended to remove old or unused Docker images. To remove images from your system, use the [Docker CLI](https://docs.docker.com/engine/reference/commandline/rmi/) or the [VS Code Docker extension](https://code.visualstudio.com/docs/containers/overview).\n\n## Debug and troubleshoot machine learning pipelines\n\nIn some cases, you may need to interactively debug the Python code used in your ML pipeline. By using VS Code and debugpy, you can attach to the code as it runs in the training environment.\n\n### Prerequisites\n\n* An __Azure Machine Learning workspace__ that is configured to use an __Azure Virtual Network__.\n* An __Azure Machine Learning pipeline__ that uses Python scripts as part of the pipeline steps. For example, a PythonScriptStep.\n* An Azure Machine Learning Compute cluster, which is __in the virtual network__ and is __used by the pipeline for training__.\n* A __development environment__ that is __in the virtual network__. The development environment might be one of the following:\n\n  * An Azure Virtual Machine in the virtual network\n  * A Compute instance of Notebook VM in the virtual network\n  * A client machine that has private network connectivity to the virtual network, either by VPN or via ExpressRoute.\n\nFor more information on using an Azure Virtual Network with Azure Machine Learning, see [Virtual network isolation and privacy overview](how-to-network-security-overview.md).\n\n> [!TIP]\n> Although you can work with Azure Machine Learning resources that are not behind a virtual network, using a virtual network is recommended.\n\n### How it works\n\nYour ML pipeline steps run Python scripts. These scripts are modified to perform the following actions:\n\n1. Log the IP address of the host that they're running on. You use the IP address to connect the debugger to the script.\n\n2. Start the debugpy debug component, and wait for a debugger to connect.\n\n3. From your development environment, you monitor the logs created by the training process to find the IP address where the script is running.\n\n4. You tell VS Code the IP address to connect the debugger to by using a `launch.json` file.\n\n5. You attach the debugger and interactively step through the script.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-debug-visual-studio-code-52.md"
                }
            },
            {
                "page_content": "\n# Monitor and analyze jobs in studio\n\n\nYou can use [Azure Machine Learning studio](https://ml.azure.com) to monitor, organize, and track your jobs for training and experimentation. Your ML job history is an important part of an explainable and repeatable ML development process.\n\nThis article shows how to do the following tasks:\n\n* Add job display name. \n* Create a custom view. \n* Add a job description. \n* Tag and find jobs.\n* Run search over your job history.\n* Cancel or fail jobs.\n* Monitor the job status by email notification.\n* Monitor your job resources (preview)\n \n\n> [!TIP]\n> * If you're looking for information on using the Azure Machine Learning SDK v1 or CLI v1, see [How to track, monitor, and analyze jobs (v1)](./v1/how-to-track-monitor-analyze-runs.md).\n> * If you're looking for information on monitoring training jobs from the CLI or SDK v2, see [Track experiments with MLflow and CLI v2](how-to-use-mlflow-cli-runs.md).\n> * If you're looking for information on monitoring the Azure Machine Learning service and associated Azure services, see [How to monitor Azure Machine Learning](monitor-azure-machine-learning.md).\n>\n> If you're looking for information on monitoring models deployed to online endpoints, see [Monitor online endpoints](how-to-monitor-online-endpoints.md).\n\n## Prerequisites\n\nYou'll need the following items:\n\n* To use Azure Machine Learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n* You must have an Azure Machine Learning workspace. A workspace is created in [Install, set up, and use the CLI (v2)](how-to-configure-cli.md).\n\n## Job display name \n\nThe job display name is an optional and customizable name that you can provide for your job. To edit the job display name:\n\n1. Navigate to the **Jobs** list. \n\n1. Select the job to edit.\n\n    :::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/select-job.png\" alt-text=\"Screenshot of Jobs list.\":::\n\n1. Select the **Edit** button to edit the job display name.\n\n    :::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/display-name.gif\" alt-text=\"Screenshot of how to edit the display name.\":::\n\n## Custom View \n    \nTo view your jobs in the studio: \n    \n1. Navigate to the **Jobs** tab.\n    \n1. Select either **All experiments** to view all the jobs in an experiment or select **All jobs** to view all the jobs submitted in the Workspace.\n    \nIn the **All jobs'** page, you can filter the jobs list by tags, experiments, compute target and more to better organize and scope your work.  \n    \n1. Make customizations to the page by selecting jobs to compare, adding charts or applying filters. These changes can be saved as a **Custom View** so you can easily return to your work. Users with workspace permissions can edit, or view the custom view. Also, share the custom view with team members for enhanced collaboration by selecting **Share view**.\n\n1. To view the job logs, select a specific job and in the **Outputs + logs** tab, you can find diagnostic and error logs for your job.\n\n    :::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/custom-views-2.gif\" alt-text=\"Screenshot of how to create a custom view.\":::   \n\n## Job description \n\nA job description can be added to a job to provide more context and information to the job. You can also search on these descriptions from the jobs list and add the job description as a column in the jobs list. \n\nNavigate to the **Job Details** page for your job and select the edit or pencil icon to add, edit, or delete descriptions for your job. To persist the changes to the jobs list, save the changes to your existing Custom View or a new Custom View. Markdown format is supported for job descriptions, which allows images to be embedded and deep linking as shown below.\n\n:::image type=\"content\" source=\"media/how-to-track-monitor-analyze-runs/run-description-2.gif\" alt-text=\"Screenshot of how to create a job description.\"::: \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-track-monitor-analyze-runs-0.md"
                }
            },
            {
                "page_content": "    > Replace `<domain>` and `<ingress-secret-name>` in the above Ingress Resource with the domain pointing to LoadBalancer of the **Nginx ingress controller/Application Gateway** and name of your secret. Store the above Ingress Resource in a file name `ing-azureml-fe-tls.yaml`.\n\n1. Deploy ing-azureml-fe-tls.yaml by running\n\n    ```bash\n    kubectl apply -f ing-azureml-fe-tls.yaml\n    ```\n\n2. Check the log of the ingress controller for deployment status.\n\n3. Now the `azureml-fe` application will be available on HTTPS. You can check this by visiting the public LoadBalancer address of the Nginx Ingress Controller.\n\n4. [Create an inference job and invoke](../machine-learning/how-to-deploy-online-endpoints.md).\n\n    >[!NOTE]\n    >\n    > Replace the protocol and IP in scoring_uri with https and domain pointing to LoadBalancer of the Nginx Ingress Controller  or the Application Gateway before invoking.\n\n## Use ARM Template to Deploy Extension\nExtension on managed cluster can be deployed with ARM template. A sample template can be found from [deployextension.json](https://github.com/Azure/AML-Kubernetes/blob/master/files/deployextension.json), with a demo parameter file [deployextension.parameters.json](https://github.com/Azure/AML-Kubernetes/blob/master/files/deployextension.parameters.json) \n\nTo leverage the sample deployment template, edit the parameter file with correct value, then run the following command:\n\n```azurecli\naz deployment group create --name <ARM deployment name> --resource-group <resource group name> --template-file deployextension.json --parameters deployextension.parameters.json\n```\nMore information about how to use ARM template can be found from [ARM template doc](../azure-resource-manager/templates/overview.md)\n\n\n## Azureml extension release note\n> [!NOTE]\n >\n > New features are released at a biweekly calendar.\n\n| Date | Version |Version description |\n|---|---|---|\n| Dec 27, 2022 | 1.1.17 | Move the Fluent-bit from DaemonSet to sidecars. Add MDC support. Refine error messages. Support cluster mode (windows, linux) jobs. Bug fixes|\n| Nov 29, 2022 | 1.1.16 |Add instance type validation by new CRD. Support Tolerance. Shorten SVC Name. Workload Core hour. Multiple Bug fixes and improvements. |\n| Sep 13, 2022 | 1.1.10 | Bug fixes.|\n| Aug 29, 2022 | 1.1.9 | Improved health check logic. Bug fixes.|\n| Jun 23, 2022 | 1.1.6 | Bug fixes. |\n| Jun 15, 2022 | 1.1.5 | Updated training to use new common runtime to run jobs. Removed Azure Relay usage for AKS extension. Removed service bus usage from the extension. Updated security context usage. Updated inference azureml-fe to v2. Updated to use Volcano as training job scheduler. Bug fixes. |\n| Oct 14, 2021 | 1.0.37 | PV/PVC volume mount support in AMLArc training job. |\n| Sept 16, 2021 | 1.0.29 | New regions available, WestUS, CentralUS, NorthCentralUS, KoreaCentral. Job queue explainability. See job queue details in AML Workspace Studio. Auto-killing policy. Support max_run_duration_seconds in ScriptRunConfig. The system will attempt to automatically cancel the run if it took longer than the setting value. Performance improvement on cluster auto scaling support. Arc agent and ML extension deployment from on premises container registry.|\n| August 24, 2021 | 1.0.28 | Compute instance type is supported in job YAML. Assign Managed Identity to AMLArc compute.|\n| August 10, 2021 | 1.0.20 |New Kubernetes distribution support, K3S - Lightweight Kubernetes. Deploy AzureML extension to your AKS cluster without connecting via Azure Arc. Automated Machine Learning (AutoML) via Python SDK. Use 2.0 CLI to attach the Kubernetes cluster to AML Workspace. Optimize AzureML extension components CPU/memory resources utilization.|\n| July 2, 2021 | 1.0.13 | New Kubernetes distributions support, OpenShift Kubernetes and GKE (Google Kubernetes Engine). Autoscale support. If the user-managed Kubernetes cluster enables the autoscale, the cluster will be automatically scaled out or scaled in according to the volume of active runs and deployments. Performance improvement on job launcher, which shortens the job execution time to a great deal.|\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-kubernetes-349.md"
                }
            }
        ]
    },
    {
        "query": "can you give me an example of a CLI v2 command to register a model?",
        "result": "Sure! Here's an example of a CLI v2 command to register a model:\n\n```\naz ml model create --name <model_name> --type \"custom_model\" --path <model_path>\n```\n\nReplace `<model_name>` with the name you want to give your model, and `<model_path>` with the path to the directory containing your model files.",
        "context": [
            {
                "page_content": "\n# Image processing with batch deployments\n\n[!INCLUDE [ml v2](../../includes/machine-learning-dev-v2.md)]\n\nBatch Endpoints can be used for processing tabular data, but also any other file type like images. Those deployments are supported in both MLflow and custom models. In this tutorial, we will learn how to deploy a model that classifies images according to the ImageNet taxonomy.\n\n## About this sample\n\nThe model we are going to work with was built using TensorFlow along with the RestNet architecture ([Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)). A sample of this model can be downloaded from `https://azuremlexampledata.blob.core.windows.net/data/imagenet/model.zip`. The model has the following constrains that are important to keep in mind for deployment:\n\n* It works with images of size 244x244 (tensors of `(224, 224, 3)`).\n* It requires inputs to be scaled to the range `[0,1]`.\n\nThe information in this article is based on code samples contained in the [azureml-examples](https://github.com/azure/azureml-examples) repository. To run the commands locally without having to copy/paste YAML and other files, clone the repo, and then change directories to the `cli/endpoints/batch` if you are using the Azure CLI or `sdk/endpoints/batch` if you are using our SDK for Python.\n\n```azurecli\ngit clone https://github.com/Azure/azureml-examples --depth 1\ncd azureml-examples/cli/endpoints/batch\n```\n\n### Follow along in Jupyter Notebooks\n\nYou can follow along this sample in a Jupyter Notebook. In the cloned repository, open the notebook: [imagenet-classifier-batch.ipynb](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/batch/imagenet-classifier-batch.ipynb).\n\n## Prerequisites\n\n[!INCLUDE [basic cli prereqs](../../includes/machine-learning-cli-prereqs.md)]\n\n* You must have a batch endpoint already created. This example assumes the endpoint is named `imagenet-classifier-batch`. If you don't have one, follow the instructions at [Use batch endpoints for batch scoring](how-to-use-batch-endpoint.md).\n* You must have a compute created where to deploy the deployment. This example assumes the name of the compute is `cpu-cluster`. If you don't, follow the instructions at [Create compute](how-to-use-batch-endpoint.md#create-compute).\n\n## Image classification with batch deployments\n\nIn this example, we are going to learn how to deploy a deep learning model that can classify a given image according to the [taxonomy of ImageNet](https://image-net.org/). \n\n### Registering the model\n\nBatch Endpoint can only deploy registered models so we need to register it. You can skip this step if the model you are trying to deploy is already registered.\n\n1. Downloading a copy of the model:\n\n    # [Azure CLI](#tab/cli)\n    \n    ```azurecli\n    wget https://azuremlexampledata.blob.core.windows.net/data/imagenet/model.zip\n    mkdir -p imagenet-classifier\n    unzip model.zip -d imagenet-classifier\n    ```\n    \n    # [Python](#tab/sdk)\n\n    ```python\n    import os\n    import urllib.request\n    from zipfile import ZipFile\n    \n    response = urllib.request.urlretrieve('https://azuremlexampledata.blob.core.windows.net/data/imagenet/model.zip', 'model.zip')\n    \n    os.mkdirs(\"imagenet-classifier\", exits_ok=True)\n    with ZipFile(response[0], 'r') as zip:\n      model_path = zip.extractall(path=\"imagenet-classifier\")\n    ```\n    \n2. Register the model:\n   \n    # [Azure CLI](#tab/cli)\n\n    ```azurecli\n    MODEL_NAME='imagenet-classifier'\n    az ml model create --name $MODEL_NAME --type \"custom_model\" --path \"imagenet-classifier/model\"\n    ```\n\n    # [Python](#tab/sdk)\n\n    ```python\n    model_name = 'imagenet-classifier'\n    model = ml_client.models.create_or_update(\n        Model(name=model_name, path=model_path, type=AssetTypes.CUSTOM_MODEL)\n    )\n    ```\n\n### Creating a scoring script\n\nWe need to create a scoring script that can read the images provided by the batch deployment and return the scores of the model. The following script:\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-image-processing-batch-0.md"
                }
            },
            {
                "page_content": "The Bandit early termination policy is also used. This policy terminates poor performing trials; that is, those trials that are not within 20% slack of the best performing trial, which significantly saves compute resources.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\nsweep:\n  sampling_algorithm: random\n  early_termination:\n    type: bandit\n    evaluation_interval: 2\n    slack_factor: 0.2\n    delay_evaluation: 6\n```\n\n```yaml\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value: 0.0001\n      max_value: 0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\n```\n\n# [Python SDK](#tab/python)\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=sweep-settings)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=search-space-settings)]\n\n\nOnce the search space and sweep settings are defined, you can then submit the job to train an image model using your training dataset.\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nTo submit your AutoML job, you run the following CLI v2 command with the path to your .yml file, workspace name, resource group and subscription ID.\n\n```azurecli\naz ml job create --file ./hello-automl-job-basic.yml --workspace-name [YOUR_AZURE_WORKSPACE] --resource-group [YOUR_AZURE_RESOURCE_GROUP] --subscription [YOUR_AZURE_SUBSCRIPTION]\n```\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nWhen you've configured your AutoML Job to the desired settings, you can submit the job.\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=submit-run)]\n\n\nWhen doing a hyperparameter sweep, it can be useful to visualize the different trials that were tried using the HyperDrive UI. You can navigate to this UI by going to the 'Child jobs' tab in the UI of the main automl_image_job from above, which is the HyperDrive parent job. Then you can go into the 'Child jobs' tab of this one.\n\nAlternatively, here below you can see directly the HyperDrive parent job and navigate to its 'Child jobs' tab:\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n```yaml\nCLI example not available, please use Python SDK.\n```\n\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n```python\nhd_job = ml_client.jobs.get(returned_job.name + '_HD')\nhd_job\n```\n\n\n## Register and deploy model\n\nOnce the job completes, you can register the model that was created from the best trial (configuration that resulted in the best primary metric). You can either register the model after downloading or by specifying the azureml path with corresponding jobid.  \n\n### Get the best trial\n\n\n# [Azure CLI](#tab/cli)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n```yaml\nCLI example not available, please use Python SDK.\n```\n\n\n# [Python SDK](#tab/python)\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=best_run)] \n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-auto-train-image-models-387.md"
                }
            },
            {
                "page_content": "\n# CLI (v2) core YAML syntax\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nEvery Azure Machine Learning entity has a schematized YAML representation. You can create a new entity from a YAML configuration file with a `.yml` or `.yaml` extension.\n\nThis article provides an overview of core syntax concepts you will encounter while configuring these YAML files.\n\n\n\n## Referencing an Azure ML entity\n\nAzure ML provides a reference syntax (consisting of a shorthand and longhand format) for referencing an existing Azure ML entity when configuring a YAML file. For example, you can reference an existing registered environment in your workspace to use as the environment for a job.\n\n### Referencing an Azure ML asset\n\nThere are two options for referencing an Azure ML asset (environments, models, data, and components):\n* Reference an explicit version of an asset:\n  * Shorthand syntax: `azureml:<asset_name>:<asset_version>`\n  * Longhand syntax, which includes the Azure Resource Manager (ARM) resource ID of the asset:\n  ```\n  azureml:/subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.MachineLearningServices/workspaces/<workspace-name>/environments/<environment-name>/versions/<environment-version>\n  ```\n* Reference the latest version of an asset:\n\n  In some scenarios you may want to reference the latest version of an asset without having to explicitly look up and specify the actual version string itself. The latest    version is defined as the latest (also known as most recently) created version of an asset under a given name. \n\n  You can reference the latest version using the following syntax: `azureml:<asset_name>@latest`. Azure ML will resolve the reference to an explicit asset version in the workspace.\n\n### Reference an Azure ML resource\n\nTo reference an Azure ML resource (such as compute), you can use either of the following syntaxes:\n* Shorthand syntax: `azureml:<resource_name>`\n* Longhand syntax, which includes the ARM resource ID of the resource:\n```\nazureml:/subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.MachineLearningServices/workspaces/<workspace-name>/computes/<compute-name>\n```\n\n## Azure ML data reference URI\n\nAzure ML offers a convenience data reference URI format to point to data in an Azure storage service. This can be used for scenarios where you need to specify a cloud storage location in your YAML file, such as creating an Azure ML model from file(s) in storage, or pointing to data to pass as input to a job.\n\nTo use this data URI format, the storage service you want to reference must first be registered as a datastore in your workspace. Azure ML will handle the data access using the credentials you provided during datastore creation.\n\nThe format consists of a datastore in the current workspace and the path on the datastore to the file or folder you want to point to:\n\n```\nazureml://datastores/<datastore-name>/paths/<path-on-datastore>/\n```\n\nFor example:\n\n* `azureml://datastores/workspaceblobstore/paths/example-data/`\n* `azureml://datastores/workspaceblobstore/paths/example-data/iris.csv`\n\nIn addition to the Azure ML data reference URI, Azure ML also supports the following direct storage URI protocols: `https`, `wasbs`, `abfss`, and `adl`, as well as public `http` and `https` URIs.\n\n## Expression syntax for configuring Azure ML jobs and components\n\nv2 job and component YAML files allow for the use of expressions to bind to contexts for different scenarios. The essential use case is using an expression for a value that might not be known at the time of authoring the configuration, but must be resolved at runtime.\n\nUse the following syntax to tell Azure ML to evaluate an expression rather than treat it as a string:\n\n`${{ <expression> }}`\n\nThe supported scenarios are covered below.\n\n### Parameterizing the `command` with the `inputs` and `outputs` contexts of a job\n\nYou can specify literal values, URI paths, and registered Azure ML data assets as inputs to a job. The `command` can then be parameterized with references to those input(s) using the `${{inputs.<input_name>}}` syntax. References to literal inputs will get resolved to the literal value at runtime, while references to data inputs will get resolved to the download path or mount path (depending on the `mode` specified).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/reference-yaml-core-syntax-0.md"
                }
            }
        ]
    },
    {
        "query": "how can I create a datastore using the new CLI v2?",
        "result": "You can create a datastore using the new Azure CLI v2 by running the following command: \n\n```azurecli\naz ml datastore create --name <datastore_name> --workspace-name <workspace_name> --resource-group <resource_group_name> --subscription <subscription_id> --container <container_name> --account-name <storage_account_name> --account-key <storage_account_key>\n```\n\nReplace the placeholders with the appropriate values for your workspace, resource group, subscription, and storage account. This command creates a new datastore with the specified name and links it to the specified storage account.",
        "context": [
            {
                "page_content": "If you have previously labeled data that you would like to use to train your model, you will first need to upload the images to the default Azure Blob Storage of your Azure ML Workspace and register it as a [data asset](how-to-create-data-assets.md). \n\nThe following script uploads the image data on your local machine at path \"./data/odFridgeObjects\" to datastore in Azure Blob Storage. It then creates a new data asset with the name \"fridge-items-images-object-detection\" in your Azure ML Workspace. \n\nIf there already exists a data asset with the name \"fridge-items-images-object-detection\" in your Azure ML Workspace, it will update the version number of the data asset and point it to the new location where the image data uploaded.\n\n# [Azure CLI](#tab/cli)\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nCreate a .yml file with the following configuration.\n\n```yml\n$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json\nname: fridge-items-images-object-detection\ndescription: Fridge-items images Object detection\npath: ./data/odFridgeObjects\ntype: uri_folder\n```\n\nTo upload the images as a data asset, you run the following CLI v2 command with the path to your .yml file, workspace name, resource group and subscription ID.\n\n```azurecli\naz ml data create -f [PATH_TO_YML_FILE] --workspace-name [YOUR_AZURE_WORKSPACE] --resource-group [YOUR_AZURE_RESOURCE_GROUP] --subscription [YOUR_AZURE_SUBSCRIPTION]\n```\n\n# [Python SDK](#tab/python)\n\n [!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\n[!Notebook-python[] (~/azureml-examples-main/sdk/python/jobs/automl-standalone-jobs/automl-image-object-detection-task-fridge-items/automl-image-object-detection-task-fridge-items.ipynb?name=upload-data)]\n\n# [Studio](#tab/Studio)\n\n![Animation showing how to register a dataset from local files](media\\how-to-prepare-datasets-for-automl-images\\ui-dataset-local.gif)\n\n\nIf you already have your data present in an existing datastore and want to create a data asset out of it, you can do so by providing the path to the data in the datastore, instead of providing the path of your local machine. Update the code [above](how-to-prepare-datasets-for-automl-images.md#using-pre-labeled-training-data-from-local-machine) with the following snippet.\n\n# [Azure CLI](#tab/cli)\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\nCreate a .yml file with the following configuration.\n\n```yml\n$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json\nname: fridge-items-images-object-detection\ndescription: Fridge-items images Object detection\npath: azureml://subscriptions/<my-subscription-id>/resourcegroups/<my-resource-group>/workspaces/<my-workspace>/datastores/<my-datastore>/paths/<path_to_image_data_folder>\ntype: uri_folder\n```\n\n# [Python SDK](#tab/python)\n\n \n```Python\nmy_data = Data(\n    path=\"azureml://subscriptions/<my-subscription-id>/resourcegroups/<my-resource-group>/workspaces/<my-workspace>/datastores/<my-datastore>/paths/<path_to_image_data_folder>\",\n    type=AssetTypes.URI_FOLDER,\n    description=\"Fridge-items images Object detection\",\n    name=\"fridge-items-images-object-detection\",\n)\n```\n\n# [Studio](#tab/Studio)\n\n![Animation showing how to register a dataset from data already present in datastore](media\\how-to-prepare-datasets-for-automl-images\\ui-dataset-datastore.gif)\n\n\nNext, you will need to get the label annotations in JSONL format. The schema of labeled data depends on the computer vision task at hand. Refer to [schemas for JSONL files for AutoML computer vision experiments](reference-automl-images-schema.md) to learn more about the required JSONL schema for each task type.\n\nIf your training data is in a different format (like, pascal VOC or COCO), [helper scripts](https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/image-object-detection/coco2jsonl.py) to convert the data to JSONL are available in [notebook examples](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/automl-standalone-jobs).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-prepare-datasets-for-automl-images-70.md"
                }
            },
            {
                "page_content": "\n# Troubleshoot data access errors\n\n[!INCLUDE [sdk v2](../../includes/machine-learning-sdk-v2.md)]\n\nIn this guide, learn how to identify and resolve known issues with data access with the [Azure Machine Learning SDK](https://aka.ms/sdk-v2-install).\n\n## Error Codes\n\nData access error codes are hierarchical. Error codes are delimited by the full stop character `.` and are more specific the more segments there are.\n\n## ScriptExecution.DatabaseConnection\n\n### ScriptExecution.DatabaseConnection.NotFound\n\nThe database or server defined in the datastore couldn't be found or no longer exists. Check if the database still exists in Azure portal or linked to from the Azure Machine Learning studio datastore details page. If it doesn't exist, recreating it with the same name will enable the existing datastore to be used. If a new server name or database is used, the datastore will have to be deleted, and recreated to use the new name.\n\n### ScriptExecution.DatabaseConnection.Authentication\n\nThe authentication failed while trying to connect to the database. The authentication method is stored inside the datastore and supports SQL authentication, service principal, or no stored credential (identity based access). Enabling workspace MSI makes the authentication use the workspace MSI when previewing data in Azure Machine Learning studio. A SQL server user needs to be created for the service principal and workspace MSI (if applicable) and granted classic database permissions. More info can be found [here](/azure/azure-sql/database/authentication-aad-service-principal-tutorial#create-the-service-principal-user-in-azure-sql-database).\n\nContact your data admin to verify or add the correct permissions to the service principal or user identity.\n\nErrors also include:\n\n- ScriptExecution.DatabaseConnection.Authentication.AzureIdentityAccessTokenResolution.InvalidResource\n  - The server under the subscription and resource group couldn't be found. Check that the subscription ID and resource group defined in the datastore matches that of the server and update the values if needed.\n    > [!NOTE]\n    > Use the subscription ID and resource group of the server and not of the workspace. If the datastore is cross subscription or cross resource group server, these will be different.\n- ScriptExecution.DatabaseConnection.Authentication.AzureIdentityAccessTokenResolution.FirewallSettingsResolutionFailure\n  - The identity doesn't have permission to read firewall settings of the target server. Contact your data admin to the Reader role to the workspace MSI.\n\n## ScriptExecution.DatabaseQuery\n\n### ScriptExecution.DatabaseQuery.TimeoutExpired\n\nThe executed SQL query took too long and timed out. The timeout can be specified at time of data asset creation. If a new timeout is needed, a new asset must be created, or a new version of the current asset must be created. In Azure Machine Learning studio SQL preview, there will have a fixed query timeout, but the defined value will always be honored for jobs.\n\n## ScriptExecution.StreamAccess\n\n### ScriptExecution.StreamAccess.Authentication\n\nThe authentication failed while trying to connect to the storage account. The authentication method is stored inside the datastore and depending on the datastore type, can support account key, SAS token, service principal or no stored credential (identity based access). Enabling workspace MSI makes the authentication use the workspace MSI when previewing data in Azure Machine Learning studio.\n\nContact your data admin to verify or add the correct permissions to the service principal or user identity.\n\n> [!IMPORTANT]\n> If identity based access is used, the required RBAC role is Storage Blob Data Reader. If workspace MSI is used for Azure Machine Learning studio preview, the required RBAC roles are Storage Blob Data Reader and Reader.\n\nErrors also include:\n\n- ScriptExecution.StreamAccess.Authentication.AzureIdentityAccessTokenResolution.FirewallSettingsResolutionFailure\n  - The identity doesn't have permission to read firewall settings of the target storage account. Contact your data admin to the Reader role to the workspace MSI.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-troubleshoot-data-access-0.md"
                }
            },
            {
                "page_content": "Invoking a batch endpoint triggers an asynchronous batch inference job. Compute resources are automatically provisioned when the job starts, and automatically de-allocated as the job completes. So you only pay for compute when you use it.\n\nYou can [override compute resource settings](batch-inference/how-to-use-batch-endpoint.md#overwrite-deployment-configuration-per-each-job) (like instance count) and advanced settings (like mini batch size, error threshold, and so on) for each individual batch inference job to speed up execution and reduce cost.\n\n### Flexible data sources and storage\n\nYou can use the following options for input data when invoking a batch endpoint:\n\n- Cloud data: Either a path on Azure Machine Learning registered datastore, a reference to Azure Machine Learning registered V2 data asset, or a public URI. For more information, see [Data in Azure Machine Learning](concept-data.md).\n- Data stored locally: The data will be automatically uploaded to the Azure ML registered datastore and passed to the batch endpoint.\n\n> [!NOTE]\n> - If you're using existing V1 FileDatasets for batch endpoints, we recommend migrating them to V2 data assets. You can then refer to the V2 data assets directly when invoking batch endpoints. Currently, only data assets of type `uri_folder` or `uri_file` are supported. Batch endpoints created with GA CLIv2 (2.4.0 and newer) or GA REST API (2022-05-01 and newer) will not support V1 Datasets.\n> - You can also extract the datastores' URI or path from V1 FileDatasets. For this, you'll use the `az ml dataset show` command with the `--query` parameter and use that information for invoke.\n> - While batch endpoints created with earlier APIs will continue to support V1 FileDatasets, we'll be adding more support for V2 data assets in the latest API versions for better usability and flexibility. For more information on V2 data assets, see [Work with data using SDK v2](how-to-read-write-data-v2.md). For more information on the new V2 experience, see [What is v2](concept-v2.md).\n\nFor more information on supported input options, see [Accessing data from batch endpoints jobs](batch-inference/how-to-access-data-batch-endpoints-jobs.md).\n\nSpecify the storage output location to any datastore and path. By default, batch endpoints store their output to the workspace's default blob store, organized by the Job Name (a system-generated GUID).\n\n### Security\n\n- Authentication: Azure Active Directory Tokens\n- SSL: enabled by default for endpoint invocation\n- VNET support: Batch endpoints support ingress protection. A batch endpoint with ingress protection will accept scoring requests only from hosts inside a virtual network but not from the public internet. A batch endpoint that is created in a private-link enabled workspace will have ingress protection. To create a private-link enabled workspace, see [Create a secure workspace](tutorial-create-secure-workspace.md).\n\n> [!NOTE]\n> Creating batch endpoints in a private-link enabled workspace is only supported in the following versions.\n> - CLI - version 2.15.1 or higher.\n> - REST API - version 2022-05-01 or higher.\n> - SDK V2 - version 0.1.0b3 or higher.\n\n## Next steps\n\n- [How to deploy online endpoints with the Azure CLI and Python SDK](how-to-deploy-online-endpoints.md)\n- [How to deploy batch endpoints with the Azure CLI and Python SDK](batch-inference/how-to-use-batch-endpoint.md)\n- [How to use online endpoints with the studio](how-to-use-managed-online-endpoint-studio.md)\n- [Deploy models with REST](how-to-deploy-with-rest.md)\n- [How to monitor managed online endpoints](how-to-monitor-online-endpoints.md)\n- [How to view managed online endpoint costs](how-to-view-online-endpoints-costs.md)\n- [Manage and increase quotas for resources with Azure Machine Learning](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints)\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-endpoints-187.md"
                }
            }
        ]
    },
    {
        "query": "How can I get a list of running compute instances and how long they have been running with the v2 CLI?",
        "result": "You can use the `az ml computetarget list` command to get a list of running compute instances. To see how long they have been running, you can add the `--query` parameter with the `creationTime` and `status` properties. The following command will return a list of running compute instances and their creation time and status:\n\n```\naz ml computetarget list --query \"[?provisioningState=='Succeeded'].{Name:name, CreationTime:creationTime, Status:status}\"\n```",
        "context": [
            {
                "page_content": "> The compute instance has 120GB OS disk. If you run out of disk space and get into an unusable state, please clear at least 5 GB disk space on OS disk (mounted on /) through the compute instance terminal by removing files/folders and then do `sudo reboot`. Temporary disk will be freed after restart; you do not need to clear space on temp disk manually. To access the terminal go to compute list page or compute instance details page and click on **Terminal** link. You can check available disk space by running `df -h` on the terminal. Clear at least 5 GB space before doing `sudo reboot`. Please do not stop or restart the compute instance through the Studio until 5 GB disk space has been cleared. Auto shutdowns, including scheduled start or stop as well as idle shutdowns(preview), will not work if the CI disk is full.\n\n## Next steps\n\n* [Quickstart: Create workspace resources you need to get started with Azure Machine Learning](quickstart-create-resources.md).\n* [Tutorial: Train your first ML model](tutorial-1st-experiment-sdk-train.md) shows how to use a compute instance with an integrated notebook.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/concept-compute-instance-125.md"
                }
            },
            {
                "page_content": "\n# How to use parallel job in pipeline (V2)\n\n[!INCLUDE [dev v2](../../includes/machine-learning-dev-v2.md)]\n\nParallel job lets users accelerate their job execution by distributing repeated tasks on powerful multi-nodes compute clusters. For example, take the scenario where you're running an object detection model on large set of images. With Azure ML Parallel job, you can easily distribute your images to run custom code in parallel on a specific compute cluster. Parallelization could significantly reduce the time cost. Also by using Azure ML parallel job you can simplify and automate your process to make it more efficient.\n\n## Prerequisite\n\nAzure ML parallel job can only be used as one of steps in a pipeline job. Thus, it's important to be familiar with using pipelines. To learn more about Azure ML pipelines, see the following articles.\n\n- Understand what is a [Azure Machine Learning pipeline](concept-ml-pipelines.md)\n- Understand how to use Azure ML pipeline with [CLI v2](how-to-create-component-pipelines-cli.md) and [SDK v2](how-to-create-component-pipeline-python.md).\n\n## Why are parallel jobs needed?\n\nIn the real world, ML engineers always have scale requirements on their training or inferencing tasks. For example, when a data scientist provides a single script to train a sales prediction model, ML engineers need to apply this training task to each individual store. During this scale out process, some challenges are:\n\n- Delay pressure caused by long execution time.\n- Manual intervention to handle unexpected issues to keep the task proceeding.\n\nThe core value of Azure ML parallel job is to split a single serial task into mini-batches and dispatch those mini-batches to multiple computes to execute in parallel. By using parallel jobs, we can:\n\n - Significantly reduce end-to-end execution time.\n - Use Azure ML parallel job's automatic error handling settings.\n\nYou should consider using Azure ML Parallel job if:\n\n - You plan to train many models on top of your partitioned data.\n - You want to accelerate your large scale batch inferencing task.\n\n## Prepare for parallel job\n\nUnlike other types of jobs, a parallel job requires preparation. Follow the next sections to prepare for creating your parallel job.\n\n### Declare the inputs to be distributed and partition setting\n\nParallel job requires only one **major input data** to be split and processed with parallel. The major input data can be either tabular data or a set of files. Different input types can have a different partition method.\n\nThe following table illustrates the relation between input data and partition setting:\n\n| Data format | AML input type | AML input mode | Partition method |\n|: ---------- |: ------------- |: ------------- |: --------------- |\n| File list | `mltable` or<br>`uri_folder` | ro_mount or<br>download | By size (number of files) |\n| Tabular data | `mltable` | direct | By size (estimated physical size) |\n\nYou can declare your major input data with `input_data` attribute in parallel job YAML or Python SDK. And you can bind it with one of your defined `inputs` of your parallel job by using `${{inputs.<input name>}}`. Then to define the partition method for your major input.\n\nFor example, you could set numbers to `mini_batch_size` to partition your data **by size**.\n\n- When using file list input, this value defines the number of files for each mini-batch.\n- When using tabular input, this value defines the estimated physical size for each mini-batch.\n\n# [Azure CLI](#tab/cliv2)\n\n[!INCLUDE [cli v2](../../includes/machine-learning-cli-v2.md)]\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\ntype: pipeline\n\ndisplay_name: iris-batch-prediction-using-parallel\ndescription: The hello world pipeline job with inline parallel job\ntags:\n  tag: tagvalue\n  owner: sdkteam\n\nsettings:\n  default_compute: azureml:cpu-cluster\n\njobs:\n  batch_prediction:\n    type: parallel\n    compute: azureml:cpu-cluster\n    inputs:\n      input_data: \n        type: mltable\n        path: ./neural-iris-mltable\n        mode: direct\n      score_model: \n        type: uri_folder\n        path: ./iris-model\n        mode: download\n    outputs:\n      job_output_file:\n        type: uri_file\n        mode: rw_mount\n\n    input_data: ${{inputs.input_data}}\n    mini_batch_size: \"10kb\"\n    resources:\n        instance_count: 2\n    max_concurrency_per_instance: 2\n\n    logging_level: \"DEBUG\"\n    mini_batch_error_threshold: 5\n    retry_settings:\n      max_retries: 2\n      timeout: 60\n\n    task:\n      type: run_function\n      code: \"./script\"\n      entry_script: iris_prediction.py\n      environment:\n        name: \"prs-env\"\n        version: 1\n        image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n        conda_file: ./environment/environment_parallel.yml\n      program_arguments: >-\n        --model ${{inputs.score_model}}\n        --error_threshold 5\n        --allowed_failed_percent 30\n        --task_overhead_timeout 1200\n        --progress_update_timeout 600\n        --first_task_creation_timeout 600\n        --copy_logs_to_parent True\n        --resource_monitor_interva 20\n      append_row_to: ${{outputs.job_output_file}}\n\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-parallel-job-in-pipeline-0.md"
                }
            },
            {
                "page_content": "These same snippets are available when you open your notebook in VS Code. For a complete list of available snippets, see [Azure Machine Learning VS Code Snippets](https://github.com/Azure/azureml-snippets/blob/main/snippets/snippets.md).\n\nYou can browse and search the list of snippets by using the notebook toolbar to open the snippet panel.\n\n:::image type=\"content\" source=\"media/how-to-run-jupyter-notebooks/open-snippet-panel.png\" alt-text=\"Open snippet panel tool in the notebook toolbar\":::\n\nFrom the snippets panel, you can also submit a request to add new snippets.\n\n:::image type=\"content\" source=\"media/how-to-run-jupyter-notebooks/propose-new-snippet.png\" alt-text=\"Snippet panel allows you to propose a new snippet\":::\n-->\n## Share a notebook\n\nYour notebooks are stored in your workspace's storage account, and can be shared with others, depending on their [access level](how-to-assign-roles.md) to your workspace.  They can open and edit the notebook as long as they have the appropriate access. For example, a Contributor can edit the notebook, while a Reader could only view it.\n\nOther users of your workspace can find your notebook in the **Notebooks**, **User files** section of Azure ML studio. By default, your notebooks are in a folder with your username, and others can access them there.\n\nYou can also copy the URL from your browser when you open a notebook, then send to others.  As long as they have appropriate access to your workspace, they can open the notebook.\n\nSince you don't share compute instances, other users who run your notebook will do so on their own compute instance.  \n\n## Collaborate with notebook comments (preview)\n\nUse a notebook comment to collaborate with others who have access to your notebook.\n\nToggle the comments pane on and off with the **Notebook comments** tool at the top of the notebook.  If your screen isn't wide enough, find this tool by first selecting the **...** at the end of the set of tools.\n\n:::image type=\"content\" source=\"media/how-to-run-jupyter-notebooks/notebook-comments-tool.png\" alt-text=\"Screenshot of notebook comments tool in the top toolbar.\":::  \n\nWhether the comments pane is visible or not, you can add a comment into any code cell:\n\n1. Select some text in the code cell.  You can only comment on text in a code cell.\n1. Use the **New comment thread** tool to create your comment.\n    :::image type=\"content\" source=\"media/how-to-run-jupyter-notebooks/comment-from-code.png\" alt-text=\"Screenshot of add a comment to a code cell tool.\":::\n1. If the comments pane was previously hidden, it will now open.  \n1. Type your comment and post it with the tool or use **Ctrl+Enter**.\n1. Once a comment is posted, select **...** in the top right to:\n    * Edit the comment\n    * Resolve the thread\n    * Delete the thread\n\nText that has been commented will appear with a purple highlight in the code. When you select a comment in the comments pane, your notebook will scroll to the cell that contains the highlighted text.\n\n> [!NOTE]\n> Comments are saved into the code cell's metadata.\n\n## Clean your notebook (preview)\n\nOver the course of creating a notebook, you typically end up with cells you used for data exploration or debugging. The *gather* feature will help you produce a clean notebook without these extraneous cells.\n\n1. Run all of your notebook cells.\n1. Select the cell containing the code you wish the new notebook to run. For example, the code that submits an experiment, or perhaps the code that registers a model.\n1. Select the **Gather** icon that appears on the cell toolbar.\n    :::image type=\"content\" source=\"media/how-to-run-jupyter-notebooks/gather.png\" alt-text=\"Screenshot: select the Gather icon\":::\n1. Enter the name for your new \"gathered\" notebook.  \n\nThe new notebook contains only code cells, with all cells required to produce the same results as the cell you selected for gathering.\n\n## Save and checkpoint a notebook\n\nAzure\u00a0Machine\u00a0Learning\u00a0creates\u00a0a\u00a0checkpoint\u00a0file\u00a0when\u00a0you\u00a0create\u00a0an\u00a0*ipynb*\u00a0file.\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-run-jupyter-notebooks-58.md"
                }
            }
        ]
    },
    {
        "query": "how can I create a GPU compute cluster using the new CLI v2?",
        "result": "I'm sorry, but the given context does not provide information about how to create a GPU compute cluster using the new CLI v2. The context only provides information on how to create a GPU compute cluster using Azure Machine Learning Studio.",
        "context": [
            {
                "page_content": "\nIf you prefer to use a browser to sign in and authenticate, you should uncomment the following code and use it instead.\n\n```python\n# Handle to the workspace\n# from azure.ai.ml import MLClient\n\n# Authentication package\n# from azure.identity import InteractiveBrowserCredential\n# credential = InteractiveBrowserCredential()\n```\n\nNext, get a handle to the workspace by providing your Subscription ID, Resource Group name, and workspace name. To find these parameters:\n\n1. Look for your workspace name in the upper-right corner of the Azure Machine Learning studio toolbar.\n2. Select your workspace name to show your Resource Group and Subscription ID.\n3. Copy the values for Resource Group and Subscription ID into the code.\n\n```python\n# Get a handle to the workspace\nml_client = MLClient(\n    credential=credential,\n    subscription_id=\"<SUBSCRIPTION_ID>\",\n    resource_group_name=\"<RESOURCE_GROUP>\",\n    workspace_name=\"<AML_WORKSPACE_NAME>\",\n)\n```\n\nThe result of running this script is a workspace handle that you'll use to manage other resources and jobs.\n\n> [!NOTE]\n> - Creating `MLClient` will not connect the client to the workspace. The client initialization is lazy and will wait for the first time it needs to make a call. In this article, this will happen during compute creation.\n\n### Create a compute resource to run the job\n\nAzureML needs a compute resource to run a job. This resource can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n\nIn the following example script, we provision a Linux [`compute cluster`](./how-to-create-attach-compute-cluster.md?tabs=python). You can see the [`Azure Machine Learning pricing`](https://azure.microsoft.com/pricing/details/machine-learning/) page for the full list of VM sizes and prices. Since we need a GPU cluster for this example, let's pick a *STANDARD_NC6* model and create an AzureML compute.\n\n```python\nfrom azure.ai.ml.entities import AmlCompute\n\ngpu_compute_target = \"gpu-cluster\"\n\ntry:\n    # let's see if the compute target already exists\n    gpu_cluster = ml_client.compute.get(gpu_compute_target)\n    print(\n        f\"You already have a cluster named {gpu_compute_target}, we'll reuse it as is.\"\n    )\n\nexcept Exception:\n    print(\"Creating a new gpu compute target...\")\n\n    # Let's create the Azure ML compute object with the intended parameters\n    gpu_cluster = AmlCompute(\n        # Name assigned to the compute cluster\n        name=\"gpu-cluster\",\n        # Azure ML Compute is the on-demand VM service\n        type=\"amlcompute\",\n        # VM Family\n        size=\"STANDARD_NC6\",\n        # Minimum running nodes when there is no job running\n        min_instances=0,\n        # Nodes in cluster\n        max_instances=4,\n        # How many seconds will the node running after the job termination\n        idle_time_before_scale_down=180,\n        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n        tier=\"Dedicated\",\n    )\n\n    # Now, we pass the object to MLClient's create_or_update method\n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()\n\nprint(\n    f\"AMLCompute with name {gpu_cluster.name} is created, the compute size is {gpu_cluster.size}\"\n)\n```\n\n### Create a job environment\n\nTo run an AzureML job, you'll need an environment. An AzureML [environment](concept-environments.md) encapsulates the dependencies (such as software runtime and libraries) needed to run your machine learning training script on your compute resource. This environment is similar to a Python environment on your local machine.\n\nAzureML allows you to either use a curated (or ready-made) environment\u2014useful for common training and inference scenarios\u2014or create a custom environment using a Docker image or a Conda configuration. \n\nIn this article, you'll reuse the curated AzureML environment `AzureML-tensorflow-2.7-ubuntu20.04-py38-cuda11-gpu`. You'll use the latest version of this environment using the `@latest` directive.\n\n```python\ncurated_env_name = \"AzureML-tensorflow-2.7-ubuntu20.04-py38-cuda11-gpu@latest\"\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-train-tensorflow-56.md"
                }
            },
            {
                "page_content": "\nIf you prefer to use a browser to sign in and authenticate, you should uncomment the following code and use it instead.\n\n```python\n# Handle to the workspace\n# from azure.ai.ml import MLClient\n\n# Authentication package\n# from azure.identity import InteractiveBrowserCredential\n# credential = InteractiveBrowserCredential()\n```\n\nNext, get a handle to the workspace by providing your Subscription ID, Resource Group name, and workspace name. To find these parameters:\n\n1. Look for your workspace name in the upper-right corner of the Azure Machine Learning studio toolbar.\n2. Select your workspace name to show your Resource Group and Subscription ID.\n3. Copy the values for Resource Group and Subscription ID into the code.\n\n```python\n# Get a handle to the workspace\nml_client = MLClient(\n    credential=credential,\n    subscription_id=\"<SUBSCRIPTION_ID>\",\n    resource_group_name=\"<RESOURCE_GROUP>\",\n    workspace_name=\"<AML_WORKSPACE_NAME>\",\n)\n```\n\nThe result of running this script is a workspace handle that you'll use to manage other resources and jobs.\n\n> [!NOTE]\n> - Creating `MLClient` will not connect the client to the workspace. The client initialization is lazy and will wait for the first time it needs to make a call. In this article, this will happen during compute creation.\n\n### Create a compute resource to run the job\n\nAzureML needs a compute resource to run a job. This resource can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n\nIn the following example script, we provision a Linux [`compute cluster`](./how-to-create-attach-compute-cluster.md?tabs=python). You can see the [`Azure Machine Learning pricing`](https://azure.microsoft.com/pricing/details/machine-learning/) page for the full list of VM sizes and prices. Since we need a GPU cluster for this example, let's pick a *STANDARD_NC6* model and create an AzureML compute.\n\n```python\nfrom azure.ai.ml.entities import AmlCompute\n\ngpu_compute_taget = \"gpu-cluster\"\n\ntry:\n    # let's see if the compute target already exists\n    gpu_cluster = ml_client.compute.get(gpu_compute_taget)\n    print(\n        f\"You already have a cluster named {gpu_compute_taget}, we'll reuse it as is.\"\n    )\n\nexcept Exception:\n    print(\"Creating a new gpu compute target...\")\n\n    # Let's create the Azure ML compute object with the intended parameters\n    gpu_cluster = AmlCompute(\n        # Name assigned to the compute cluster\n        name=\"gpu-cluster\",\n        # Azure ML Compute is the on-demand VM service\n        type=\"amlcompute\",\n        # VM Family\n        size=\"STANDARD_NC6\",\n        # Minimum running nodes when there is no job running\n        min_instances=0,\n        # Nodes in cluster\n        max_instances=4,\n        # How many seconds will the node running after the job termination\n        idle_time_before_scale_down=180,\n        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n        tier=\"Dedicated\",\n    )\n\n    # Now, we pass the object to MLClient's create_or_update method\n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()\n\nprint(\n    f\"AMLCompute with name {gpu_cluster.name} is created, the compute size is {gpu_cluster.size}\"\n)\n```\n\n### Create a job environment\n\nTo run an AzureML job, you'll need an environment. An AzureML [environment](concept-environments.md) encapsulates the dependencies (such as software runtime and libraries) needed to run your machine learning training script on your compute resource. This environment is similar to a Python environment on your local machine.\n\nAzureML allows you to either use a curated (or ready-made) environment or create a custom environment using a Docker image or a Conda configuration. In this article, you'll reuse the curated AzureML environment `AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu`. You'll use the latest version of this environment using the `@latest` directive.\n\n```python\ncurated_env_name = \"AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu@latest\"\n```\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-train-pytorch-52.md"
                }
            },
            {
                "page_content": "For more information on workspaces, see [how to manage resources in VS Code](how-to-manage-resources-vscode.md).\n\n## Create a GPU cluster for training\n\nA compute target is the computing resource or environment where you run training jobs. For more information, see the [Azure Machine Learning compute targets documentation](./concept-compute-target.md).\n\n1. In the Azure Machine Learning view, expand your workspace node.\n1. Right-click the **Compute clusters** node inside your workspace's **Compute** node and select **Create Compute**\n\n    > [!div class=\"mx-imgBorder\"]\n    > ![Create training compute cluster](./media/tutorial-train-deploy-image-classification-model-vscode/create-compute.png)\n\n1. A specification file appears. Configure the specification file with the following options.\n\n    ```yml\n    $schema: https://azuremlschemas.azureedge.net/latest/compute.schema.json\n    name: gpu-cluster\n    type: amlcompute\n    size: Standard_NC12\n    \n    min_instances: 0\n    max_instances: 3\n    idle_time_before_scale_down: 120\n    ```\n\n    The specification file creates a GPU cluster called `gpu-cluster` with at most 3 Standard_NC12 VM nodes that automatically scales down to 0 nodes after 120 seconds of inactivity.\n\n    For more information on VM sizes, see [sizes for Linux virtual machines in Azure](../virtual-machines/sizes.md).\n\n1. Right-click the specification file and select **Azure ML: Execute YAML**.\n\nAfter a few minutes, the new compute target appears in the *Compute > Compute clusters* node of your workspace.\n\n## <a name=\"train-the-model\"></a> Train image classification model\n\nDuring the training process, a TensorFlow model is trained by processing the training data and learning patterns embedded within it for each of the respective digits being classified.\n\nLike workspaces and compute targets, training jobs are defined using resource templates. For this sample, the specification is defined in the *job.yml* file which looks like the following:\n\n```yml\n$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\ncode: src\ncommand: >\n    python train.py\nenvironment: azureml:AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11-gpu:48\ncompute: azureml:gpu-cluster\nexperiment_name: tensorflow-mnist-example\ndescription: Train a basic neural network with TensorFlow on the MNIST dataset.\n```\n\nThis specification file submits a training job called `tensorflow-mnist-example` to the recently created `gpu-cluster` computer target that runs the code in the *train.py* Python script. The environment used is one of the curated environments provided by Azure Machine Learning which contains TensorFlow and other software dependencies required to run the training script. For more information on curated environments, see [Azure Machine Learning curated environments](resource-curated-environments.md).\n\nTo submit the training job:\n\n1. Open the *job.yml* file.\n1. Right-click the file in the text editor and select **Azure ML: Execute YAML**.\n\nAt this point, a request is sent to Azure to run your experiment on the selected compute target in your workspace. This process takes several minutes. The amount of time to run the training job is impacted by several factors like the compute type and training data size. To track the progress of your experiment, right-click the current run node and select **View Job in Azure portal**.\n\nWhen the dialog requesting to open an external website appears, select **Open**.\n\n> [!div class=\"mx-imgBorder\"]\n> ![Track experiment progress](./media/tutorial-train-deploy-image-classification-model-vscode/track-experiment-progress.png)\n\nWhen the model is done training, the status label next to the run node updates to \"Completed\".\n\n## Next steps\n\nIn this tutorial, you learn the following tasks:\n\n> [!div class=\"checklist\"]\n> * Understand the code\n> * Create a workspace\n> * Create a GPU cluster for training\n> * Train a model\n\nFor next steps, see:\n\n* [Create and manage Azure Machine Learning resources using Visual Studio Code](how-to-set-up-vs-code-remote.md).\n",
                "metadata": {
                    "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/tutorial-train-deploy-image-classification-model-vscode-62.md"
                }
            }
        ]
    }
]