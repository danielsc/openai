
In the above code sample:
- `code` parameter defines relative path of the folder containing parameterized `titanic.py` file.
- `resource` parameter defines `instance_type` and Apache Spark `runtime_version` used by Managed (Automatic) Spark compute. The following instance types are currently supported:
    - `Standard_E4S_V3`
    - `Standard_E8S_V3`
    - `Standard_E16S_V3`
    - `Standard_E32S_V3`
    - `Standard_E64S_V3`

# [Studio UI](#tab/studio-ui)
First, upload the parameterized Python code `titanic.py` to the Azure Blob storage container for workspace default datastore `workspaceblobstore`. To submit a standalone Spark job using the Azure Machine Learning studio UI:

:::image type="content" source="media/quickstart-spark-jobs/create-standalone-spark-job.png" lightbox="media/quickstart-spark-jobs/create-standalone-spark-job.png" alt-text="Expandable screenshot showing creation of a new Spark job in the Azure Machine Learning studio UI.":::

1. In the left pane, select **+ New**.
2. Select **Spark job (preview)**.
3. On the **Compute** screen:

    :::image type="content" source="media/quickstart-spark-jobs/create-standalone-spark-job-compute.png" lightbox="media/quickstart-spark-jobs/create-standalone-spark-job-compute.png" alt-text="Expandable screenshot showing compute selection screen for a new Spark job in the Azure Machine Learning studio UI.":::

   1. Under **Select compute type**, select **Spark automatic compute (Preview)** for Managed (Automatic) Spark compute.
   2. Select **Virtual machine size**. The following instance types are currently supported:
      - `Standard_E4s_v3`
      - `Standard_E8s_v3`
      - `Standard_E16s_v3`
      - `Standard_E32s_v3`
      - `Standard_E64s_v3`
   3. Select **Spark runtime version** as **Spark 3.2**.
   4. Select **Next**.
4. On the **Environment** screen, select **Next**.
5. On **Job settings** screen:
    1. Provide a job **Name**, or use the job **Name**, which is generated by default.
    2. Select an **Experiment name** from the dropdown menu.
    3. Under **Add tags**, provide **Name** and **Value**, then select **Add**. Adding tags is optional.
    4. Under the **Code** section:
        1. Select **Azure Machine Learning workspace default blob storage** from **Choose code location** dropdown.
        2. Under **Path to code file to upload**, select **Browse**.
        3. In the pop-up screen titled **Path selection**, select the path of code file `titanic.py` on the workspace default datastore `workspaceblobstore`.
        4. Select **Save**.
        5. Input `titanic.py` as the name of **Entry file** for the standalone job.
        6. To add an input, select **+ Add input** under **Inputs** and
            1. Enter **Input name** as `titanic_data`. The input should refer to this name later in the **Arguments**.
            2. Select **Input type** as **Data**.
            3. Select **Data type** as **File**.
            4. Select **Data source** as **URI**.
            5. Enter an Azure Data Lake Storage (ADLS) Gen 2 data URI for `titanic.csv` file in the `abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/<PATH_TO_DATA>` format. Here, `<FILE_SYSTEM_NAME>` matches the container name.
        7.  To add an input, select **+ Add output** under **Outputs** and
            1. Enter **Output name** as `wrangled_data`. The output should refer to this name later in the **Arguments**.
            2. Select **Output type** as **Folder**.
            3. For **Output URI destination**, enter an Azure Data Lake Storage (ADLS) Gen 2 folder URI in the `abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/<PATH_TO_DATA>` format. Here `<FILE_SYSTEM_NAME>` matches the container name.
        8.  Enter **Arguments**  as `--titanic_data ${{inputs.titanic_data}} --wrangled_data ${{outputs.wrangled_data}}`.
    5. Under the **Spark configurations** section:
        1. For **Executor size**:
            1. Enter the number of executor **Cores** as 2 and executor **Memory (GB)** as 2.
