        - `%Y%m%d%H`: The sub folder name is the time to hour.
            - `processes_%M`: The file ends with the minute of the checking time.
    - `node_disk_usage.csv`: Detailed disk usage of the node.
    - `node_resource_usage.csv`: Resource usage overview of the node.
    - `processes_resource_usage.csv`: Resource usage overview of each process.

### How to log in scoring script

You can use Python logging in your scoring script. Logs are stored in `logs/user/stdout/<node_id>/processNNN.stdout.txt`. 

```python
import argparse
import logging

# Get logging_level
arg_parser = argparse.ArgumentParser(description="Argument parser.")
arg_parser.add_argument("--logging_level", type=str, help="logging level")
args, unknown_args = arg_parser.parse_known_args()
print(args.logging_level)

# Initialize Python logger
logger = logging.getLogger(__name__)
logger.setLevel(args.logging_level.upper())
logger.info("Info log statement")
logger.debug("Debug log statement")
```

## Common issues

The following section contains common problems and solutions you may see during batch endpoint development and consumption.

### No module named 'azureml'

__Message logged__: `No module named 'azureml'`.

__Reason__: Azure Machine Learning Batch Deployments require the package `azureml-core` to be installed.

__Solution__: Add `azureml-core` to your conda dependencies file.

### Output already exists

__Reason__: Azure Machine Learning Batch Deployment can't overwrite the `predictions.csv` file generated by the output.

__Solution__: If you are indicated an output location for the predictions, ensure the path leads to a non-existing file.

### The run() function in the entry script had timeout for [number] times

__Message logged__: `No progress update in [number] seconds. No progress update in this check. Wait [number] seconds since last update.`

__Reason__: Batch Deployments can be configured with a `timeout` value that indicates the amount of time the deployment shall wait for a single batch to be processed. If the execution of the batch takes more than such value, the task is aborted. Tasks that are aborted can be retried up to a maximum of times that can also be configured. If the `timeout` occurs on each retry, then the deployment job fails. These properties can be configured for each deployment.

__Solution__: Increase the `timemout` value of the deployment by updating the deployment. These properties are configured in the parameter `retry_settings`. By default, a `timeout=30` and `retries=3` is configured. When deciding the value of the `timeout`, take into consideration the number of files being processed on each batch and the size of each of those files. You can also decrease them to account for more mini-batches of smaller size and hence quicker to execute. 

### Dataset initialization failed

__Message logged__: Dataset initialization failed: UserErrorException: Message: Cannot mount Dataset(id='xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', name='None', version=None). Source of the dataset is either not accessible or does not contain any data.

__Reason__: The compute cluster where the deployment is running can't mount the storage where the data asset is located. The managed identity of the compute don't have permissions to perform the mount.

__Solutions__: Ensure the identity associated with the compute cluster where your deployment is running has at least has at least [Storage Blob Data Reader](../role-based-access-control/built-in-roles.md#storage-blob-data-reader) access to the storage account. Only storage account owners can [change your access level via the Azure portal](../storage/blobs/assign-azure-role-data-access.md).

### Data set node [code] references parameter dataset_param which doesn't have a specified value or a default value

__Message logged__: Data set node [code] references parameter dataset_param which doesn't have a specified value or a default value.

__Reason__: The input data asset provided to the batch endpoint isn't supported.
