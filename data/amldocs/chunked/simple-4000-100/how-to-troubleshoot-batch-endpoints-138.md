__Message logged__: Data set node [code] references parameter dataset_param which doesn't have a specified value or a default value.

__Reason__: The input data asset provided to the batch endpoint isn't supported.

__Solution__: Ensure you are providing a data input that is supported for batch endpoints.

### User program failed with Exception: Run failed, please check logs for details

__Message logged__: User program failed with Exception: Run failed, please check logs for details. You can check logs/readme.txt for the layout of logs.

__Reason__: There was an error while running the `init()` or `run()` function of the scoring script.

__Solution__: Go to __Outputs + Logs__ and open the file at `logs > user > error > 10.0.0.X > process000.txt`. You will see the error message generated by the `init()` or `run()` method.

### ValueError: No objects to concatenate

__Message logged__: ValueError: No objects to concatenate.

__Reason__: All the files in the generated mini-batch are either corrupted or unsupported file types. Remember that MLflow models support a subset of file types as documented at [Considerations when deploying to batch inference](how-to-mlflow-batch.md?#considerations-when-deploying-to-batch-inference).

__Solution__: Go to the file `logs/usr/stdout/<process-number>/process000.stdout.txt` and look for entries like `ERROR:azureml:Error processing input file`. If the file type is not supported, please review the list of supported files. You may need to change the file type of the input data or customize the deployment by providing a scoring script as indicated at [Using MLflow models with a scoring script](how-to-mlflow-batch.md?#customizing-mlflow-models-deployments-with-a-scoring-script).

### There is no succeeded mini batch item returned from run()

__Message logged__: There is no succeeded mini batch item returned from run(). Please check 'response: run()' in https://aka.ms/batch-inference-documentation.

__Reason__: The batch endpoint failed to provide data in the expected format to the `run()` method. This may be due to corrupted files being read or incompatibility of the input data with the signature of the model (MLflow).

__Solution__: To understand what may be happening, go to __Outputs + Logs__ and open the file at `logs > user > stdout > 10.0.0.X > process000.stdout.txt`. Look for error entries like `Error processing input file`. You should find there details about why the input file can't be correctly read.

### Audiences in JWT are not allowed

__Context__: When invoking a batch endpoint using its REST APIs.

__Reason__: The access token used to invoke the REST API for the endpoint/deployment is indicating a token that is issued for a different audience/service. Azure Active Directory tokens are issued for specific actions.

__Solution__: When generating an authentication token to be used with the Batch Endpoint REST API, ensure the `resource` parameter is set to `https://ml.azure.com`. Please notice that this resource is different from the resource you need to indicate to manage the endpoint using the REST API. All Azure resources (including batch endpoints) use the resource `https://management.azure.com` for managing them. Ensure you use the right resource URI on each case. Notice that if you want to use the management API and the job invocation API at the same time, you will need two tokens. For details see: [Authentication on batch endpoints (REST)](how-to-authenticate-batch-endpoint.md?tabs=rest).

## Limitations and not supported scenarios

When designing machine learning solutions that rely on batch endpoints, some configurations and scenarios may not be supported.

The following __workspace__ configurations are __not supported__:

* Workspaces configured with an Azure Container Registries with Quarantine feature enabled.
* Workspaces with customer-managed keys (CMK).

The following __compute__ configurations are __not supported__:

* Azure ARC Kubernetes clusters.
* Granular resource request (memory, vCPU, GPU) for Azure Kubernetes clusters. Only instance count can be requested.
