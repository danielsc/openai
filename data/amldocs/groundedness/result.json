{
    "query": "Code Sample to create a datastore and use it in sweep Job using AzureML SDK V2?",
    "result": "Here is an example of how to create a datastore and use it in a sweep job using AzureML SDK v2:\n\n```python\nfrom azureml.core import Workspace, Datastore\nfrom azureml.core.compute import AmlCompute, ComputeTarget\nfrom azureml.core.runconfig import CondaDependencies, RunConfiguration\nfrom azureml.core.environment import Environment\nfrom azureml.core import ScriptRunConfig\nfrom azureml.train.hyperdrive import (\n    RandomParameterSampling,\n    BanditPolicy,\n    HyperDriveConfig,\n    PrimaryMetricGoal,\n    choice,\n    loguniform,\n)\nfrom azureml.core import Experiment\nfrom azureml.pipeline.core import Pipeline, PipelineData\nfrom azureml.pipeline.steps import HyperDriveStep\nfrom azureml.train.hyperdrive import MedianStoppingPolicy\nfrom azureml.core.runconfig import DEFAULT_CPU_IMAGE\n\n# Define workspace and datastore\nws = Workspace.from_config()\ndatastore_name = 'mydatastore'\ndatastore = Datastore.register_azure_blob_container(workspace=ws,\n                                                     datastore_name=datastore_name,\n                                                     container_name='mycontainer',\n                                                     account_name='myaccountname',\n                                                     account_key='myaccountkey')\n\n# Define compute target\ncompute_name = 'mycompute'\nif compute_name not in ws.compute_targets:\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n                                                           min_nodes=0,\n                                                           max_nodes=1)\n    compute_target = ComputeTarget.create(ws, compute_name, compute_config)\n    compute_target.wait_for_completion(show_output=True)\nelse:\n    compute_target = ws.compute_targets[compute_name]\n\n# Define environment\nenv = Environment.from_conda_specification(name='myenv', file_path='./conda_dependencies.yml')\nenv.docker.enabled = True\nenv.docker.base_image = DEFAULT_CPU_IMAGE\n\n# Define script run configuration\nsrc = ScriptRunConfig(source_directory='./',\n                      script='train.py',\n                      compute_target=compute_target,\n                      environment=env)\n\n# Define hyperdrive configuration\nparam_sampling = RandomParameterSampling(\n    {\n        '--batch-size': choice(25, 50, 100),\n        '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n        '--second-layer-neurons': choice(10, 50, 200, 500),\n        '--learning-rate': loguniform(-6, -1)\n    }\n)\n\nearly_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n\nhd_config = HyperDriveConfig(run_config=src,\n                             hyperparameter_sampling=param_sampling,\n                             policy=early_termination_policy,\n                             primary_metric_name='validation_acc',\n                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                             max_total_runs=4,\n                             max_concurrent_runs=4)\n\n# Define pipeline data\nmetrics_output_name = 'metrics_output'\nmetrics_data = PipelineData(name='metrics_data',\n                            datastore=datastore,\n                            pipeline_output_name=metrics_output_name,\n                            training_output=TrainingOutput(\"Metrics\"))\n\nmodel_output_name = 'model_output'\nsaved_model = PipelineData(name='saved_model',\n                            datastore=datastore,\n                            pipeline_output_name=model_output_name,\n                            training_output=TrainingOutput(\"Model\",\n                                                           model_file=\"outputs/model/saved_model.pb\"))\n\n# Define HyperDriveStep\nhd_step_name = 'hd_step01'\nhd_step = HyperDriveStep(\n    name=hd_step_name,\n    hyperdrive_config=hd_config,\n    inputs=[datastore.as_input('mydatastore')],\n    outputs=[metrics_data, saved_model])\n\n# Define pipeline\npipeline = Pipeline(workspace=ws, steps=[hd_step])\n\n# Submit pipeline\npipeline_run = Experiment(ws, 'myexperiment').submit(pipeline)\n```\n\nIn this example, we first create a datastore using `Datastore.register_azure_blob_container()`. We then define a compute target and an environment. We create a script run configuration using `ScriptRunConfig()` and a hyperdrive configuration using `HyperDriveConfig()`. We define pipeline data using `PipelineData()` and create a `HyperDriveStep()` using the hyperdrive configuration and the pipeline data. Finally, we define a pipeline using `Pipeline()` and submit it using `Experiment().submit()`. Note that we pass the datastore to the `HyperDriveStep()` using `datastore.as_input()`.",
    "context": [
        {
            "page_content": "\n# Upgrade hyperparameter tuning to SDK v2\n\nIn SDK v2, tuning hyperparameters are consolidated into jobs.\n\nA job has a type. Most jobs are command jobs that run a `command`, like `python main.py`. What runs in a job is agnostic to any programming language, so you can run `bash` scripts, invoke `python` interpreters, run a bunch of `curl` commands, or anything else.\n\nA sweep job is another type of job, which defines sweep settings and can be initiated by calling the sweep method of command.\n\nTo upgrade, you'll need to change your code for defining and submitting your hyperparameter tuning experiment to SDK v2. What you run _within_ the job doesn't need to be upgraded to SDK v2. However, it's recommended to remove any code specific to Azure ML from your model training scripts. This separation allows for an easier transition between local and cloud and is considered best practice for mature MLOps. In practice, this means removing `azureml.*` lines of code. Model logging and tracking code should be replaced with MLflow. For more information, see [how to use MLflow in v2](how-to-use-mlflow-cli-runs.md).\n\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\n\n## Run hyperparameter tuning in an experiment\n\n* SDK v1\n\n    ```python\n    from azureml.core import ScriptRunConfig, Experiment, Workspace\n    from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n    from azureml.train.hyperdrive import choice, loguniform\n    \n    dataset = Dataset.get_by_name(ws, 'mnist-dataset')\n    \n    # list the files referenced by mnist dataset\n    dataset.to_path()\n    \n    #define the search space for your hyperparameters\n    param_sampling = RandomParameterSampling(\n        {\n            '--batch-size': choice(25, 50, 100),\n            '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n            '--second-layer-neurons': choice(10, 50, 200, 500),\n            '--learning-rate': loguniform(-6, -1)\n        }\n    )\n    \n    args = ['--data-folder', dataset.as_named_input('mnist').as_mount()]\n    \n    #Set up your script run\n    src = ScriptRunConfig(source_directory=script_folder,\n                          script='keras_mnist.py',\n                          arguments=args,\n                          compute_target=compute_target,\n                          environment=keras_env)\n    \n    # Set early stopping on this one\n    early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n    \n    # Define the configurations for your hyperparameter tuning experiment\n    hyperdrive_config = HyperDriveConfig(run_config=src,\n                                         hyperparameter_sampling=param_sampling,\n                                         policy=early_termination_policy,\n                                         primary_metric_name='Accuracy',\n                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                         max_total_runs=20,\n                                         max_concurrent_runs=4)\n    # Specify your experiment details                                     \n    experiment = Experiment(workspace, experiment_name)\n    \n    hyperdrive_run = experiment.submit(hyperdrive_config)\n    \n    #Find the best model\n    best_run = hyperdrive_run.get_best_run_by_primary_metric()\n    ```\n\n* SDK v2\n\n    ```python\n    from azure.ai.ml import MLClient\n    from azure.ai.ml import command, Input\n    from azure.ai.ml.sweep import Choice, Uniform, MedianStoppingPolicy\n    from azure.identity import DefaultAzureCredential\n    \n    # Create your command\n    command_job_for_sweep = command(\n        code=\"./src\",\n        command=\"python main.py --iris-csv ${{inputs.iris_csv}} --learning-rate ${{inputs.learning_rate}} --boosting ${{inputs.boosting}}\",\n        environment=\"AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu@latest\",\n        inputs={\n            \"iris_csv\": Input(\n                type=\"uri_file\",\n                path=\"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\",\n            ),\n            #define the search space for your hyperparameters\n            \"learning_rate\": Uniform(min_value=0.01, max_value=0.9),\n            \"boosting\": Choice(values=[\"gbdt\", \"dart\"]),\n        },\n        compute=\"cpu-cluster\",\n    )\n    \n    # Call sweep() on your command job to sweep over your parameter expressions\n    sweep_job = command_job_for_sweep.sweep(\n        compute=\"cpu-cluster\", \n        sampling_algorithm=\"random\",\n        primary_metric=\"test-multi_logloss\",\n        goal=\"Minimize\",\n    )\n    \n    # Define the limits for this sweep\n    sweep_job.set_limits(max_total_trials=20, max_concurrent_trials=10, timeout=7200)\n    \n    # Set early stopping on this one\n    sweep_job.early_termination = MedianStoppingPolicy(delay_evaluation=5, evaluation_interval=2)\n    \n    # Specify your experiment details\n    sweep_job.display_name = \"lightgbm-iris-sweep-example\"\n    sweep_job.experiment_name = \"lightgbm-iris-sweep-example\"\n    sweep_job.description = \"Run a hyperparameter sweep job for LightGBM on Iris dataset.\"\n    \n    # submit the sweep\n    returned_sweep_job = ml_client.create_or_update(sweep_job)\n    \n    # get a URL for the status of the job\n    returned_sweep_job.services[\"Studio\"].endpoint\n    \n    # Download best trial model output\n    ml_client.jobs.download(returned_sweep_job.name, output_name=\"model\")\n    ```\n",
            "metadata": {
                "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-execution-hyperdrive-0.md"
            }
        },
        {
            "page_content": "\n### Python SDK\n\nThe Python SDK example can be found in [azureml-example repo](https://github.com/Azure/azureml-examples). Navigate to *azureml-examples/sdk/jobs/pipelines/1c_pipeline_with_hyperparameter_sweep* to check the example.\n\nIn Azure Machine Learning Python SDK v2, you can enable hyperparameter tuning for any command component by calling `.sweep()` method.\n\nBelow code snippet shows how to enable sweep for `train_model`.\n\n```python\ntrain_component_func = load_component(source=\"./train.yml\")\nscore_component_func = load_component(source=\"./predict.yml\")\n\n# define a pipeline\n@pipeline()\ndef pipeline_with_hyperparameter_sweep():\n    \"\"\"Tune hyperparameters using sample components.\"\"\"\n    train_model = train_component_func(\n        data=Input(\n            type=\"uri_file\",\n            path=\"wasbs://datasets@azuremlexamples.blob.core.windows.net/iris.csv\",\n        ),\n        c_value=Uniform(min_value=0.5, max_value=0.9),\n        kernel=Choice([\"rbf\", \"linear\", \"poly\"]),\n        coef0=Uniform(min_value=0.1, max_value=1),\n        degree=3,\n        gamma=\"scale\",\n        shrinking=False,\n        probability=False,\n        tol=0.001,\n        cache_size=1024,\n        verbose=False,\n        max_iter=-1,\n        decision_function_shape=\"ovr\",\n        break_ties=False,\n        random_state=42,\n    )\n    sweep_step = train_model.sweep(\n        primary_metric=\"training_f1_score\",\n        goal=\"minimize\",\n        sampling_algorithm=\"random\",\n        compute=\"cpu-cluster\",\n    )\n    sweep_step.set_limits(max_total_trials=20, max_concurrent_trials=10, timeout=7200)\n\n    score_data = score_component_func(\n        model=sweep_step.outputs.model_output, test_data=sweep_step.outputs.test_data\n    )\n\n\npipeline_job = pipeline_with_hyperparameter_sweep()\n\n# set pipeline level compute\npipeline_job.settings.default_compute = \"cpu-cluster\"\n```\n\n We first load `train_component_func` defined in `train.yml` file. When creating `train_model`, we add `c_value`, `kernel` and `coef0` into search space(line 15-17). Line 30-35 defines the primary metric, sampling algorithm etc.\n\n## Check pipeline job with sweep step in Studio\n\nAfter you submit a pipeline job, the SDK or CLI widget will give you a web URL link to Studio UI. The link will guide you to the pipeline graph view by default.\n\nTo check details of the sweep step, double click the sweep step and navigate to the **child job** tab in the panel on the right.\n\n:::image type=\"content\" source=\"./media/how-to-use-sweep-in-pipeline/pipeline-view.png\" alt-text=\"Screenshot of the pipeline with child job and the train_model node highlighted.\" lightbox= \"./media/how-to-use-sweep-in-pipeline/pipeline-view.png\":::\n\nThis will link you to the sweep job page as seen in the below screenshot. Navigate to **child job** tab, here you can see the metrics of all child jobs and list of all child jobs.\n\n:::image type=\"content\" source=\"./media/how-to-use-sweep-in-pipeline/sweep-job.png\" alt-text=\"Screenshot of the job page on the child jobs tab.\" lightbox= \"./media/how-to-use-sweep-in-pipeline/sweep-job.png\":::\n\nIf a child jobs failed, select the name of that child job to enter detail page of that specific child job (see screenshot below). The useful debug information is under **Outputs + Logs**.\n\n:::image type=\"content\" source=\"./media/how-to-use-sweep-in-pipeline/child-run.png\" alt-text=\"Screenshot of the output + logs tab of a child run.\" lightbox= \"./media/how-to-use-sweep-in-pipeline/child-run.png\":::\n\n## Sample notebooks\n\n- [Build pipeline with sweep node](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/1c_pipeline_with_hyperparameter_sweep/pipeline_with_hyperparameter_sweep.ipynb)\n- [Run hyperparameter sweep on a command job](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/single-step/lightgbm/iris/lightgbm-iris-sweep.ipynb)\n\n## Next steps\n\n- [Track an experiment](how-to-log-view-metrics.md)\n- [Deploy a trained model](how-to-deploy-online-endpoints.md)\n",
            "metadata": {
                "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/how-to-use-sweep-in-pipeline-305.md"
            }
        },
        {
            "page_content": "\n## Run hyperparameter tuning in a pipeline\n\n* SDK v1\n\n    ````python\n    \n    tf_env = Environment.get(ws, name='AzureML-TensorFlow-2.0-GPU')\n    data_folder = dataset.as_mount()\n    src = ScriptRunConfig(source_directory=script_folder,\n                          script='tf_mnist.py',\n                          arguments=['--data-folder', data_folder],\n                          compute_target=compute_target,\n                          environment=tf_env)\n    \n    #Define HyperDrive configs\n    ps = RandomParameterSampling(\n        {\n            '--batch-size': choice(25, 50, 100),\n            '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n            '--second-layer-neurons': choice(10, 50, 200, 500),\n            '--learning-rate': loguniform(-6, -1)\n        }\n    )\n    \n    early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n    \n    hd_config = HyperDriveConfig(run_config=src, \n                                 hyperparameter_sampling=ps,\n                                 policy=early_termination_policy,\n                                 primary_metric_name='validation_acc', \n                                 primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                                 max_total_runs=4,\n                                 max_concurrent_runs=4)\n                                 \n    metrics_output_name = 'metrics_output'\n    metrics_data = PipelineData(name='metrics_data',\n                                datastore=datastore,\n                                pipeline_output_name=metrics_output_name,\n                                training_output=TrainingOutput(\"Metrics\"))\n    \n    model_output_name = 'model_output'\n    saved_model = PipelineData(name='saved_model',\n                                datastore=datastore,\n                                pipeline_output_name=model_output_name,\n                                training_output=TrainingOutput(\"Model\",\n                                                               model_file=\"outputs/model/saved_model.pb\"))\n    #Create HyperDriveStep\n    hd_step_name='hd_step01'\n    hd_step = HyperDriveStep(\n        name=hd_step_name,\n        hyperdrive_config=hd_config,\n        inputs=[data_folder],\n        outputs=[metrics_data, saved_model])                             \n    \n    #Find and register best model\n    conda_dep = CondaDependencies()\n    conda_dep.add_pip_package(\"azureml-sdk\")\n    \n    rcfg = RunConfiguration(conda_dependencies=conda_dep)\n    \n    register_model_step = PythonScriptStep(script_name='register_model.py',\n                                           name=\"register_model_step01\",\n                                           inputs=[saved_model],\n                                           compute_target=cpu_cluster,\n                                           arguments=[\"--saved-model\", saved_model],\n                                           allow_reuse=True,\n                                           runconfig=rcfg)\n    \n    register_model_step.run_after(hd_step)\n    \n    #Run the pipeline\n    pipeline = Pipeline(workspace=ws, steps=[hd_step, register_model_step])\n    pipeline_run = exp.submit(pipeline)\n    \n    ````\n\n* SDK v2\n\n    ```python\n    train_component_func = load_component(path=\"./train.yml\")\n    score_component_func = load_component(path=\"./predict.yml\")\n    \n    # define a pipeline\n    @pipeline()\n    def pipeline_with_hyperparameter_sweep():\n        \"\"\"Tune hyperparameters using sample components.\"\"\"\n        train_model = train_component_func(\n            data=Input(\n                type=\"uri_file\",\n                path=\"wasbs://datasets@azuremlexamples.blob.core.windows.net/iris.csv\",\n            ),\n            c_value=Uniform(min_value=0.5, max_value=0.9),\n            kernel=Choice([\"rbf\", \"linear\", \"poly\"]),\n            coef0=Uniform(min_value=0.1, max_value=1),\n            degree=3,\n            gamma=\"scale\",\n            shrinking=False,\n            probability=False,\n            tol=0.001,\n            cache_size=1024,\n            verbose=False,\n            max_iter=-1,\n            decision_function_shape=\"ovr\",\n            break_ties=False,\n            random_state=42,\n        )\n        sweep_step = train_model.sweep(\n            primary_metric=\"training_f1_score\",\n            goal=\"minimize\",\n            sampling_algorithm=\"random\",\n            compute=\"cpu-cluster\",\n        )\n        sweep_step.set_limits(max_total_trials=20, max_concurrent_trials=10, timeout=7200)\n    \n        score_data = score_component_func(\n            model=sweep_step.outputs.model_output, test_data=sweep_step.outputs.test_data\n        )\n    \n    \n    pipeline_job = pipeline_with_hyperparameter_sweep()\n    \n    # set pipeline level compute\n    pipeline_job.settings.default_compute = \"cpu-cluster\"\n    \n    # submit job to workspace\n    pipeline_job = ml_client.jobs.create_or_update(\n        pipeline_job, experiment_name=\"pipeline_samples\"\n    )\n    pipeline_job\n    ```\n",
            "metadata": {
                "sourcefile": "UI/2023-04-06_191207_UTC/simple-4000-100/migrate-to-v2-execution-hyperdrive-119.md"
            }
        }
    ],
    "rating_out_of_10": "10",
    "error": "Error parsing result json: Invalid control character at: line 3 column 1227 (char 1327)"
}